<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>It's all about the interfaces</title>
  <meta name="description" content="">

  <link rel="shortcut icon" type="image/x-icon" href="/img/GT.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://zipcpu.com/blog/2017/10/07/study-interfaces.html">
  <link rel="alternate" type="application/rss+xml" title="The ZipCPU by Gisselquist Technology" href="https://zipcpu.com/feed.xml">
</head>


  <body>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-102570964-1', 'auto');
  ga('send', 'pageview');

</script>

    <header class="site-header">
  <div id="banner">
  <a href="/"><picture>
    <img height=120 id="site-logo" src="/img/fullgqtech.png" alt="Gisselquist Technology, LLC">
  </picture></A>
  </div>

  <div class="site-nav">
<ul>

<li><a HREF="/">Main/Blog</a>


<li><a HREF="/about/">About Us</a>


<li><a HREF="/fpga-hell.html">FPGA Hell</a>


<li><a HREF="/tutorial/">Tutorial</a>


<li><a HREF="/projects.html">Projects</a>


<li><a HREF="/topics.html">Site Index</a>


<li><a href="https://twitter.com/zipcpu"><span class="icon--twitter"><svg viewBox="0 0 400 400"><path fill="#1da1f2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>
</span><span class="username">@zipcpu</span></a>

<li><a HREF="https://www.patreon.com/ZipCPU"><IMG SRC="/img/patreon_logomark_color_on_white.png" WIDTH="25"> Support</a>
</ul>
</div>


</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="https://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">It's all about the interfaces</h1>
    <p class="post-meta"><time datetime="2017-10-07T00:00:00-04:00" itemprop="datePublished">Oct 7, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <table style="float: right"><tr><td><img src="/img/cpu-serial.svg" alt="CPU algorithms take time" width="60" /></td></tr></table>

<p>If you are looking for raw computational speed,
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>s
are a well known solution.</p>

<p>Unlike
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>’,
a <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>s
speed is limited by the fact that the
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
must perform its instructions one at a time, even though many algorithms
could be made to run faster.  The
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>s
general purpose nature simpy slows it down when compared to what
could be done within an electronic circuit.</p>

<p>While many of the more modern
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>s
have eight or more separate processing cores within a single chip,
this still doesn’t compare to the sheer parallelism offered within an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>.</p>

<table style="float: left; padding: 15px"><tr><td><img src="/img/fpga-parallel.svg" alt="FPGAs run algorithms in parallel" width="120" /></td></tr></table>

<p>Not only can
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>s
run their algorithms in parallel, but they can also tailor their silicon for
just your algorithm.  Modern
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>s support
divides, floating point operations, and SIMD instructions, many of which may
have nothing to do with what you are trying to accomplish.  The
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a> on the
other hand has a majority of its logic available for you to configure.
So, just how many copies of your algorithm do you wish to configure onto your
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a> solution?</p>

<p>For these same reasons,
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>s
can be faster than
<a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a>s as well–when
the algorithm in question isn’t a video algorithm such as the
<a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> was optimized
for.</p>

<p>While you might consider using an
<a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASIC</a>
to run your algorithm even faster,
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>s
they tend to be a lot cheaper to debug and manufacture than
<a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASIC</a>s
are.  For example, making a mistake on an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a> usually
only costs your time to fix it, whereas a mistake within an
<a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASIC</a>
design may cost you many millions of dollars.</p>

<p>For all these reasons, if your goal is speed, you may find yourself
considering an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
as your solution.  They are hands down the most economical means of running
an algorithm faster than a
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a> can.</p>

<p>Just don’t neglect the speed of your interface while you consider engineering an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
solution.</p>

<h2 id="its-the-interfaces-sir">It’s the interfaces, Sir</h2>

<p>I’ve noticed, as I’ve personally worked with
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>s, that the
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
fabric has rarely ever limited my algorithm’s speed and performance needs.
That’s always been the easy part of the task.</p>

<table style="float: left; padding: 15px"><tr><td><img src="/img/fpga-bottleneck.svg" alt="Interfaces are an FPGAs achilles heel" width="360" /></td></tr></table>

<p>Want to calculate a
<a href="https://en.wikipedia.org/wiki/Haar_wavelet">Haar wavelet</a>
transform on an image?
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>s have
enough logic to run the algorithm many times over within them!  You can run
the transform horizontally, vertically, no problem–the raw task is easy for an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>.</p>

<p>That’s not the hard part.</p>

<p>The hard part is feeding the algorithm with data, and getting the results back
out fast enough to be competitive with the alternatives.</p>

<p>Perhaps an example will help explain what I’m talking about.</p>

<p>Years ago, I had the opportunity to work on a really neat
<a href="https://en.wikipedia.org/wiki/Global_Positioning_System">GPS</a>
processing algorithm.  If you are familiar with
<a href="https://en.wikipedia.org/wiki/Global_Positioning_System">GPS</a> processing,
you’ll know that the success of a
<a href="https://en.wikipedia.org/wiki/Global_Positioning_System">GPS</a>
processing algorithm is based upon how many correlations you can do and how
fast you can do them.</p>

<p>In my case, I was starting with a special algorithm that had been demonstrated
in software, and had proven itself in software on a general purpose
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>.  My problem
was that the algorithm ran slower than pond water.  My team needed speed.
For other reasons, there was an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
connected to to our
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
microcontroller.  So we asked ourselves, why not use the
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
to speed up the processing?</p>

<table style="float: right"><tr><td><img src="/img/fft-offload.svg" alt="Offloading an FFT process from a CPU onto an FPGA" width="360" /></td></tr></table>

<p>Specifically, I was interfacing an
<a href="https://en.wikipedia.org/wiki/ARM_architecture">ARM</a>
based PXA271 <a href="https://en.wikipedia.org/wiki/XScale">XScale</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>, built by
<a href="https://www.intel.com">Intel</a> at the time, with a Spartan 3
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>,
from <a href="https://www.xilinx.com">Xilinx</a>.  The
<a href="https://en.wikipedia.org/wiki/Global_Positioning_System">GPS</a>
algorithms performance was dominated by the number
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>s
the <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
had to accomplish.  Why not do those
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>s
within the <a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>,
and run them that much faster?</p>

<p>If you look at
<a href="https://www.xilinx.com">Xilinx</a>’s
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
IP core, they offer a pipelined
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
core that can run one
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
sample per clock–just like they did when I was working on this
problem.  Hence, an N point
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
costs N clocks to ingest into the
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>,
and after some (short) processing delay it takes N clocks to get the data
back out.  How much faster could you get?  The
<a href="https://en.wikipedia.org/wiki/ARM_architecture">ARM</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
took many more clocks than that to process the
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>, so this should
be much faster, right?</p>

<p>So I built it.</p>

<p>The
<a href="https://en.wikipedia.org/wiki/ARM_architecture">ARM</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
and the
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
were both attached on the same physical bus.  That meant I could have the
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
send the
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a> the
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
data over the bus and then read it back when finished.</p>

<p>With a little bit of debugging, I managed to get it all to work.  It wasn’t
all that hard technically to build, and speed was very important to
our application.  So, we ran the algorithm with a stop watch, anxiously
waiting to see how much faster it would run.</p>

<table style="float: right"><tr><td><img src="/img/fft-bottleneck.svg" alt="The interfaces are the bottleneck" width="360" /></td></tr></table>

<p>Much to my displeasure, the new
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
enhanced
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
algorithm <em>didn’t run any faster</em>.  Indeed, as I recall, I think it even ran
slower.  I was shocked.  This wasn’t what I was expecting.</p>

<p>That’s when I learned the painful lesson that an algorithm’s speed is dependent
upon the interface speed that feeds the algorithm.  In our case, the interface
was so slow that just transferring the data to the
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
and reading the results back took more time to do than to perform the
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
in the first place.</p>

<h2 id="learn-the-interfaces">Learn the interfaces!</h2>

<p>This is one of the reasons why the study of
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
design needs to include a study of interfaces and how to interact with them.
Indeed, if you look at one of my favorite
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
websites, <a href="http://fpga4fun.com">fpga4fun.com</a>,
you’ll find a lot of discussions about how to build interfaces.
They discuss <a href="http://fpga4fun.com/SerialInterface.html">serial ports</a>,
<a href="http://www.fpga4fun.com/I2C.html">I2C</a>,
<a href="http://www.fpga4fun.com/SPI.html">SPI</a>,
<a href="http://www.fpga4fun.com/JTAG.html">JTAG</a>,
<a href="http://www.fpga4fun.com/PongGame.html">simple video ports (play Pong!)</a>,
<a href="http://www.fpga4fun.com/HDMI.html">HDMI</a>,
and more.  All of these interfaces have their purpose, and the
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
student is well served by studying how to interact with them.</p>

<p>Sadly, none of them would’ve been fast enough to rescue my
<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a>
processing needs above.  (Although, … using the
<a href="https://en.wikipedia.org/wiki/Direct_memory_access">DMA</a>
controller on the <a href="https://en.wikipedia.org/wiki/XScale">XScale</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
might’ve helped …)</p>

<p>So, for this reason, let me recommend to you that before you spend your whole
dime on making your
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a> run
super fast with multiple copies of your algorithm all running in parallel,
that you at least spend as much (or more) of that dime guaranteeing that the
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
can read and write your data fast enough to keep your
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
busy running that super-algorithm.</p>


  </div>


<div class "verse">
<HR align="center;" width="25%">
<P><em>But ye said, No; for we will flee upon horses; therefore shall ye flee:  and, We will ride upon the swift; therefore shall they that pursue you be swift. (Is 30:16)</em>


</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The ZipCPU by Gisselquist Technology</h2>
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <!-- <li></li> -->
          <li><a href="mailto:zipcpu@gmail.com">zipcpu@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="soc-medlist">
          
          <li>
            <a href="https://github.com/ZipCPU"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">ZipCPU</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/zipcpu"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">@zipcpu</span></a>

          </li>
          
          
          <li><A href="https://www.patreon.com/ZipCPU"><img src="/img/become_a_patron_button.png"></a></li>
          

        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The ZipCPU blog, featuring how to discussions of FPGA and soft-core CPU design.  This site will be focused on Verilog solutions, using exclusively OpenSource IP products for FPGA design.  Particular focus areas include topics often left out of more mainstream FPGA design courses such as how to debug an FPGA design.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
