<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>An Overview of a 10Gb Ethernet Switch</title>
  <meta name="description" content="Fig 1. The KlusterLab board used for the 10Gb Ethernet Switch testing">

  <link rel="shortcut icon" type="image/x-icon" href="/img/GT.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://zipcpu.com/blog/2023/11/25/eth10g.html">
  <link rel="alternate" type="application/rss+xml" title="The ZipCPU by Gisselquist Technology" href="https://zipcpu.com/feed.xml">
</head>


  <body>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4ZK7HKHSVW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4ZK7HKHSVW');
</script>

    <header class="site-header">
  <div id="banner">
  <a href="/"><picture>
    <img height=120 id="site-logo" src="/img/fullgqtech.png" alt="Gisselquist Technology, LLC">
  </picture></A>
  </div>

  <div class="site-nav">
<ul>

<li><a HREF="/">Main/Blog</a>


<li><a HREF="/about/">About Us</a>


<li><a HREF="/fpga-hell.html">FPGA Hell</a>


<li><a HREF="/tutorial/">Tutorial</a>
<li><a HREF="/tutorial/formal.html">Formal training</a>


<li><a HREF="/quiz/quizzes.html">Quizzes</a>


<li><a HREF="/projects.html">Projects</a>


<li><a HREF="/topics.html">Site Index</a>

<HR>

<li><a href="https://twitter.com/zipcpu"><span class="icon--twitter"><svg viewBox="0 0 400 400"><path fill="#1da1f2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>
</span><span class="username">@zipcpu</span></a>

<li><a href="https://www.reddit.com/r/ZipCPU"><span class="username">Reddit</a>
<li><a HREF="https://www.patreon.com/ZipCPU"><IMG SRC="/img/patreon_logomark_color_on_white.png" WIDTH="25"> Support</a>
</ul>
</div>


</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="https://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">An Overview of a 10Gb Ethernet Switch</h1>
    <p class="post-meta"><time datetime="2023-11-25T00:00:00-05:00" itemprop="datePublished">Nov 25, 2023</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <table align="center" style="float: right"><caption>Fig 1. The KlusterLab board used for the 10Gb Ethernet Switch testing</caption><tr><td><img src="/img/eth10g/klusterlab_1.0.jpeg" alt="" width="360" /></td></tr></table>

<p>I’ve now been working with <a href="https://www.symbioticeda.com/">Symbiotic EDA</a>
and <a href="https://www.pcb-arts.com">PCB Arts</a> on a <a href="https://github.com/ZipCPU/eth10g">10Gb Ethernet switch
project</a> for
<a href="https://www.netidee.at/fastopenswitch">NetIdee</a> for some time.  Indeed,
I’ve discussed <a href="https://github.com/ZipCPU/eth10g">this project</a> several times
on the blog.  I first brought it up in the context of building a <a href="/blog/2023/04/08/vpktfifo.html">Virtual Packet
FIFO</a>.  The topic then came
up again during two articles on building an <a href="https://github.com/ZipCPU/sdspi">SDIO (SD-Card)
controller</a>: first when <a href="/blog/2023/06/28/sdiopkt.html">discussing how to
build a Verilog test bench for
it</a>, and then again when
<a href="/formal/2023/07/18/sdrxframe.html">discussing what bugs managed to slip past the verification, which then had to
be caught in hardware</a>.
What we haven’t yet discussed is the switch itself, and how it works.</p>

<p>When discussing this project, I’ve often shown the <a href="https://github.com/ZipCPU/eth10g/blob/master/doc/eth10g-busblocks.png">component
diagram</a> below.</p>

<table align="center" style="float: none"><caption>Fig 2. The ETH10G project, from a bus component viewpoint</caption><tr><td><img src="/img/eth10g/eth10g-busblocks.svg" alt="" width="780" /></td></tr></table>

<p>This illustrates the design based upon how the bus views the design–where the
two <a href="/blog/2019/07/17/crossbar.html">crossbars</a>
are, and what components connect to them.  I’ve also used diagram
this from a management context to show how far the project is along.  Each
component was shaded in red initially, and it’s color slowly adjusted as the
project moved along.  It’s a color code I’ve used often to help communicate
project progress with others (i.e. customers), and it’s now worked nicely
across many projects.  The basic color legend works as:</p>

<ul>
  <li>
    <p>Red boxes indicate components that either haven’t yet been designed, or
whose design isn’t complete.</p>
  </li>
  <li>
    <p>Once a component has been designed and passes a basic Verilator lint check,
its color changes from red to yellow.</p>
  </li>
  <li>
    <p>Components with formal proofs are then colored green.</p>

    <p>Sometimes, I might also color as green components that aren’t going to
be formally verified, but that pass a test bench based simulation.</p>
  </li>
  <li>
    <p>Once a component gets tested in hardware, an outline is given to it.  A
red outline indicates the component has failed hardware testing.</p>
  </li>
  <li>
    <p>A dark green outline is used to indicate a component that has passed all
hardware testing.</p>
  </li>
</ul>

<p>The diagram has been modified a bit from my basic encoding with annotations
underneath the components to indicate if a
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
scope (a type of
internal logic analyzer) had been connected to the component, or if software
had been built for that component or not.  As you can see, I still have a bit
of work left to do on the <a href="https://github.com/ZipCPU/wbsata">SATA controller</a>,
and the PCB still needs to go through another revision.  What is new, however,
is that the networking components of this design are now working.  The design
now functions as a switch.  Therefore, I thought this might be a good time to
discuss the network switch portion of this design, how it accepts 10Gb Ethernet
packets, processes them, and forwards the same 10Gb Ethernet packets on.</p>

<p>We’ll work our way from the edges of this design, where the Ethernet packets
come in, all the way to the routing algorithm used by the switch.  We’ll
then discuss a special bus arbiter I needed to write, go through the network
components of this design that I expect will be reusable, and then discuss
some lessons learned from building the networking components.  Before we
get there, though, we have to take a moment to remind our readers of the
internal network protocol that is making all of this possible.</p>

<h2 id="the-internal-protocol">The Internal Protocol</h2>

<p>The entire packet processing system of this 10Gb switch is built around an
<a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">AXI Network (AXIN) packet
protocol</a>.  I’ve <a href="/blog/2022/02/23/axis-abort.html">discussed
this some time before on this
blog</a>.  However, since <a href="/blog/2022/02/23/axis-abort.html">this
protocol</a> is so central
to everything that follows, it only makes sense to take a moment to quickly
review it here.  Specifically, let’s note the differnces between <a href="/blog/2022/02/23/axis-abort.html">this
AXIN protocol</a> and the
<a href="/doc/axi-stream.pdf">standard AXI stream protocol</a> it
was based upon.</p>

<table align="center" style="float: left"><caption>Fig 3. AXI Network protocol signals</caption><tr><td><img src="/img/eth10g/axinsignals.png" alt="" width="472" /></td></tr></table>

<p>Let’s start with why the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>
is necessary.  To put it in one word,
<a href="/blog/2022/02/23/axis-abort.html">AXIN</a>
is necessary because of <em>backpressure</em>.  A <a href="/doc/axi-stream.pdf">true AXI
stream</a> implementation
requires backpressure support–where a slave can tell the source it is not
ready, and hold the ready line false indefinitely.  However, in a network
context the incoming network source has a limited buffer to support any
backpressure.  Hence <a href="/blog/2022/02/23/axis-abort.html">the need for a new
protocol</a>.</p>

<p>Many have argued that this isn’t a sufficient need to justify creating a new
protocol.  Why not, they have argued, just buffer any incoming packets until
a whole packet is in the buffer before forwarding it with full backpressure
support?  I’ve now seen several FIFO implementations which can do this sort of
thing: buffer until a full packet has been received, then forward the packet
via traditional AXI stream.  The reason why I haven’t chosen this approach is
because I had a customer ask for jumbo packet support (64kB+ packet sizes).
True “jumbo packets” will be larger than my largest incoming FIFO, so I haven’t
chosen this approach either.</p>

<p>The <a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a> differs
from <a href="/doc/axi-stream.pdf">AXI stream</a> in two key respects.
The first is the addition of an <code class="language-plaintext highlighter-rouge">ABORT</code> signal.  The <code class="language-plaintext highlighter-rouge">ABORT</code> signal can be
raised by an <a href="/blog/2022/02/23/axis-abort.html">AXIN master</a>
master at any time–even when <code class="language-plaintext highlighter-rouge">VALID &amp;&amp; !READY</code>.  If raised, it signals that
to the <a href="/blog/2022/02/23/axis-abort.html">AXIN slave</a>
that the current packet needs to be dropped in its entirety.
Many reasons might cause the <code class="language-plaintext highlighter-rouge">ABORT</code> to be asserted.  For example, if the
initial packet source can’t handle the slave’s <code class="language-plaintext highlighter-rouge">!READY</code> signal, it could abort
the packet.  If the packet CRC doesn’t match, the packet might also be aborted.</p>

<p>The second differences is the <code class="language-plaintext highlighter-rouge">BYTES</code> field.  The <a href="/doc/axi-stream.pdf">original AXI stream
protocol</a>
identified valid bytes via <code class="language-plaintext highlighter-rouge">TKEEP</code> and <code class="language-plaintext highlighter-rouge">TSTRB</code>.  This allows packet data to
be discontinuous, and requires processing <code class="language-plaintext highlighter-rouge">2*WIDTH/8</code> signals per beat.  The
<a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">AXIN protocol</a>
instead uses a <code class="language-plaintext highlighter-rouge">BYTES</code> field having only <code class="language-plaintext highlighter-rouge">$clog2(WIDTH/8)</code> bits.
This <code class="language-plaintext highlighter-rouge">BYTES</code> field has the requirement that it must be zero (all bytes valid)
for all but the <code class="language-plaintext highlighter-rouge">LAST</code> beat, where it can be anything.  This also carries
the implied requirement that all beats prior to the last beat must be full,
and the last beat must be right (or left) justified.</p>

<table align="center" style="float: right"><caption>Fig 4. Watch out for mixed endianness!</caption><tr><td><img src="/img/eth10g/endianbug.svg" alt="" width="420" /></td></tr></table>

<p>When defining <a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">this
protocol</a>, I didn’t define
whether or not it was little endian or big endian.  As a consequence, some of
the components of this design are little endian (Ethernet is little endian by
nature), and others are big endian–since I implement both
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> and the
<a href="/about/zipcpu.html">ZipCPU</a> in a big-endian fashion.</p>

<h2 id="gtx-phy-front-end">GTX PHY Front End</h2>

<p><a href="https://github.com/ZipCPU/eth10g">This design</a> <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/xgtxphy.v">uses Xilinx’s GTX IO
controllers</a>
to generate and ingest the 10Gb/s links.  As <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/xgtxphy.v">I’ve configured
them</a>, the GTX
IO controllers within this project act like 1:32 and 32:1 I/OSERDES macros,
simplifying their implementation.  Some key features of these components is
that they can recover the clock when receiving and they
can perform some amount of receive decision feedback channel equalization (DFE).</p>

<p>Although these transceivers appear amazing in capability, this great capability
seems also to be their Achilles heel.  I found myself reading, re-reading, and
then re-reading their user guide again and again only to find myself confused
regarding how to configure them.  It didn’t help that many of the configuration
options said that the Xilinx wizard’s configuration was to be used, and nothing
more explained about the respective option.  Nor did it help that many of the
ports remain as legacy from previous versions of the controller, and the user
guide suggests that they should not be used anymore.  The GTX transceivers have
many, many options associated with them, many of which are either not
documented at all or are only poorly documented within the user guide.  The
“official” solution to this problem is to use a Vivado wizard for that purpose,
and then to use one of the canned configurations Vivado offers.  Perhaps this
is ideal for Xilinx, who sells a fixed number of IP components based upon these
configurations.  However, this doesn’t really work well when you wish to
post or share an all–RTL design.  It also fails when you want to step off
of the beaten path.</p>

<table align="center" style="float: left"><caption>Fig 5. The next GTX project will bee SATA</caption><tr><td><img src="/img/eth10g/nextgtx.svg" alt="" width="420" /></td></tr></table>

<p>As a result, I often ended up rather confused when configuring the GTX
components.</p>

<p>The most obvious consequence of this is that I relied heavily on the GTX
simulation models.  This meant that I needed to <a href="https://github.com/ZipCPU/eth10g/blob/master/bench/rtl/tbenet.v">model a 10Gb Ethernet link
in Verilog</a>
to stimulate those models.  If you look, you’ll notice I’ve
got quite the <a href="https://github.com/ZipCPU/eth10g/blob/fc846af41987236f1c00886e5f87e58ca7e6ba51/bench/rtl/tbenet.v#L162-L306">clock recovery circuit</a>
written there in Verilog as well, so
that I can recover the clock from the signal generated by the GTX simulation
model.  For those who know me, this is also a rather drastic departure from my
all-<a href="/blog/2017/06/21/looking-at-verilator.html">Verilator</a>
approach to simulation.  This is understandable, though, given both the
complexity of these components and the fact that Xilinx did not provide a
simulation model that would work with
<a href="/blog/2017/06/21/looking-at-verilator.html">Verilator</a>.</p>

<p>A second consequence is that I was never able to get the GTX’s 64/66b
encoder/decoder working.  I’m sure the interface was quite intuitive to
whoever designed it.  I just couldn’t get it working–not even in simulation.
Then again, schedule pressure being what it was, it was just simpler to build
(and formally verify) my own
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66brxgears.v">32:66</a>
and <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66btxgears.v">66:32</a>
gearboxes, and to use my own <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66brxgears.v">66b synchronization
module</a>.
Perhaps I might’ve figured out how to do this with their GTX transceiver if I
had another month to work on it.</p>

<table align="center" style="float: right"><caption>Fig 6. Note to Xilinx's designers</caption><tr><td><img src="/img/eth10g/nowizard.svg" alt="" width="420" /></td></tr></table>

<p>Were I to offer any suggestions to Xilinx regarding their GTX design, I
would simply suggest that they simplify it drastically.  A good hardware IO
module on an FPGA should handle any required high speed IO, while also leaving
the protocol processing to be implemented in the FPGA fabric.  It should also
be versatile enough to continue to support the same protocols it currently
supports (and more), without doing protocol specific handling, such as 64/66b
or 8b/10b encoding and decoding, in the PHY components themselves.  This
means I’d probably remove phase alignment and “comma detection” from the PHY
(I wasn’t using either of them for this project anyway) and force them back
into the FPGA fabric.  (Ask me again about these features, though, after I’ve
had to use the GTX for a project that requires them.)</p>

<p>So, in the end, I used the GTX transceiver simply as a combined 32:1 OSERDES
(for the transmit side) and a 1:32 ISERDES with clock recovery (for receive).
Everything else was relegated to the fabric.</p>

<h2 id="the-digital-front-end">The Digital Front End</h2>

<p>Ethernet processing in FPGA Logic was split into two parts.  The first part,
what I call the “digital front end”, converts the 32b data interface
required by the PHY to the
<a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">AXIN interfaces</a> used by
everything else.  That’s all done in my
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netpath.v">netpath</a>
module.</p>

<p>Early on in this project, I diagrammed out this path, as shown in Fig. 7 below.</p>

<table align="center" style="float: none"><caption>Fig 7. Networking flow block diagram</caption><tr><td><img src="/img/eth10g/eth10g-blocks.svg" alt="" width="780" /></td></tr></table>

<p>The diagram begin as a simple block diagram of networking components that
needed to be built and verified for the switch to work.  (The color legend
for this component is roughly the same as the color legend for the bus
components shown in Fig. 2 above.)</p>

<p>Let’s walk through that module briefly here, working through first the receive
chain followed by the transmit chain.  In each case, processing is accomplished
a discrete number of steps.</p>

<h3 id="the-incoming-rx-chain">The (Incoming) RX Chain</h3>

<p>The receive data processing chain starts with 32b words, sampled at
322MHz, and converts them to 128b wide <a href="/blog/2022/02/23/axis-abort.html">AXIN packet
streams</a> sampled at 100MHz.
(Why 100MHz?  Because that was the DDR3 SDRAM memory controller’s speed in this
design.)</p>

<ol>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66brxgears.v">32b/66b Gearbox</a>.</p>

    <p>As I mentioned in the GTX section above, I could never figure out how to
get the 64/66b conversion working in the GTX front end.  I’m sure Xilinx’s
interface made sense to some, it just never quite made sense to me.  As a
result, it was just easier to use the PHY as a 32b ISERDES and process
everything from there.  This way I could control the signaling and gearbox
handling.  Even better, I could formally verify that I was doing things
right using the signals I had that were under my own control.</p>

    <p>That means that the first step is a <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66brxgears.v">32:66b
gearbox</a>.</p>

    <p>This gearbox is also where the 66b synchronization happens.  For those not
familiar with the 64/66b protocol, the extra two bits are used for
synchronization.  These two bits are guaranteed (by protocol) to be
different.  One bit combination will indicate the presence of packet data,
the other indicates control information.</p>

    <p>The <a href="https://github.com/ZipCPU/eth10g/blob/fb19d04f46d5bb485020e2d8664c5606c6645612/rtl/net/p66brxgears.v#L114-L138">alignment algorithm is fairly straightforward, and centers on an
alignment counter</a>.
We first assume some arbitrary shift will produce aligned data.  <a href="https://github.com/ZipCPU/eth10g/blob/fb19d04f46d5bb485020e2d8664c5606c6645612/rtl/net/p66brxgears.v#L124">If the two
control bits differ</a>,
<a href="https://github.com/ZipCPU/eth10g/blob/fb19d04f46d5bb485020e2d8664c5606c6645612/rtl/net/p66brxgears.v#L127">this counter is incremented by one</a>.
Once the counter sets the MSB, this particular shift is declared
to be aligned.  If the two control bits are the same–that is if they are
invalid, then <a href="https://github.com/ZipCPU/eth10g/blob/fb19d04f46d5bb485020e2d8664c5606c6645612/rtl/net/p66brxgears.v#L128-L129">the counter is decremented by three</a>.
Once the counter gets to zero, an alignment failure is declared and
the algorithm moves on to check the next potential alignment by <a href="https://github.com/ZipCPU/eth10g/blob/fb19d04f46d5bb485020e2d8664c5606c6645612/rtl/net/p66brxgears.v#L131-L136">incrementing
the shift amount</a>.</p>

    <p>From a rate standpoint, data comes in at 32b/clk, and leaves <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66brxgears.v">this
gearbox</a>
via an <a href="/doc/axi-stream.pdf">AXI stream protocol</a>
that requires <em>READY</em> to be held high.  There’s no room to support any
backpressure here.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">CDC</a> from 322.3MHz to 200MHz</p>

    <p>The problem with data coming in from the GTX PHY is that the received data
is on a derived clock.  As with any other externally provided clock, this
means that the incoming clock (after recovery) will appear to be <em>near</em>
its anticipated 10.3125GHz rate, but is highly unlikely to match any
internal reference we have to 10.3125GHz exactly.  Even when we divide it
down by 32x, it will only be somewhere <em>near</em> a local 322.3MHz reference.
For this reason, we want to cross from the per-channel clock rate to a
common rate that can be used within our design–one allowing some timing
overhead.  In this case, we convert to a 200MHz clock.</p>

    <p>The CDC is accomplished via a <a href="/blog/2018/07/06/afifo.html">basic asynchronous
FIFO</a>, with the exception that
the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">FIFO has been modified to recognize and propagate ABORT
signals</a>.</p>
  </li>
</ol>

<table align="center" style="float: left"><caption>Fig 8. Turning the scrambling off?</caption><tr><td><img src="/img/eth10g/scrambleon.svg" alt="" width="420" /></td></tr></table>

<ol>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p64bscrambler.v">Descrambler</a></p>

    <p>Embedding clock and data together, as described above, requires having
data that is sufficiently pseudorandom–otherwise there might not be enough
bit transitions to reconstruct the clock signal.  Likewise, if the
incoming data isn’t sufficiently random, the 66b frame detector might suffer
from “false locks” and fail to properly detect packet data.  The 10Gb
Ethernet specification describes a feedthrough scrambling algorithm that is
to be applied to the 64 data bits of every 66b code word.  Our first task,
therefore, before we can process the 66b code word is to remove this
scrambling.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p642pkt.v">Packet delimiter and fault detector</a></p>

    <p>The next step is to convert these descrambled 66b codewords into our
internal AXI network (AXIN) packet stream format.</p>

    <p>For those familiar with Ethernet, they may recall that the Ethernet
specification describes an <a href="https://en.wikipedia.org/wiki/Media-independent_interface#XGMII">XGMII “10Gigabit Media Independent
Interface”</a>,
and suggests this interface should be used to feed the link.  This project
didn’t use the XGMII interface at all.  Why not?  Because the 66b encoding
describes a set of either 64b data or 64b codewords.  As a result, it makes
sense to process these words 64b at a time–rather than the 32b at a time
used by the XGMII protocol.  Transitioning from 64b at a time to the
32b/clk of the XGMII protocol would require either processing data at
the original 322.3MHz, or doing another clock domain crossing.  It was just
easier to go straight from the 66b format of the 10Gb interface directly
to a 64b <a href="/blog/2022/02/23/axis-abort.html">AXIN</a> format.</p>

    <p>Following this converter, we are now in a standard packet format.
Everything from here until a (roughly) equivalent point in the transmit
path takes place in this AXIN packet protocol.</p>

    <p>While <a href="/blog/2022/02/23/axis-abort.html">AXIN</a>
backpressure is supported from here on out, any backpressure will
likely cause a dropped packet.</p>

    <p>This is the first place in our processing chain where an ABORT may be
generated.  Any ABORTs that follow in the receive chain will either be
propagated from this one, or due to subsequently detected errors in the
packet stream.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/dropshort.v">Cull short packets</a></p>

    <p>Our next two steps massage our data just a little bit.  This first step
drops any packets shorter than the Ethernet minimum packet length: 64Bytes.
These packets are dropped via the
<a href="/blog/2022/02/23/axis-abort.html">AXIN ABORT</a> signal.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/crc_axin.v">CRC Checking</a></p>

    <p>From here, we check packet CRCs.  Any packet whose CRC doesn’t match will
be dropped via an ABORT signal.</p>

    <p><a href="https://github.com/ZipCPU/eth10g/blob/01b6322d7950cf29035c6dfcfca077ebddf1669c/rtl/net/crc_axin.v#L124-L141">Although a basic CRC algorithm is quite straightforward</a>,
getting this algorithm to pass timing was a bit harder.  Remember, packets
arrive at 64b/clk, and can have any length.  Therefore, the four byte CRC
may be found on any byte boundary.  That means we need to be prepared to
check for any one of eight possible locations for a final CRC.</p>

    <p><a href="https://github.com/ZipCPU/eth10g/blob/01b6322d7950cf29035c6dfcfca077ebddf1669c/rtl/net/crc_axin.v">Our first attempt to check CRCs</a>
generated the correct CRC for each byte cut based upon the CRC from the
last byte cut.  This didn’t pass timing at 200MHz.</p>

    <p>So, let’s take a moment to analyze the basic CRC algorithm.  You can <a href="https://github.com/ZipCPU/eth10g/blob/01b6322d7950cf29035c6dfcfca077ebddf1669c/rtl/net/crc_axin.v#L124-L141">see
it summarized in Verilog here</a>.
It works based upon a register I’ll call the “fill” (it’s called “current”
in <a href="https://github.com/ZipCPU/eth10g/blob/01b6322d7950cf29035c6dfcfca077ebddf1669c/rtl/net/crc_axin.v#L124-L141">this Verilog summary</a>).
On each new bit, this “fill” is
shifted right by one.  The bit that then falls off the end of the register
is exclusive OR’d with the incoming bit.  If the result is a ‘1’, then a
32b value is added to the register, otherwise the result is left as a
straight shift.</p>

    <p>The important conclusion to draw from this is that the CRC “fill” register
is simply propagated via linear algebra over GF2.  <em>It’s just a linear
system!</em>  This is important to understand when working with any algorithm
of this type.</p>

    <p>Not only is this a linear system, but 1) the equations for each bit have
fixed coefficients, and 2) they are roughly pseudorandom.  That means that
it should be possible to calculate any bit in the next CRC value based upon
the previous fill and the new data using only 96 input bits, of which only
a rough half of them will be non-zero (due to the pseudorandomness of the
operator).  Exclusive OR’ing 48bits together can be done with only 12 LUT6s
and three levels of logic.  (My out-of-date version of Yosys maps this to
20 LUT6s and 4 LUT4s.)  My point is this: if you can convince your synthesis
tool to remap this into a set of independnt linear equations, each will
cheap and easy to implement.</p>

    <p>Rewriting the CRC checker so that it first transformed the problem into a
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/crc_eqn.v">linear equation set</a>,
turned out to be just the juice needed to pass timing at 200MHz.</p>

    <p>The result of this <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/crc_axin.v">check
module</a>
is that packets with failing CRCs will be ABORTed.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">Resize from 64b/clk to 128b/clk</a></p>

    <p>Since the system clock for this design is 100MHz, we need to cross clock
domains one more time.  This requires more parallelism, so we first increase
our packet width from 64b/clk to 128b/clk in preparation of this final
clock domain crossing.</p>

    <p>The <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">width converter</a>
used for this purpose has been designed to be very generic.  As a result,
you’ll find it used many times over in this design.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">CDC</a> from 200MHz to 100MHz – the bus clock speed</p>

    <p>The last step of our incoming packet processor is to
<a href="/blog/2017/10/20/cdc.html">cross clock domains</a>
from our intermediate clock speed of 200MHz to 100MHz.</p>
  </li>
</ol>

<p>You may notice two additional blocks in Fig. 7 that aren’t connected to the
data stream.  These
are outlined with a dashed line to indicate that they are optional.  I placed
the components in the chain because I have used a similar component in previous
designs.  These components might check the IP version, and potentially check
that the header checksum or the packet length matches the one arriving.
In other designs, these components would also verify that arriving packets
were properly addressed to <em>this</em> destination.  However, in
<a href="https://github.com/ZipCPU/eth10g">this project</a>, a choice was made early on
that these IP-specific components wouldn’t be required for an <em>Ethernet</em> (not
IP) switch, and so they have never been either built or integrated into the
design.</p>

<p>From here on out, all processing takes place via the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>.  It is
possible for a long packet to come through this portion of the interface, only
to be ABORTed right at the end.</p>

<h3 id="the-outgoing-tx-chain">The (Outgoing) TX Chain</h3>

<p>The other half of the digital front end takes packets incoming, via the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>,
and converts them to a set of 32b words for the GTX PHY.  As with
the receive processing chain, this is also accomplish in a series of discrete
steps.  Both halves are found within the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netpath.v">netpath.v</a>
module.</p>

<ol>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">CDC</a>
from 100MHz to 200MHz</p>

    <p>Our first step is to cross from the 100MHz bus clock speed to a 200MHz
intermediate clock speed.  This simply moves us closer to the clock we
ultimately need to be on, while reducing the amount of logic required on
each step.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">Resize</a> from 128b/clk to 64b/clk</p>

    <p>Our first step, once we cross back into the 200MHz clock domain, is to
move from 128b/clk back to 64b/clk.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktgate.v">Packet gate and FIFO</a></p>

    <p>One problem with Ethernet transmission is that there’s no way to pause an
outgoing packet.  If the data isn’t ready when it’s time to send, the
packet will need to be dropped.  In order to keep this from happening, I’ve
inserted what I call a “<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktgate.v">packet gate</a>”.  This component first
loads incoming data into an <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">AXIN FIFO</a>, and then it holds up each individual
packet until either 1) the entire packet has entered the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">FIFO</a>, or
2) the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">FIFO</a>
becomes full. (Remember, backpressure is fully supported on the input.)
This way we can have confidence, going forward, that any memory delays from
the <a href="/blog/2023/04/08/vpktfifo.html">virtual FIFO</a>
feeding us our data from DDR3 memory will not cause us
to drop packets.  For shorter packets, this is a guarantee.  For longer
packets, the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">FIFO</a>
just mitigates any potential problems.</p>
  </li>
  <li>
    <p>Spoiler</p>

    <p>According to the Ethernet standard, I should have a packet spoiler at my
next step.  This spoiler would guarantee that any packets that must be
ABORTed (for whatever reason) have failing CRCs.</p>

    <p>This spoiler is not (yet) a part of my design.  As a result, there is a
(low) risk of a corrupt packet crossing the interface and (somehow) having
a valid CRC on the other end.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pkt2p64b.v">Packet assembler</a></p>

    <p>At this point, it’s time to switch from the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>
back to the 66b/clk Ethernet network protocol.  This includes inserting idle
indications, link error (remote fault) indications, start and end of packet
indications, as well as the packet data itself.</p>

    <p>The big thing to remember here is that, once a packet enters this component,
the packet data stream cannot be allowed to run dry without corrupting the
outgoing data stream.</p>

    <p>The result of this stage is a 66b AXI stream, whose <em>VALID</em> signal must be
held high.  <em>READY</em> will not be constantly high, but will be adjusted as
necessary to match the speed of the ultimate transmit clock.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p64bscrambler.v">Scrambler</a></p>

    <p>Just like the incoming packet data, we need to apply the same feedthrough
scrambler to the data going out.</p>

    <p>The result of this stage remains a 66b AXI stream.  As with the prior
stage, <em>VALID</em> needs to be held high.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66btxgears.v">66/64b gearbox</a></p>

    <p>As I mentioned earlier, the Xilinx GTX interface was a challenge to use
and get working.  I only use, therefore, the 32b interface as either a
ISERDES or in this case an OSERDES type of operator.  That means I need
to move from 66b/clk to 32b/clk.  The first half of this conversion takes
place at the 200MHz clock rate, converting to 64b/clock.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">CDC</a> from 200MHz to 322.3MHz</p>

    <p>From here, we can cross from our intermediate clock rate to the clock rate
of the GTX transmitter.  This is done via a <a href="/blog/2018/07/06/afifo.html">standard asynchronous FIFO–such
as we’ve written about on this blog
before</a>.</p>
  </li>
  <li>
    <p>64/32b gearbox</p>

    <p>The last step in the digital front end processor is to switch from 64b
at a time to 32b at a time.  Every other clock cycle reads a new 64b from
the asynchronous FIFO and sends 32b of those 64b, whereas the other 32b are
sent on the next clock.</p>

    <p>Why not match the receive handler, and place the full 64/66b gearbox at
the 322.3MHz rate?  I tried.  It didn’t pass timing.  This two step approach,
first going from <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66btxgears.v">66b to
64b</a>,
and then from 64b to 32b is a compromise that works well enough.</p>
  </li>
</ol>

<p>That’s the two digital components of the PHY.  Ever after these two components,
everything takes place using the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>.</p>

<h2 id="the-switch">The Switch</h2>

<p>Now that we’ve moved to a common protocol, everything else can be handled via
more generic components.  This brings us to the core of the project, the 4x4
Ethernet switch function itself.  This switch function is built in two parts.
The first part is all about buffering incoming packets, and then routing them
to their ultimate destinations.  The second component, the routing algorithm,
we’ll discuss in the next section.</p>

<p>The challenge of the switch is simply that packets may arrive at any time on
any of our interfaces, and we want to make sure those packets can be properly
routed to any outgoing interface–even if that outgoing interface is currently
busy.  While one approach might be to drop packets if the outgoing interface
is busy, I chose the approach in this project of instead trying to buffer
packets in memory via the <a href="/blog/2023/04/08/vpktfifo.html">virtual packet
FIFOs</a>.</p>

<ol>
  <li>
    <p>Notify all routing tables of the incoming MAC</p>

    <p>The first step in the switch is to grab the MAC source address from the
incoming, and to notify all of the per-port routing tables of this MAC
address.</p>

    <p>We’ll discuss the routing algorithm more in the next section.  It’s based
upon first observing the Ethernet MAC addresses of where packets come from,
and then routing packets to those ports when the addresses are known, or
to all ports when the destination port is unknown.</p>
  </li>
  <li>
    <p><a href="/blog/2023/04/08/vpktfifo.html">Virtual packet FIFOs</a></p>

    <p>The second step is to buffer the place the incoming packet into either a
<a href="/blog/2023/04/08/vpktfifo.html">Virtual packet FIFO</a>,
or (based upon a configuration choice) just run it through a block RAM
based <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">NetFIFO</a>.</p>

    <ul>
      <li>
        <p>Resize from 128b/clk to 512b/clk</p>

        <p>Pushing all incoming packets into memory requires some attention be paid
to memory bandwidth.  At 128b/clk, there’s not enough bandwidth to push
more than one incoming packet to memory, much less to read it back out
again.  For this reason, the DDR3 SDRAM width was selected to be wide
enough to handle 512b/clock, or (equivalently) just over four incoming
packets at once.</p>

        <p>Sadly, that means we need to convert our packet width again.  This time,
we convert from 128b/clk to 512b/clk.</p>
      </li>
      <li>
        <p>Incoming <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">NetFIFO</a></p>

        <p>SDRAM memory is not known for its predictable timing.  Lots of things can
happen that would make an incoming packet stream suffer.  It could be that
the CPU is currently using the memory and the network needs to wait.  It
might be that the memory is in the middle of a refresh cycle, and so all 
users need to wait.  To make sure we can ride through any of these delays,
the first step is to push the incoming packet into a <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">block-RAM based
AXIN FIFO</a>.</p>
      </li>
      <li>
        <p>Write packets to memory, then prefixing it with a 32b length word</p>

        <p>Packet data is then written to memory.</p>

        <p>My <a href="/blog/2023/04/08/vpktfifo.html">Virtual packet FIFO</a>
implementations work on a 32b alignment.  This means that the incoming
packet data may need to be realigned to avoid overwriting valid data that
might already be in the FIFO.</p>

        <p>Once the entire packet has been written to memory, the 32b word following
is set to zero and the 32b word preceding the packet is then written with
the packets’ size.  This creates sort of a linked-list structure in memory,
but one that only gets updated once a completed packet has been written to
memory.</p>

        <p>If the incoming packet is ever ABORTed, then the length word for the packet
is kept at zero, and the next packet is just written to the place this
one would’ve been written to.</p>

        <p>Once the packet size word has been written to memory, and the memory
has acknowledged it, the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">virtual packet FIFO writer</a>
then notifies the reader of its new write pointer.  This signals to the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>
that a new packet may be available.</p>
      </li>
      <li>
        <p>Read packets back from memory, length word first</p>

        <p>Once a packet has been written to memory, and hence once the write pointer
changes, that packet can then be read from memory.  This task works by
first reading the packet length from memory, and then reading that many
bytes from memory to form an outgoing packet.</p>

        <p>One thing to beware of is that
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
has no ability to offer read
backpressure on the bus.  (AXI has the ability, but for performance and
potential <a href="https://en.wikipedia.org/wiki/Deadlock">deadlock</a>
reasons it really shouldn’t be used.)  This means
that the part of the bus handler requesting data needs to be very aware
of the number of words both requested by the bus as well as those contained
in the synchronous FIFO to follow.  Bus requests should not be issued
unless room in the subsequent FIFO can be guaranteed.</p>

        <p>I’ve also made an attempt to guarantee that FIFO performance will be
handled on a <em>burst</em> basis.  Hence,
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
requests will not be made
unless the FIFO is less than half full, and they won’t stop being made
until the FIFO is either fully committed, or the end of the packet has
been reached.</p>
      </li>
      <li>
        <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/sfifo.v">Synchronous FIFO</a></p>

        <p>This is just a basic <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/sfifo.v">synchronous FIFO</a>
with nothing special about it.  It’s almost identical to the <a href="/blog/2017/07/29/fifo.html">FIFO I’ve
written about before</a>,
save that <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/sfifo.v">this one</a>
has been formally verified and gets used any time I need a FIFO.</p>
      </li>
      <li>
        <p>Width adjustment, from 512b/clk back to 128b/clk</p>

        <p>The last step in the <a href="/blog/2023/04/08/vpktfifo.html">virtual packet
FIFO</a> is to convert
the bit width back from 512b/clk to 128b/clk.</p>

        <p>Remember, 128b/clk is just barely enough to keep up at a 100MHz clock.
We needed the 512b/clk width to use the memory.  Now that we’re done with
the memory transactions, we can go back to the 128b/clk rate to lower
our design’s LUT count.</p>

        <p>From this point forward, we can support as much backpressure as necessary.
In many ways, this is the purpose of the
<a href="/blog/2023/04/08/vpktfifo.html">virtual packet FIFOs</a>:
allowing us to buffer unlimited packet sizes in memory, so that we can
guarantee full backpressure support following this point.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Get the packet’s destination MAC</p>

    <p>Once a packet comes back out of the
<a href="/blog/2023/04/08/vpktfifo.html">FIFO</a>, the next step
is to look up its MAC destination address.  <a href="https://en.wikipedia.org/wiki/Ethernet_frame">This can be found in the first
6 octets</a> of the packet.</p>
  </li>
  <li>
    <p>Look up a destination for this MAC</p>

    <p>We then send this MAC address to the routing algorithm, to look up where
it should be sent to.</p>
  </li>
  <li>
    <p>Broadcast the packet to all desired destinations</p>

    <p>The final step is to <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">broadcast</a>
this packet to all of its potential destination ports at once.</p>

    <p>This is built off of an <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">AXIN broadcast</a>
component to create many
<a href="/blog/2022/02/23/axis-abort.html">AXIN</a>
streams from one, followed by an
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinarbiter.v">AXIN arbiter</a>
used to select from many sources to determine which source should be
output at a given time.</p>
  </li>
</ol>

<p>That’s how most packets are handled.  Packets to and from the CPU are handled
differently.  Unlike the regular packet paths, the CPU’s virtual FIFO
involves the CPU.  This changes the logic for this path subtly.</p>

<ol>
  <li>
    <p>The CPUNet acts like its own port to the switch.  That means that,
internally, the switch is a 5x5 switch and not a 4x4 switch.  It still has
four physical ports, but it also has a virtual internal port going to
the CPU.</p>
  </li>
  <li>
    <p>The CPU port doesn’t require a
<a href="/blog/2023/04/08/vpktfifo.html">virtual packet FIFO</a>
within the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/routecore.v">router core</a>.  Instead, its
<a href="/blog/2023/04/08/vpktfifo.html">virtual packet FIFO</a>
is kept external to the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/routecore.v">router
core</a>.</p>
  </li>
  <li>
    <p>As a result, packets come straight in to the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/routecore.v">switch
component</a>,
get their MAC source recorded in the routing table, and then do all of
the route processing other packets go through, save that they don’t need
to through a <a href="/blog/2023/04/08/vpktfifo.html">virtual packet
FIFO</a> since they come
from <a href="/blog/2023/04/08/vpktfifo.html">a virtual packet
FIFO</a> in the first place.</p>
  </li>
  <li>
    <p>Likewise, in the reverse direction, packets routed to the CPU leave the
router and go straight to the CPU’s
<a href="/blog/2023/04/08/vpktfifo.html">virtual packet FIFO</a>.</p>
  </li>
  <li>
    <p>On entry, the CPU path has the option to filter out packets not addressed
to its MAC–whatever assignment it is given.</p>
  </li>
  <li>
    <p>However, for packet inspection and testing, this extra filter has often been
turned off.  Indeed, a special routing extension has been added to allow
the CPU to “see” all ports coming in, and so it can inspect any packet
going through the switch if bandwidth allows.  (If bandwidth doesn’t allow
this, packets in the switch may be dropped as well.)</p>
  </li>
</ol>

<h2 id="the-routing-algorithm">The Routing Algorithm</h2>

<p>The very first thing I built was the routing algorithm.  This I felt was the
core of the design, the soul and spirit of everything else.  Without routing,
there would be no switch.</p>

<p>The problem was simply that I’d never built a routing algorithm before.</p>

<p>My first and foremost goal, therefore, was to just build something that works.
I judged that I could always come back later and build something better.  Even
better, because this system is open source, released under the Apache 2.0
license, someone else is always welcome to come back later and build something
better.  That’s how open source worked, right?</p>

<p>So let’s go through the basic requirements.</p>

<ol>
  <li>
    <p>All packets must be routed.</p>
  </li>
  <li>
    <p>Hence, if the router can’t tell which destination to send a packet to, 
then that packet should be broadcast to all destinations.</p>
  </li>
  <li>
    <p>Packets should not be looped back upon their source.</p>
  </li>
  <li>
    <p>Packets sent to broadcast addresses must be broadcast.</p>
  </li>
</ol>

<p>In hindsight, I should have built the table with some additional requirements:</p>

<ol start="4">
  <li>
    <p>It should be possible for the internal CPU to read the state of the table at
any time.</p>
  </li>
  <li>
    <p>The CPU should be able to generate configure fixed routes.</p>

    <p>These include both routes where all packets from a given port go to a
given destination port or set of ports, but also routes where a given MAC
address goes to a given port.</p>
  </li>
  <li>
    <p>In hindsight, my routing algorithm used a lot of internal logic resources.
Perhaps a better solution might be to share the routing algorithm across
ports.  I didn’t do that.  Instead, each port had its own routing table.</p>
  </li>
</ol>

<p>Given these requiremnts, I chose to build the routing algorithm around an
internal <em>routing table</em>.</p>

<table align="center" style="float: right"><caption>Fig 9. Routing table columns</caption><tr><td><img src="/img/eth10g/routetbl.svg" alt="" width="480" /></td></tr></table>

<p>Each network port was given its own routing table.  As packets arrived,
the source MAC from the packet was isolated and then a doublet of source
<code class="language-plaintext highlighter-rouge">[PORT, MAC]</code> was forwarded to the routing tables in the design
associated with all of the other ports.</p>

<p>Each table entry has four components:</p>

<ul>
  <li>
    <p>Each entry has a <em>valid</em> flag.</p>

    <p>When a new source MAC doublet arrives, it will be placed into the first
table entry without a valid entry within it.  That entry will then become
valid.</p>
  </li>
  <li>
    <p>Each entry has a <em>MAC address</em> associated with it.  This is the source address
of the packet used to create this entry.</p>
  </li>
  <li>
    <p>Each entry has a <em>port number</em> associated with it.  This is the number of the
port a packet with the given source address last arrived at.</p>
  </li>
  <li>
    <p>Each entry has an <em>age</em>.  (It’s really a timeout value …)</p>

    <ul>
      <li>
        <p>The age of any new entry to the table is given this timeout value.  The
timeout then counts down by one every clock cycle.</p>
      </li>
      <li>
        <p>If a MAC declaration arrives for an existing entry, its timeout is reset.
It will then start counting down from its full timeout value.</p>
      </li>
      <li>
        <p>If 1) a new MAC declaration arrives for an entry that isn’t in the table,
and , and 2) all entries are full, then 4) the oldest entry will be
rewritten with this new entry.</p>

        <p>Calculating “oldest” turned out to be one of the more difficult parts of
this algorithm.</p>
      </li>
      <li>
        <p>After a period of time, if no packets arrive from a particular source,
then the entry will die of old age.</p>

        <p>This allows us to accommodate routes that may need to change over time.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Note that no effort is made to sort this table one way or other.</p>

<p>Now that we have this table, we can look up the destination port for a given
packet’s destination MAC address as follows:</p>

<ol>
  <li>
    <p>If the packet’s destination MAC matches one from the table, the packet will
then be sent sent to the port associated with that MAC.</p>

    <p>This basically sends packets to the last source producing a packet with that
MAC address.  This will fail if a particular source sends to a particular
destination, but that destination never responds.</p>

    <p>Still, this constitutes a successful MAC table lookup.</p>
  </li>
  <li>
    <p>If the destination MAC can’t be found in the table, then it will be
forwarded to all possible destinations.</p>

    <p>This constitutes a failed MAC lookup.  In the worst case, this will increase
network traffic going out by a factor of 4x in a 4x4 router.</p>
  </li>
</ol>

<p>This algorithm made a workable draft algorithm, up and until the network links
needed to be debugged.</p>

<p>Just to make certain everything was working, the hardware was set up with the
bench-top configuration shown in Fig. 10.</p>

<table align="center" style="float: right"><caption>Fig 10. Testing configuration</caption><tr><td><img src="/img/eth10g/netbench.svg" alt="" width="420" /></td></tr></table>

<ul>
  <li>
    <p>Ports #0 and #1 were connected in a loopback fashion.  Packets sent to port
#0 would be received at port #1, and vice versa–packets sent to port #1
would be received at port #0.</p>
  </li>
  <li>
    <p>Port #2 was connected to an external computer via a coaxial cable.  We’ll
call this PC#2.</p>
  </li>
  <li>
    <p>Port #3 was connected to an external computer via an optical fiber.  We’ll
call this PC#3.</p>
  </li>
</ul>

<p>Now, let me ask, what’s that loopback going to do to our routing algorithm?
Packets arriving on interface #2 for an unknown destination (PC#3’s address)
will then be forwarded to interfaces #0 and #1 in addition to port #3.
Ports #0 and #1 will then generate two incoming packets to be sent to the same
unknown MAC address (PC#3’s address), which will then cause our table to learn
that the MAC address generated by PC#2 comes from either port #0 or port #1.
(A race condition will determine which port it gets registered to.)  This
is also going to flood our virtual FIFOs with a never ending number of packets.</p>

<p>This is not a good thing.</p>

<p>I tried a quick patch to solve this issue.  The patch involved two new
parameters, <code class="language-plaintext highlighter-rouge">OPT_ALWAYS</code> and <code class="language-plaintext highlighter-rouge">OPT_NEVER</code>.  Using these two options, I was
able to adjust the outgoing port so that it read:</p>

<figure class="highlight"><pre><code class="language-verilog" data-lang="verilog">	<span class="k">parameter</span>	<span class="n">NETH</span><span class="o">=</span><span class="mi">4</span><span class="p">;</span>		<span class="c1">// Number of ports</span>
	<span class="k">parameter</span> <span class="p">[</span><span class="n">NETH</span><span class="o">-</span><span class="mi">1</span><span class="o">:</span><span class="mi">0</span><span class="p">]</span>	<span class="n">OPT_NEVER</span>  <span class="o">=</span> <span class="mh">4'h3</span><span class="p">;</span>
	<span class="k">parameter</span> <span class="p">[</span><span class="n">NETH</span><span class="o">-</span><span class="mi">1</span><span class="o">:</span><span class="mi">0</span><span class="p">]</span>	<span class="n">OPT_ALWAYS</span> <span class="o">=</span> <span class="mh">4'h0</span><span class="p">;</span>
	<span class="c1">//</span>
	<span class="c1">// ...</span>
	<span class="c1">//</span>

	<span class="n">TX_PORT</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">lkup_port</span> <span class="o">&amp;</span> <span class="p">(</span><span class="o">~</span><span class="n">OPT_NEVER</span><span class="p">))</span> <span class="o">|</span> <span class="n">OPT_ALWAYS</span><span class="p">;</span></code></pre></figure>

<p>I could then use this to keep my design from forwarding packets to
port #0 or #1.</p>

<p>It wasn’t good enough.</p>

<p>For some reason, I was able to receive ARP requests from either ports #2
or #3, but they would never acknowledge any packets sent to them.  So … I
started instrumenting everything.  I wondered if things were misrouted,
so I tested sending packets everywhere arbitrarily.  Suddenly, ports #2
and #3 started acknowledging pings!  But when I cleaned up my arbitrary
routing, they no longer acknowledged pings anymore.</p>

<p>This lead me into a cycle of adjusting <code class="language-plaintext highlighter-rouge">OPT_NEVER</code> and <code class="language-plaintext highlighter-rouge">OPT_ALWAYS</code> over
and over again, and rebuilding every time.  Eventually, this got so painful
that I turned these into run-time configurable registers.  (Lesson learned …)</p>

<p>I’ll admit, I started getting pretty frustrated over this one bug.  The design
worked in simulation.  I could transmit packets from port #0 in simulation to
port #3, going through the router, so I <em>knew</em> things should work–they just
weren’t working.  Like any good engineer, I started blaming the PCB designer
for miswiring the board.  To do that, though, I needed a good characterization
of what was taking place on the board.  So I started meticulously running
tests and drawing things out.  This lead to the following picture:</p>

<table align="center" style="float: none"><caption>Fig 11. The nightmare routing bug</caption><tr><td><img src="/img/eth10g/20231122-nightmare.svg" alt="" width="780" /></td></tr></table>

<p>This showed that packets sent to either ports #2 or #3 weren’t arriving.  If
I used the CPU to send to port #0, I could see the result on port #1.  However,
if I sent to port #1, nothing would be received on port #0.  Just to make
things really weird, if I sent to port #0 then ports #3 and #2 would see the
packet, but I couldn’t send packets to ports #2 or #3.</p>

<p>Drawing the figure above out really helped.  Perhaps you can even see the bug
from the figure.</p>

<p>If not, here it was: I used <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/xgtxphy.v">one module to control all four GTX
transceivers</a>.
This module had, as its input therefore, a value <code class="language-plaintext highlighter-rouge">S_DATA</code> containing 32-bits
for every port.  This module was not subject to lint testing, however, since
… the GTX PHY couldn’t be Verilated–or I might’ve noticed what was going
on.  I was then forwarding bits [63:0] of <code class="language-plaintext highlighter-rouge">S_DATA</code> to <em>every</em> outgoing port,
rather than 32-bits of zero followed by bits <code class="language-plaintext highlighter-rouge">[32*gk +: 32]</code> of <code class="language-plaintext highlighter-rouge">S_DATA</code> to
port <code class="language-plaintext highlighter-rouge">gk</code>.  Because my network bench tests were limited to only look at
selected outputs, I then never noticed this issue.  The result was that anything
sent to port #0 would be sent to <em>all</em> ports: #0-#3, and anything sent to
ports #1-#3 would go nowhere within the design.</p>

<p>So let me apologize now to the PCB designer for this board.  This one was my
bug after all.</p>

<p>Let me also suggest that this bug would’ve been much easier to find if I had
designed the routing algorithm from the beginning so that I could test specific
hardware paths.  The CPU, for example, should be able to override any and all
routing paths.  Likewise, the CPU should be able to read (and so verify) any
current routing paths.</p>

<p>A second problem with the router algorithm was that it consumed too many
resources.  The routing tables were first designed to have 64 entries in each
of the four tables.  When space got tight, this was dropped to 32 entries,
and then to 16, and then down to 8.  Any rewrite of this algorithm should
therefore address the space used by the table–in addition to making it easier
for the CPU to read and modify the table at will.</p>

<h2 id="the-special-bus-arbiter">The Special Bus Arbiter</h2>

<p>This design required a special
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
arbiter.  To explain this need, though, I’ll need to do a little bit of math.</p>

<p>Suppose a port is receiving data at 10Gb/s.  By the time we’ve adjusted the
data down so that the stream is moving 512b per clock at a 10ns clock period,
that means one 512b word will need to be written to memory 51.2ns.</p>

<table align="center" style="float: left"><caption>Fig 12. Bus width.  We had to go wide</caption><tr><td><img src="/img/eth10g/buswidth.svg" alt="" width="360" /></td></tr></table>

<p>The typical DDR3 SDRAM access takes about 20 clock cycles of latency or so,
with longer latency requirements if the SDRAM requires either a refresh cycle
or a bank swapping cycle.  Bank swapping can be avoided if the SDRAM accesses
are burst together in a group–a good reason to use a FIFO, but there’s no easy
way to avoid refresh cycles.</p>

<p>That’s only the first problem.</p>

<table align="center" style="float: none"><caption>Fig 13. Wishbone can be very inefficient</caption><tr><td><a href="/img/eth10g/b4drom.svg"><img src="/img/eth10g/b4drom.svg" alt="" width="780" /></a></td></tr></table>

<p>The second problem is illustrated in Fig. 13 above.  In this diagram, the
<code class="language-plaintext highlighter-rouge">S1_*</code> signals are from the first source, while the
<code class="language-plaintext highlighter-rouge">S2_*</code> signals are from the second source.  The <code class="language-plaintext highlighter-rouge">M_*</code> signals would then go
to the downstream device–such as the DDR3 SDRAM or, in this case, a second
arbiter (shown above in Fig. 2) to then go to the DDR3 SDRAM.  (Yeah, I know,
there’s a <em>lot</em> of levels to bus processing …)</p>

<p>The problem in this illustration is associated with how
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
normally works: <code class="language-plaintext highlighter-rouge">CYC &amp;&amp; STB</code> are raised on the first beat of any transaction
from a given master, <code class="language-plaintext highlighter-rouge">STB</code> is dropped once all requests have been accepted,
and then <code class="language-plaintext highlighter-rouge">CYC</code> is dropped once all <code class="language-plaintext highlighter-rouge">ACK</code>s have been received.  The arbiter
then takes a cycle to know that <code class="language-plaintext highlighter-rouge">CYC</code> has dropped, before allowing the
second master to have access to the bus.  This creates a great inefficiency,
since the master isn’t likely to make any requests between when it drops <code class="language-plaintext highlighter-rouge">STB</code>
and when it drops <code class="language-plaintext highlighter-rouge">CYC</code>.  Given what we know of DDR3 SDRAM, this inefficiency
could cost about 200ns per access.</p>

<p>However, if all four ports are receiving at the same time, then 512b will
need to be written four times every 51ns, from each of four masters.</p>

<p>This requirement was impossible to meet with my normal
<a href="/blog/2019/07/17/crossbar.html">crossbar</a>.
That <a href="/blog/2019/07/17/crossbar.html">crossbar</a>
would wait until a master dropped <code class="language-plaintext highlighter-rouge">CYC</code>, before allowing a second
master to access the bus–suffering the inefficiency each time.  This is all
illustrated in Fig. 13 above.</p>

<p>The solution that I’ve chosen to use is to create a <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/wbmarbiter.v">special Wishbone
master arbiter</a>:
one that multiplexes wishbone accesses together from multiple masters to a
single slave.  This way I could transition from one master’s bus requests to
a second master’s bus requests, and then just route the return data to the
next master looking for return data.</p>

<p>You can see how this might save time in Fig. 14 below.</p>

<table align="center" style="float: none"><caption>Fig 14. Merging WB requests within the same cycle</caption><tr><td><a href="/img/eth10g/aftrdrom.svg"><img src="/img/eth10g/aftrdrom.svg" alt="" width="780" /></a></td></tr></table>

<p>Notice how, in this figure, the requests get tightly packed together going to
memory.</p>

<p>(Yes, this is what AXI was designed to do naturally, even though my AXI
<a href="/blog/2019/07/17/crossbar.html">crossbars</a> don’t yet do
this.)</p>

<p>There are some consequences to this approach, however, that really keep
it from being used generally.  One consequence is the loss of the ability
to lock the bus and do atomic transactions by simply holding <code class="language-plaintext highlighter-rouge">CYC</code> high.
(The virtual packet FIFOs don’t need atomic transactions, so this isn’t
an issue.)
A second consequence is in error handling.  Because the
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> bus aborts
any outstanding transactions on any error return, all outstanding requests
need to be aborted should any master receive a <a href="https://en.wikipedia.org/wiki/Bus_error">bus
error</a> return.  As a result, all
masters will report a <a href="https://en.wikipedia.org/wiki/Bus_error">bus error</a>
return even if only one master caused the
<a href="https://en.wikipedia.org/wiki/Bus_error">error</a>.</p>

<p>Given that the <a href="/blog/2023/04/08/vpktfifo.html">virtual packet
FIFO</a>
bus masters would only ever interact with memory, this solution works in this
circumstance.  I may not be able to use it again.</p>

<h2 id="reusable-components">Reusable Components</h2>

<p>We’ve discussed <a href="/blog/2020/01/13/reuse.html">hardware reuse</a>
on this blog before, and we’ll probably come back and do so again.  I’ve said
before, I will say again: well designed, well verified IP is gold in this
business.  It’s gold because it can be reused over and over again.</p>

<table align="center" style="float: left"><caption>Fig 15. Good as gold</caption><tr><td><img src="/img/eth10g/ipgold.svg" alt="" width="360" /></td></tr></table>

<p>As examples, the network portion of this project alone has reused many
components,
to include the <a href="/about/zipcpu.html">ZipCPU</a>, my <a href="/blog/2018/07/06/afifo.html">asynchronous
FIFO</a>, my <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/sfifo.v">synchronous
FIFO</a>,
my <a href="/blog/2019/05/22/skidbuffer.html">skidbuffer</a>, and my
<a href="/blog/2019/07/17/crossbar.html">Wishbone crossbar</a>.  If you
look over the bus diagram in Fig. 2 above, you’ll also see many other
components that may have either been reused, or are likely to be reused
again–to include my <a href="/blog/2019/03/27/qflexpress.html">Quad SPI flash
controller</a>,
<a href="/blog/2018/11/29/llvga.html">HDMI Controller(s)</a>,
<a href="https://github.com/ZipCPU/wbicapetwo">ICAPE2 controller</a>, <a href="/blog/2021/11/15/ultimate-i2c.html">I2C
Controller</a>, and now my
new <a href="/formal/2023/07/18/sdrxframe.html">SD-Card Controller</a>–but
I’m trying to limit today’s focus on the network specific components for now.</p>

<table align="center" style="float: right"><caption>Fig 16. A list of my various network designs</caption><tr><td><img src="/img/eth10g/netprojs.svg" alt="" width="520" /></td></tr></table>

<p>Sadly, this is my fourth network design and many of the components from my
first three network designs cannot be reused.  My first network design used
a 100M/s link.  The next two network designs were based off of a 1Gb/s
Ethernet, and so used either 8b/clk or 32b/clk–depending on which clock was
being used.  Any component specific to the bit-width of the network could
(sadly) not be reused.  Likewise, the first two of these three previous designs
didn’t use the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>–limiting
their potential reuse.</p>

<table align="center" style="float: left"><caption>Fig 17. What is a "store+notify" design?</caption><tr><td><img src="/img/eth10g/storenotify.svg" alt="" width="420" /></td></tr></table>

<p>Perhaps this is to be expected.  The first time you design a solution to a
problem, you are likely to make a lot of mistakes.  The second time you’ll
get a lot closer.  The third time even closer, etc.  This is one of the reasons
I like to tell students to “Plan for failure.”  Why?  Because “success” always
comes after failure, and sometimes many “failures” are required to get there.
Therefore, you need to start projects early enough that you have time to get
past your early failures.  Hence, you should always plan for these failures
if you wish to succeed.</p>

<p>That said, this <a href="/blog/2022/02/23/axis-abort.html">AXIN
protocol</a> was enough of a
success on my third network project that I’m likely to use it again and again.
The fact that I chose to use it here attests to the fact that it worked well
enough on the last project that it didn’t need to be rebuilt at all.</p>

<p>That also means I’m likely going to be using and reusing many of the generic
<a href="/blog/2022/02/23/axis-abort.html">AXIN</a> components from this
project to the next.  These include:</p>

<ul>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">NetFIFO</a></p>

    <p>This is a basic FIFO, but for the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>.
Packets that have been completely received by the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">FIFO</a>
will not be aborted–no matter how the master adjusts the ABORT signal.
What makes this component different from other network FIFOs I’ve seen is
that the ABORT signal may still propagate from the input to the output prior
to a full packet being recorded in the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">FIFO</a>.
This means that <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">this particular network FIFO implementation</a>
is able to handle packet sizes that might be much larger than the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">FIFO</a> itself.</p>

    <p>Although <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">my particular
implementation</a>
doesn’t include the <a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">AXIN
<code class="language-plaintext highlighter-rouge">BYTES</code></a> signal, the data
width can be adjusted to include it alongside the data signal for no loss of
functionality.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netskid.v">NetSkid</a></p>

    <p>This is a version of your
<a href="/blog/2019/05/22/skidbuffer.html">basic skidbuffer</a>,
but this time applied to the
<a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>.
As a result, it supports the ABORT signal, but in all other ways it’s just a
basic <a href="/blog/2019/05/22/skidbuffer.html">skidbuffer</a>.</p>

    <p>This particular component was easy enough to design, and it would make a good
classroom exercise for the student to design his own.</p>
  </li>
  <li>
    <p><a href="/blog/2023/04/08/vpktfifo.html">Virtual Packet FIFOs</a>.</p>

    <p>The most challenging component to verify, and hence to build, has been the
<a href="/blog/2023/04/08/vpktfifo.html">virtual Packet FIFOs</a>.
On the other hand, these are <em>very</em> versatile, and so I’m likely to use them
again and again in the future.  This verification work, therefore, has been
time well spent.  You can read more about virtual packet FIFOs, what they are
and how they work, in <a href="/blog/2023/04/08/vpktfifo.html">this article</a>.</p>

    <p>As of writing this, my
<a href="/blog/2023/04/08/vpktfifo.html">virtual Packet FIFO</a>.
work isn’t quite done.  There’s still a bit of a signaling issue, and the
formal proof of the memory reader portion isn’t passing.  However, the
component is working in hardware so this is something I’ll just have to
come back to as I have the opportunity.</p>

    <p>Once completed, the only real upgrade potential remaining might be to
convert these virutal FIFOs to AXI.  Getting <a href="/blog/2020/06/16/axiaddr-limits.html">AXI
bursting</a> right
won’t be easy, but it would certainly add value to my <a href="https://github.com/ZipCPU/wb2axip">AXI
library</a> to have such a component within
it.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">Width converter</a></p>

    <p>The <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">AXIN width converter</a>
will likely be reused.  It should be generic enough
to convert <a href="/blog/2022/02/23/axis-abort.html">AXIN streams</a>
from any power of two byte width to any other.  That
means it will likely be reused on my next network project again as well.
It’s just necessary network infrastructure, so that’s how it will be used
and reused.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktgate.v">Packet Gate</a></p>

    <p>This component buffers either a whole packet, or fills its buffer with a
full packet before releasing the packet downstream.  It’s really nothing more
than a <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">network
FIFO</a>
with some additional logic added.  Still, this one is
generic enough that it can be reused in other projects as necessary.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/dropshort.v">Dropping short packets</a></p>

    <p>I’ve now written multiple components that can/will drop short packets.
This new one, however, should be versatile enough to be able to be reused
across projects.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/crc_axin.v">CRC checker</a></p>

    <p>As with the short packet detector, my CRC checker should work nicely across
projects–even when the widths are different.  This will then form the
basis for future network CRC projects.</p>
  </li>
</ul>

<table align="center" style="float: right"><caption>Fig 18. The AXIN broadcaster and AXIN arbiter work together</caption><tr><td><img src="/img/eth10g/bcastarb.svg" alt="" width="480" /></td></tr></table>

<ul>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">AXIN Broadcaster</a></p>

    <p>The <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">AXIN broadcasting component</a>
is only slightly modified from my <a href="https://github.com/ZipCPU/wb2axip/blob/master/rtl/axisbroadcast.v">AXI stream
broadcaster</a>.  It’s a simple component: an
<a href="/blog/2022/02/23/axis-abort.html">AXIN stream</a>
comes in, specifying one (or more) destinations that this <a href="/blog/2022/02/23/axis-abort.html">AXIN
stream</a> should be
broadcast to.  The broadcaster then sends a beat to each
<a href="/blog/2022/02/23/axis-abort.html">AXIN stream</a>
and, as beats are accepted from all outgoing streams, sends more beats.</p>

    <p>Or, rather, that was the initial design.</p>

    <p>As it turns out, this approach had a serious design flaw in it contributing
to an honest to goodness, bona-fide
<a href="https://en.wikipedia.org/wiki/Deadlock">deadlock</a>.  As a result, both this
component, and the corresponding arbiter that follows it, needed fixing.</p>

    <p>The new/updated algorithm includes two non-<a href="/blog/2022/02/23/axis-abort.html">AXIN</a> signals:
CHREQ (channel request, from the master) and <code class="language-plaintext highlighter-rouge">ALLOC</code> (channel allocated,
returned from the slave).  When “broadcasting” to a single port, <code class="language-plaintext highlighter-rouge">CHREQ</code>
will equal <code class="language-plaintext highlighter-rouge">VALID</code>.  When broadcasting to multiple ports, it will first set
this <code class="language-plaintext highlighter-rouge">CHREQ</code> signal to indicate each of the downstream ports it is attempting
to broadcast to.  This will be done before setting <code class="language-plaintext highlighter-rouge">VALID</code> to forward any
data downstream.  The downstream ports, on seeing a <code class="language-plaintext highlighter-rouge">CHREQ</code> signal, will
raise the <code class="language-plaintext highlighter-rouge">ALLOC</code> (allocated channel) return signal if they aren’t busy.
The design then waits for <code class="language-plaintext highlighter-rouge">ALLOC</code> to match <code class="language-plaintext highlighter-rouge">CHREQ</code>.  Once the two match, the
packet will be forwarded.  If, however, after 64 clock cycles, enough
channels haven’t been allocated to match the channel requests, then we may
have detected a <a href="https://en.wikipedia.org/wiki/Deadlock">deadlock</a>.  In this
case, all requests will be dropped for a pseudorandom time period before
<code class="language-plaintext highlighter-rouge">CHREQ</code> will be raised to try again.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinarbiter.v">AXIN Arbiter</a></p>

    <p>As shown in Fig. 18 above, the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinarbiter.v">AXIN
arbiter</a>
is the other side of the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">AXIN broadcaster</a>.
Whereas the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">AXIN
broadcaster</a>
takes a single <a href="/blog/2022/02/23/axis-abort.html">AXIN
stream</a> input and
(potentially) forwards it to multiple output streams, the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinarbiter.v">arbiter</a>
takes multiple
<a href="/blog/2022/02/23/axis-abort.html">AXIN</a> input streams
and selects from among them to source a single output stream.</p>

    <p>As with the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">broadcaster</a>,
the original design was quite simple: select from among many packet sources
based upon the <code class="language-plaintext highlighter-rouge">VALID</code> signal, and then hold on to that selection until
<code class="language-plaintext highlighter-rouge">VALID &amp;&amp; READY &amp;&amp; LAST</code>.</p>

    <p>The <a href="https://en.wikipedia.org/wiki/Deadlock">deadlock</a>
occurred when at least two simultaneous attempts were made to broadcast
packets to two (or more) separate destinations, as shown in FIG:.</p>
  </li>
</ul>

<table align="center" style="float: none"><caption>Fig 19. Deadlock!</caption><tr><td><img src="/img/eth10g/deadlock.svg" alt="" width="780" /></td></tr></table>

<p>If one destination arbitrated to allow the first source to transmit but the
  second destination chose to allow the second source, then the whole system
  would lock up and fail.</p>

<p>Now, the
  <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinarbiter.v">arbiter</a>
  selects its incoming signal based upon <code class="language-plaintext highlighter-rouge">CHREQ</code>, and then
  sets the <code class="language-plaintext highlighter-rouge">ALLOC</code> return signal once arbitration has been granted to let
  the source know it has arbitration.  Arbitration is then lost once <code class="language-plaintext highlighter-rouge">CHREQ</code>
  is dropped–independent of whether a packet has (or has not) completed.
  In this way, a failed arbitration (i.e. detected
  <a href="https://en.wikipedia.org/wiki/Deadlock">deadlock</a>)
  can be dropped and try again before any packet information gets lost.</p>

<ul>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">AXIN CDC</a></p>

    <p>As long as there are multiple clocks in a design, there will be a need for
<a href="/blog/2017/10/20/cdc.html">crossing clock domains</a>.
This is uniquely true when there’s a difference a data input clock, data
output, and memory clock domains.</p>

    <p><a href="/blog/2017/10/20/cdc.html">Crossing clock domains</a>
with the <a href="/blog/2022/02/23/axis-abort.html">AXIN protocol</a>
isn’t as pretty as when using the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">network
FIFO</a>.
Indeed, there’s no easy way to ABORT and free the FIFO resources of a packet
that’s been only partially accepted by an <a href="/blog/2018/07/06/afifo.html">asynchronous
FIFO</a>.
Instead, the ABORT signal is converted into a data wire and simply forwarded
as normal through a <a href="/blog/2018/07/06/afifo.html">standard asynchronous
FIFO</a>.
It’s a simple enough approach, but it does nothing to free up
FIFO resources on an ABORTed packet.  However, freeing up FIFO resources
can still be (mostly) accomplished by placing a small
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">AXIN CDC</a>
(i.e. an <a href="/blog/2018/07/06/afifo.html">asynchronous
FIFO</a>)
back to back with the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">standard network
FIFO</a>.
discussed above.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/wbmarbiter.v">WBMArbiter (Wishbone master arbiter)</a></p>

    <p>I mentioned this one in the last section.  Although it’s not unique to
networking, I may yet use this component again.</p>
  </li>
</ul>

<table align="center" style="float: left"><caption>Fig 20. Gearboxes can (and should) be formally verified</caption><tr><td><img src="/img/eth10g/frmlgearbox.svg" alt="" width="420" /></td></tr></table>

<p>There is another item worth mentioning in this list, and those are the two
gearboxes.
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66brxgears.v">[1]</a>,
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p66btxgears.v">[2]</a>.
I’m not listing these with the reusable components above, simply
because I’m not likely to use these particular gearboxes outside of a 64/66b
encoding system, so they aren’t really all that generic.  However, the
lessons learned from building them, together with the methodology for
formally verifying them, is likely something I’ll remember long into the
future when building gearboxes of other ratios.</p>

<h2 id="lessons-learned">Lessons Learned</h2>

<p>Perhaps the biggest lesson (re)learned during this design is that you need
to plan for debugging from the beginning.  This includes some of the following
lessons:</p>

<table align="center" style="float: right"><caption>Fig 21. For debugging purposes, count all ABORTs</caption><tr><td><img src="/img/eth10g/abortctr.svg" alt="" width="420" /></td></tr></table>

<ol>
  <li>
    <p>Anytime a component generates an ABORT signal, that should be logged and
counted somewhere.  This is different from components propagating an ABORT
signal.</p>
  </li>
  <li>
    <p>Testing requires that the router have static paths, at least until the
rest of the network interface works.</p>

    <p>Port forwarding can also permit the CPU to inspect any packet crossing
through the interface.</p>
  </li>
  <li>
    <p>Bit-order in the ethernet specification is quite confusing.</p>

    <p>I had to build and then rebuild the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/p642pkt.v">66b to AXIN protocol
component</a>
and its <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pkt2p64b.v">transmit
counterpart</a>
many times over during this project–mostly because of misunderstandings
of the Ethernet specification.  This included confusions over how bytes
are numbered within the 66b message word, the size of control bytes (7b) in
66b message words vs the size of data bytes (8b), whether the synchronization
pattern was supposed to be <code class="language-plaintext highlighter-rouge">10</code> for control words or <code class="language-plaintext highlighter-rouge">01</code> and so forth.
As a result, even though I was able to get the design working with GTX
transceivers in a 10Gb Ethernet test bench early on, I would later discover
more than once that the “working” test bench and design didn’t match
10Gb Ethernet hardware components on the market.</p>

    <p>For example, the Ethernet specification indicates that the start of packet
indication must include bits, <code class="language-plaintext highlighter-rouge">... 10101010 10101010</code> followed by the octet
<code class="language-plaintext highlighter-rouge">10101011</code>.  What it doesn’t make clear is that these bits need to be read
right to left.  This meant that the final three bytes of any start of
packet delimeter needed to be <code class="language-plaintext highlighter-rouge">0x55</code>, <code class="language-plaintext highlighter-rouge">0x55</code>, followed by <code class="language-plaintext highlighter-rouge">0xd5</code> rather
than (as I first read the specification) <code class="language-plaintext highlighter-rouge">0xaa</code>, <code class="language-plaintext highlighter-rouge">0xaa</code>, followed by <code class="language-plaintext highlighter-rouge">0xab</code>.</p>
  </li>
  <li>
    <p>The routing algorithm above doesn’t work well in loopback situations.</p>

    <p>As I mentioned above, this particular problem turned into a debugging
nightmare.  Even now that I have a working algorithm, I’m not yet convinced
it does a good job handling this reality.</p>
  </li>
  <li>
    <p>Look out for <a href="https://en.wikipedia.org/wiki/Deadlock">deadlocks</a>!</p>

    <p>Others who have dealt with
<a href="https://en.wikipedia.org/wiki/Deadlock">deadlocks</a> before have told me
that 1) <a href="https://en.wikipedia.org/wiki/Deadlock">deadlocks</a> are system
architecture level problems, and 2) that they are easy enough to spot
after you’ve had enough experience with them.  In my case, I studied them
back in my college days, but had never seen a
<a href="https://en.wikipedia.org/wiki/Deadlock">deadlocks</a> in real life before.</p>
  </li>
</ol>

<table align="center" style="float: left"><caption>Fig 22. Deadlocks are very real</caption><tr><td><img src="/img/eth10g/bigfoot.svg" alt="" width="420" /></td></tr></table>

<ol>
  <li>Because the routing tables were written entirely in RTL using Flip-Flops,
and not using any block RAM, the CPU has no ability to read the tables back
out to verify their functionality.</li>
</ol>

<p>Finally, I’ve really enjoyed <a href="/blog/2022/02/23/axis-abort.html">this network (AXIN)
protocol</a>.  It’s met my
needs now on multiple projects.  Better yet, because I’ve made this protocol
common across these projects, I can now re-use components between them.</p>

<p>Perhaps that should be a final lesson learned as well: well designed internal
protocols facilitate design reuse.</p>

<h2 id="conclusion">Conclusion</h2>

<p>So the big question I imagine on everybody’s mind at this point is, now that
you have a 10Gb Ethernet switch, how well does it work?</p>

<p>Sadly, my answer to that (at present) is … I don’t know.</p>

<p>I’ve tested the design using <code class="language-plaintext highlighter-rouge">scp</code> to move files from one PC to another,
only to discover <code class="language-plaintext highlighter-rouge">scp</code> has internal speed limitations within it.  This leaves
me and the project needing better test cases.</p>

<p>How about the hardware?</p>

<p>As with any PCB design, the original PCB design for this project has had some
issues.  The work presented above has been done with the <em>original</em>, first/draft
board design.  Part of my task has been to build enough RTL design to identify
these issues.  As of the this date, I’ve now worked over all of the hardware
design save the SATA port.  Yes, there are some issues that will need to be
corrected in the next revision of the board.  These issues, however, weren’t
significant enough that they kept us from verifying the components on the board.
As a result, I can now say with certainty that the 10GbE components of the
board work–although, IIRC, we were going to change the polarity of some of
the LED controls associated with it.  Still, its enough to say it works.</p>

<p>The flash, SD card, and eMMC interfaces?  Those will need some redesign work,
and so they should be fixed on the next revision of the board.  As for the
10GbE interfaces?  Those work.</p>

<p>If you are interested in one of these boards to work with, then please
contact <a href="edmund@symbioticeda.com">Edmund, at Symbiotic EDA</a>, for details.</p>

<p>If the Lord wills, there are several components of this design that might
be fun to blog about.  These include the <a href="https://github.com/ZipCPU/eth10g/blob/master/bench/formal/faxin_slave.v">formal property
set</a>
that I’ve been using to verify AXIN components, the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netskid.v">AXIN
Skidbuffer</a>
or even the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">NetFIFO</a>.</p>

<p>For now, I’ll just note that the design is <a href="https://github.com/ZipCPU/eth10g">posted on
github</a>, licensed under Apache 2.0, and
I will invite others to examine it and make comments on it.</p>


  </div>


<div class "verse">
<HR align="center;" width="25%">
<P><em>This also cometh from the LORD of Hosts, which is wonderful in counsel, and excellent in working.  (Is 28:29)</em>


</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The ZipCPU by Gisselquist Technology</h2>
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <!-- <li></li> -->
          <li><a href="mailto:zipcpu@gmail.com">zipcpu@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="soc-medlist">
          
          <li>
            <a href="https://github.com/ZipCPU"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">ZipCPU</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/zipcpu"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">@zipcpu</span></a>

          </li>
          
          
          <li><A href="https://www.patreon.com/ZipCPU"><img src="/img/become_a_patron_button.png"></a></li>
          

        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The ZipCPU blog, featuring how to discussions of FPGA and soft-core CPU design.  This site will be focused on Verilog solutions, using exclusively OpenSource IP products for FPGA design.  Particular focus areas include topics often left out of more mainstream FPGA design courses such as how to debug an FPGA design.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
