<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>What is a Virtual Packet FIFO?</title>
  <meta name="description" content="I first came across virtual packet FIFOs in a SONARproject by necessity.The SONAR device’sonly means of communicating with the outside world wasvia Gb Ethern...">

  <link rel="shortcut icon" type="image/x-icon" href="/img/GT.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://zipcpu.com/blog/2023/04/08/vpktfifo.html">
  <link rel="alternate" type="application/rss+xml" title="The ZipCPU by Gisselquist Technology" href="https://zipcpu.com/feed.xml">
</head>


  <body>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4ZK7HKHSVW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4ZK7HKHSVW');
</script>

    <header class="site-header">
  <div id="banner">
  <a href="/"><picture>
    <img height=120 id="site-logo" src="/img/fullgqtech.png" alt="Gisselquist Technology, LLC">
  </picture></A>
  </div>

  <div class="site-nav">
<ul>

<li><a HREF="/">Main/Blog</a>


<li><a HREF="/about/">About Us</a>


<li><a HREF="/fpga-hell.html">FPGA Hell</a>


<li><a HREF="/tutorial/">Tutorial</a>
<li><a HREF="/tutorial/formal.html">Formal training</a>


<li><a HREF="/quiz/quizzes.html">Quizzes</a>


<li><a HREF="/projects.html">Projects</a>


<li><a HREF="/topics.html">Site Index</a>

<HR>

<li><a href="https://twitter.com/zipcpu"><span class="icon--twitter"><svg viewBox="0 0 400 400"><path fill="#1da1f2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>
</span><span class="username">@zipcpu</span></a>

<li><a href="https://www.reddit.com/r/ZipCPU"><span class="username">Reddit</a>
<li><a HREF="https://www.patreon.com/ZipCPU"><IMG SRC="/img/patreon_logomark_color_on_white.png" WIDTH="25"> Support</a>
</ul>
</div>


</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="https://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">What is a Virtual Packet FIFO?</h1>
    <p class="post-meta"><time datetime="2023-04-08T00:00:00-04:00" itemprop="datePublished">Apr 8, 2023</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>I first came across virtual packet FIFOs in a <a href="/blog/2022/04/29/proto-bringup.html">SONAR
project</a> by necessity.
The <a href="/blog/2022/04/29/proto-bringup.html">SONAR device</a>’s
only means of communicating with the outside world was
via Gb Ethernet.  There was no
<a href="/formal/2019/02/21/txuart.html">UART</a> and no JTAG.
Everything went over Ethernet.  Collected data went over Ethernet.
Device control was over Ethernet.  <a href="/blog/2022/08/24/protocol-design.html">Debugging had to be done over
Ethernet</a>.  FPGA
reconfiguration and all software updates had to go over Ethernet.  Last of
all, the CPU needed to talk to the outside world over Ethernet.  This was
where I first came up with the idea of a virtual packet FIFO.</p>

<table align="center" style="float: none"><caption>Fig 1. A Virtual Packets FIFO</caption><tr><td><img src="/img/vfifo/pktvfifo.svg" alt="" width="560" /></td></tr></table>

<p>The idea came from necessity, given how <a href="https://github.com/ZipCPU/zipversa/blob/master/rtl/enet/enetpackets.v">my previous network
controller</a>
operated.  In <a href="https://github.com/ZipCPU/zipversa/blob/master/rtl/enet/enetpackets.v">that
controller</a>,
packets would be received into a small block RAM connected the controller.
That block RAM could hold only one packet at a time.  Once a packet was
received, therefore, the network controller would be deaf until the CPU
processed the packet and then notified the controller it could use its
memory for another packet.  Likewise, when the CPU wished to transmit a
packet, it would write a single packet to the controller’s memory, notify
it that a packet was present, and then wait for the controller to finish
transmitting it before writing the next packet to memory.</p>

<p>This works great–on a low bandwidth interface.  But what happens if two
packets arrive in short succession?  Or, similarly, what happens if
packets arrive that are larger than the <a href="https://github.com/ZipCPU/zipversa/blob/master/rtl/enet/enetpackets.v">controller’s internal
buffer</a>?
What about “Jumbo packets”?</p>

<p>All of these problems necessitated a new solution, and the solution I chose was
a virtual packet FIFO.  This solution has two big upgrades to the previous
one.  The first is a size upgrade.  A virtual packet FIFO can be <em>much</em> larger
than its block RAM counterpart.  The second upgrade is the number of packets
that can be held.  Frankly, it doesn’t make much sense if you can hold
lots of data, if you can’t also fill that with either lots of packets or
a small number of jumbo packets.</p>

<p>Since this is a neat idea, let’s take a moment and discuss it.</p>

<h2 id="packet-streams">Packet streams</h2>

<p>Some time ago, I discussed <a href="/blog/2022/02/23/axis-abort.html">the problems with the AXI stream
protocol</a>.
At the time, I based my discussion on three specific applications:
<a href="/video/2022/03/14/axis-video.html">video</a>,
<a href="/blog/2019/11/14/sdspi.html">data capture</a>, and network
packet handling.  In each of these applications, data would arrive at the
incoming interface independent of whether or not there was space available
to handle it.  <a href="https://en.wikipedia.org/wiki/Back_pressure">Backpressure</a>, a
key feature of the AXI stream protocol, could not be supported properly
without risking data corruption.</p>

<p>At that time, <a href="/blog/2022/02/23/axis-abort.html">I suggested a new AXI stream field:
ABORT</a>.
If the <a href="/blog/2022/02/23/axis-abort.html">ABORT</a> signal
was ever asserted from an upstream source, the rest of any data packet
would need to be dropped, and data handling would need to start over with
the first beat of the next packet.  This new
<a href="/blog/2022/02/23/axis-abort.html">ABORT</a>
signal has worked nicely in network packet handling constructs.  Indeed, it
has worked <em>very</em> well.</p>

<ul>
  <li>
    <p>Yes, it’s a bit harder to work with and harder to verify than straight AXI
streams.  This is to be expected.</p>
  </li>
  <li>
    <p>However, it was joy to watch the network design “just work” with this
protocol.  In particular, I watched network data get captured, formed
into packets, and then dropped as the design started up–because either
the network interface hadn’t finished its negotiation into 1Gb mode
(it could never keep up at less than 1Gb/s), or because the data hadn’t
been told where to go yet.  (Yes, it still needed a destination addresses
for the SONAR data, both IP and Ethernet, before it could send it out.)</p>

    <p>Once configuration completed, the protocol started blasting captured
packets without a hitch.</p>
  </li>
</ul>

<p>I loved it!</p>

<p>Others, however, have argued that my proposed
<a href="/blog/2022/02/23/axis-abort.html">ABORT</a>
field was unnecessary.  Why create a new protocol, they argued, vs. just using
straight AXI stream?  The answer to this is twofold:</p>

<ol>
  <li>
    <p><strong><a href="https://en.wikipedia.org/wiki/Jumbo_frame">Jumbo Frames</a></strong></p>

    <p>In order to use straight AXI stream, you have to first convert the incoming
network packet to AXI stream in the first place.  The follows simply because
that incoming network interface doesn’t know anything about
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>.
To do this conversion, incoming packets need to first go into a buffer.
If there’s not sufficient space in the buffer, the packet is simply dropped.
If there’s sufficient space, the packet is “committed” and can then be read
out of the buffer via standard AXI stream.</p>

    <p>The size of this buffer forces a limit on the maximum packet size that can
be handled.  Packets larger than the buffer size will need to be dropped.</p>

    <p>While I was designing the original SONAR Ethernet controller, my customer
asked about <a href="https://en.wikipedia.org/wiki/Jumbo_frame">jumbo
frames</a>–packets much larger than
the (otherwise) maximum
Ethernet packet size of 1500 Bytes.  How much larger?  They didn’t say.
All of a sudden, I could no longer size my buffer prior to hardware
layout (place and route).</p>

    <p>The Virtual Packet FIFO we’ll discuss today can solve this problem of
converting an (otherwise) unsized packet to AXI stream proper.</p>
  </li>
  <li>
    <p><strong>Vendor Infrastructure</strong></p>

    <p>If I used Xilinx (or any other vendor’s) AXI stream infrastructure, I might
be tied to that protocol.  The choice of whether or not to use AXI stream
is really a business decision: either rebuild the AXI stream infrastructure
from scratch to support a modified protocol, or stick to the AXI stream
protocol as is.</p>
  </li>
</ol>

<table align="center" style="padding: 25px; float: right"><caption>Fig 2. Advantages to using your own IP</caption><tr><td><img src="/img/vfifo/personal-ip.svg" alt="" width="320" /></td></tr></table>

<p>If I rebuild the infrastructure from scratch, I incur additional costs
   above and beyond what I might have incurred had I used someone else’s
   (free) infrastructure.  I can release any IP I build under my chosen user
   license.  I can also formally verify anything I build.  I will also gain
   the ability (and responsibility, and cost associated with) debugging and
   maintaining it.  The good news, though, is that I can guarantee the quality
   of any IP I control.</p>

<table align="center" style="padding: 25px; float: left"><caption>Fig 3. Advantages to using vendor IP</caption><tr><td><img src="/img/vfifo/vendor-ip.svg" alt="" width="320" /></td></tr></table>

<p>If I use a vendor’s infrastructure then I might save some money–while
   risking my project’s success on the vendor’s responsiveness to bugs found in
   their infrastructure.  Given that I’m aware of bugs that’ve lived in Xilinx
   IP for nearly 10 years, and given that I’m a small one-man nobody shop, I
   don’t have a strong confidence that they’ll fix anything that’s broken.</p>

<p>Yes, I suppose this is a business decision.</p>

<p>Frankly, I don’t use vendor infrastructure unless I have to.  It’s just the
   nature of how I’ve structured my own business at <a href="/about/gisselquist-technology.html">Gisselquist
   Technology</a>.
   I’ve now built <a href="/about/zipcpu.html">my own CPU</a>, my own
   GNU compiler and assembler back ends, <a href="/blog/2019/07/17/crossbar.html">my own bus
   interconnects</a>,
   <a href="https://github.com/ZipCPU/dspfilters">my own DSP filters</a>,
   <a href="/dsp/2017/08/30/cordic.html">CORDICs</a>,
   <a href="/dsp/2018/10/02/fft.html">FFTs</a>, etc.  So it should come
   as no surprise that I’d have no problems building <a href="https://github.com/ZipCPU/eth10g/tree/master/rtl/net">an AXI stream
   infrastructure</a> based
   around a new “<a href="/blog/2022/02/23/axis-abort.html">ABORT</a>”
   field.</p>

<p>Yes, there are risks with this approach.  One common risk is that I might
   need to interface with a vendor protocol, so I often have <a href="https://github.com/ZipCPU/wb2axip">conversion
   routines</a> available to move back and
   forth between one protocol and another when necessary.  For example, it’s
   not enough to use
   <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> if you need
   to interact with Xilinx’s MIG–so I use a <a href="/blog/2020/03/23/wbm2axisp.html">bridge from one protocol to the
   other</a>.  I <a href="https://github.com/ZipCPU/wb2axip">also
   have</a> sufficient infrastructure to
   use AXI without bridges if necessary.</p>

<p>Still, AXI stream is a really simple protocol, and <a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">this modified AXI
   network stream
   protocol</a>,
   while more complex, isn’t really that much more difficult to deal with.</p>

<p>Since writing <a href="/blog/2022/02/23/axis-abort.html">that
article</a>, I’ve had great
success with this new
<a href="/blog/2022/02/23/axis-abort.html">ABORT</a> field.
Indeed, I’ve had so much success, that I’m now rebuilding all of my network
data handling components to use it.</p>

<p>However, there is one (more) problem this <a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">new
protocol</a> needs
to address: stream widths.</p>

<p>When working with 1Gb Ethernet, I could operate at 8b/clock at 125MHz, and
stream widths weren’t really a problem–every beat contained exactly one byte.
Well, not quite.  Stream widths became a bit of a problem when crossing clock
domains, since I would need to guarantee sufficient handling width.  To
handle the CDC case, I first converted to a wider (32b) AXI stream, and
then prepended a 32b packet length to the packet.  This kept me from
supporting <a href="https://en.wikipedia.org/wiki/Jumbo_frame">jumbo frames</a>,
so when rebuilding for a 10Gb interface, I needed a new solution.</p>

<p>Standard AXI stream solves this problem with their TSTRB and TKEEP fields.
Each field has one bit per byte per beat within it, and allows the stream
processor to handle less than a full beat of information.  For example, when
dealing with a 32-bit interface, a 16-bit value might contain two NULL bytes,
where a NULL byte is defined as one where TKEEP and TSTRB are both low.</p>

<p>This seemed insufficient for me for a variety of reasons.  In general, to use
an AXI stream of this type, you’d first want to pack it and remove all
NULL bytes.  This would force any unused bytes into the last beat, while also
requiring that the last beat had at least one valid byte.  The last beat would
also need to be packed, so that all used bytes would be on the low end–when
using little endian semantics, or the high end otherwise.  Further, I never
saw a reason for keeping “position” bytes (TKEEP &amp;&amp; !TSTRB) around.  The
result was that TKEEP and TSTRB contained too many bits for my purpose.</p>

<p>So I created a new field: BYTES.  At first, the BYTES field had <code class="language-plaintext highlighter-rouge">$clog2(DW/8+1)</code>
bits to it, where <code class="language-plaintext highlighter-rouge">DW</code> is the number of bits in the DATA field–sometimes
called <code class="language-plaintext highlighter-rouge">C_AXIS_DATA_WIDTH</code>.  This BYTES field would then be equal to <code class="language-plaintext highlighter-rouge">DW/8</code>
for every beat prior to the last one, and between one and <code class="language-plaintext highlighter-rouge">DW/8</code> inclusive for
the last beat.  (<code class="language-plaintext highlighter-rouge">0 &lt; BYTES &lt;= DW/8</code>)  Then, on second thought, I realized the
top bit of <code class="language-plaintext highlighter-rouge">BYTES</code> was irrelevant: Since <code class="language-plaintext highlighter-rouge">BYTES</code> was never zero, and never
more than <code class="language-plaintext highlighter-rouge">DW/8</code>, I could map the <code class="language-plaintext highlighter-rouge">DW/8</code> value to zero and drop a bit.  So,
now, BYTES has <code class="language-plaintext highlighter-rouge">$clog2(DW/8)</code> bits and a value of zero (representing <code class="language-plaintext highlighter-rouge">DW/8</code>
bytes) for all but the <code class="language-plaintext highlighter-rouge">LAST</code> beat where it might represent fewer bytes per
beat.</p>

<p>So, in summary, to support packet data I made the following changes to the
AXI stream protocol:</p>

<ul>
  <li>
    <p><strong><a href="/blog/2022/02/23/axis-abort.html">ABORT</a></strong>: A new
field, indicating that the upstream processor needed to drop the packet
for any reason.  Possible reasons I’ve come across include: 1) CRC errors,
2) protocol errors, 3) hardware errors, or even 4) insufficient memory
for handling <a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>,
from downstream.</p>
  </li>
  <li>
    <p><strong>TKEEP/TSTRB</strong>: I dropped both of these fields.</p>
  </li>
  <li>
    <p><strong>BYTES</strong>: A new field to replace the TSTRB/TKEEP fields, while still
indicating how many bytes are active in a given beat.</p>
  </li>
  <li>
    <p>And, of course, all beats are fully packed.  Hence, all but the LAST
beat will have <code class="language-plaintext highlighter-rouge">DW/8</code> valid bytes in it.</p>
  </li>
</ul>

<p>I’ve named <a href="https://github.com/ZipCPU/eth10g/blob/master/doc/axin.pdf">this (new) protocol the AXI-networking, or AXIN,
protocol</a>, for lack
of a better name.  As a result, if you look through <a href="https://github.com/ZipCPU/eth10g/tree/master/rtl/net">the designs I’ve built
to use this protocol</a>,
you’ll find “AXIN” in a lot of the names.</p>

<p>I also have a lot of infrastructure for this new protocol, and that
infrastructure is growing on a daily basis.  For example, I have AXIN
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netskid.v">skidbuffers</a>,
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">asynchronous</a>
and <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/netfifo.v">synchronous
FIFO</a>s,
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinbroadcast.v">broadcasters</a>,
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinarbiter.v">arbiters</a>,
a <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">width
converter</a>,
and more.  (A CRC checker is still being verified, but will likely be posted
soon.)</p>

<p>This brings us to the topic of virtual FIFOs.</p>

<h2 id="what-is-a-virtual-fifo">What is a virtual FIFO?</h2>

<p>A virtual FIFO is simply a FIFO that uses external instead of internal memory.
That external memory is typically accessed via a bus, shared among many
potential users, and commonly exists off-chip.  A classic example would be a
DDR3 SDRAM memory accessed via an AXI (or
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>) bus.  You can
see the difference between a traditional and a virtual FIFO in Fig. 4.</p>

<table align="center" style="padding: 25px; float: right"><caption>Fig 4. Difference between a FIFO and a Virtual FIFO</caption><tr><td><img src="/img/vfifo/fifo-comparison.svg" alt="" width="320" /></td></tr></table>

<p>Some time ago, I built a <a href="https://github.com/ZipCPU/wb2axip/blob/master/rtl/axivfifo.v">Virtual FIFO for the AXI
protocol</a>.  The
flow went as follows:</p>

<ol>
  <li>
    <p>The first step was to <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L486-L490">buffer a burst of data</a>
into a <a href="https://github.com/ZipCPU/wb2axip/blob/master/rtl/sfifo.v">synchronous
FIFO</a>.  To work
smoothly, the <a href="https://github.com/ZipCPU/wb2axip/blob/master/rtl/sfifo.v">synchronous
FIFO</a> needed
space for at least two AXI bursts.</p>
  </li>
  <li>
    <p>Once a full burst’s worth of data was available in the <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L486-L490">local
FIFO</a>,
that data would be <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L486-L490">burst to the AXI
bus</a>.</p>

    <p>When using <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>,
I’d do the same thing, save that the burst would only
end after the incoming FIFO was completely drained.</p>
  </li>
  <li>
    <p><a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L409-L427">Once BVALID was then received</a>,
we would know that a full AXI burst’s
worth of memory was now available in the external RAM to be <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L681-L735">read
back</a>.</p>

    <p>In the case of
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>,
I’d count data words, not burst sizes, but it’s the same principle.</p>
  </li>
  <li>
    <p>There was again another (local, block RAM) <a href="https://github.com/ZipCPU/wb2axip/blob/master/rtl/sfifo.v">synchronous
FIFO</a> on the read
memory side.  Like the first FIFO, <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L788-L796">this
one</a>
also required enough room to contain at least two AXI bursts.</p>
  </li>
  <li>
    <p>Once a burst’s worth of data has been placed into the external RAM, <em>and</em>
there is sufficient (uncommitted) data in the second FIFO to hold it,
a <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L681-L735">burst read request would be issued</a>.</p>

    <p>Again, when using
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>,
I’d make requests until the entire FIFO’s size was committed–not just the
initial burst size.  Hence, as reads might be made from the FIFO while
requesting data from the bus, additional reads would be made.</p>
  </li>
  <li>
    <p>Data read back from memory would then get sent straight into <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L788-L796">this second
buffer</a>
once it returned from the bus.</p>
  </li>
  <li>
    <p>The final, outgoing AXI stream, would then be fed <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L786">straight from this
second buffer</a>.</p>
  </li>
  <li>
    <p>Only when the incoming FIFO is full would
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>,
attempt to slow down the upstream source.</p>

    <p>The incoming FIFO would be “full” if it wasn’t getting emptied.  This
would happen if either 1) <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L535-L536">the memory was full</a>,
or 2) the <a href="https://github.com/ZipCPU/wb2axip/blob/49f06ea0219c48a1010f95d72d78ba535b075217/rtl/axivfifo.v#L540-L541">FIFO couldn’t write to memory fast
enough</a>.</p>
  </li>
</ol>

<p>Success, when using this technique, required that the stream bandwidth be
less than 50% of the memory bandwidth.  This will often require that any stream
necessitating a high throughput might first need to be resized to a wider
width–just to reduce the throughput to something the memory can handle.
Remember, when sizing memory bandwidth, there are lots of things that can
use up your bandwidth:</p>

<table align="center" style="float: left; padding: 25px"><caption>Fig 5. Calculating memory bandwidth</caption><tr><td><img src="/img/vfifo/mem-bw.svg" alt="" width="480" /></td></tr></table>

<ol>
  <li>
    <p>You’ll need bandwidth for the data to come in and get written to memory</p>
  </li>
  <li>
    <p>You’ll need that much again to read the data back out</p>
  </li>
  <li>
    <p>You’ll need to allocate time for bus latency.</p>

    <p>This can be worse for any bus that needs to stop in order to switch
directions.</p>
  </li>
  <li>
    <p>A memory can only read or write on any given clock cycle, and also needs
a couple cycles to switch from reading to writing and back again.</p>
  </li>
  <li>
    <p>Don’t forget, you’ll also need to allocate some number of memory clock
cycles for refresh.  How many cycles will be required here depends upon
your memory, your bus structure, and whether or not your memory allows
pulling refresh cycles or whether such pulled cycles are supported in your
controller.</p>
  </li>
  <li>
    <p>Xilinx’s MIG controller also uses a couple of clock cycles per refresh
to keep it’s IO PLL locked on the memories DQS strobe signal(s).</p>
  </li>
  <li>
    <p>You’ll also need memory bandwidth for everything else that might use the
memory.</p>
  </li>
</ol>

<p>In short, <em>it depends</em>.  The best way to size memory bandwidth requirements is
to calculate how many beats per second you will need, and then make sure
your memory can support perhaps four times that amount.</p>

<p>A key problem with the standard virtual FIFO, described above, is that there’s
no (good) way to store non-data information such as packet boundaries in
memory.  Either you increase the memory storage requirement to hold a LAST bit
(often by 2x!), or it just gets dropped.  Indeed, my <a href="https://github.com/ZipCPU/wb2axip/blob/master/rtl/axivfifo.v">basic AXI virtual
FIFO</a>
implementation simply drops this data.  As a result, it works well on a
proper <em>stream</em> interface, but not very well on a <em>packet</em> interface.</p>

<p>I have a <a href="https://github.com/ZipCPU/wbscope/blob/master/rtl/memscope.v">separate virtual FIFO that I’ve built for my
<em>scope</em></a>,
sometimes called an internal logic analyzer.  (This one’s been formally
verified, but never used in any practical context.  It was fun to build, and
a good learning exercise, it just hasn’t fit into any important usage
scenarios … yet.)  In this case, if the
<a href="/blog/2017/06/08/simple-scope.html">scope</a>
ever gets overrun and can’t keep up, all the data will be dropped
and it will start collecting all over again with new data.</p>

<p>Again, the problem with the stream protocol is
<em><a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a></em>,
and what to do if you overrun the FIFO, and where/when in your stream processing
will that information be known.  When dealing with packets, the rule
is data needs to be dropped at packet boundaries.  That information needs
to be communicated to the place where the decision can be made.</p>

<p>So how do we mix the <em>packet</em> concept with the concept of a <em>virtual FIFO</em>?
The answer is a virtual packet FIFO.</p>

<h2 id="what-is-a-virtual-packet-fifo">What is a virtual packet FIFO?</h2>

<p>A virtual packet FIFO is simply a virtual FIFO that maintains packet
boundaries.</p>

<table align="center" style="float: none"><caption>Fig 6. Virtual FIFO definitions</caption><tr><td><img src="/img/vfifo/vpktdefns.svg" alt="" width="560" /></td></tr></table>

<p>That means two things.</p>

<p>First, it means we have to preserve packet boundaries.  That <em>LAST</em> signal
is important when working with packets.  Moreover, packet boundaries need
<em>octet</em> level precision.</p>

<p>Second, it means that packets are written to the FIFO before being
<em>committed</em> to the FIFO.  Only after a full packet has been written to
the FIFO can it ever get committed.</p>

<p>To handle both of these requirements, I rearranged how I stored packets
in memory.  Instead of storing packet data alone, or packet data plus a LAST
bit, or packet data plus some number of ancillary bits (TSTRB, TKEEP, TUSER,
and TLAST), I store the length of the packet <em>before</em> the packet.</p>

<p>Fig. 6 shows this pictorally.</p>

<table align="center" style="padding: 25px; float: right"><caption>Fig 6. Virtual packets in memory</caption><tr><td><img src="/img/vfifo/vpktmem.svg" alt="" width="320" /></td></tr></table>

<p>Specifically:</p>

<ul>
  <li>
    <p>All packet length fields precede the packet they describe.</p>
  </li>
  <li>
    <p>All packet lengths are 32’bits.  Yes, this is an arbitrary length.
However, 1) this seems to be the smallest bus size I’ve ever needed to work
with.  2) I rarely have more than 4GB of memory, so this seems sufficient.
3) It allows for jumbo packet sizes up to whatever memory size I have on hand.</p>
  </li>
  <li>
    <p>I also force a minimum 32-bit alignment on all accesses.  So, for a 128-bit
wide bus, this word will be aligned on a 32-bit boundary.</p>
  </li>
  <li>
    <p>Before a packet is committed, its packet length shall be NULL.  (i.e. 32’h0)
You can think of this like the NULL pointer at the end of a linked list.</p>
  </li>
  <li>
    <p>The packet data is written to memory using the full width of the bus.</p>

    <p>In the context of the <a href="https://github.com/ZipCPU/eth10g">10Gb Ethernet switch</a>
I’m working on, maintaining memory throughput is important.  As a result, I
need to use the full memory width (512 bits) as often as possible.  Anything
less would reduce my memory bandwidth.</p>
  </li>
  <li>
    <p>If ever a packet gets dropped, the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">packet
writer</a>
just goes back to the beginning of the packet data area and starts over
following the NULL packet length word when the next packet starts.</p>

    <ul>
      <li>
        <p>Packets can be dropped for any upstream reason.</p>
      </li>
      <li>
        <p>Packets are also dropped if the virtual packet FIFO runs out of room.</p>

        <p>This is a necessary criteria to prevent a deadlock created if the upstream
source never needs to abort, and yet there’s no room in memory to hold
the last of the packet in memory.</p>

        <p>In order to support packet length pointers, a packet may not be completed
unless there’s room for both the packet length before and the packet
length of the packet to follow.</p>
      </li>
    </ul>
  </li>
</ul>

<p>As I mentioned, I’m now working on building a Virtual Packet FIFO.  It hasn’t
yet been verified, or I’d present the entire FIFO here.  For now, let me point
out the three major components and discuss how they work together:</p>

<ol>
  <li>There’s the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifo.v">Controller</a>,</li>
  <li>The <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">writer</a>, and</li>
  <li>The <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>.</li>
</ol>

<p>You may expect these components to change as they eventually get verified,
and then tested and proven in hardware.  (As of today, only the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">writer</a>
has passed a formal check, and even that check didn’t properly include the
<a href="/formal/2020/06/12/four-keys.html">contract</a>.)</p>

<p>Let’s discuss each of these components briefly.</p>

<h3 id="the-virtual-packet-fifo-controller">The Virtual Packet FIFO Controller</h3>

<p>The controller is responsible for setting the base address and memory size
allocated to the virtual FIFO.  These two values are then propagated down to
both <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">writer</a>
and <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>.
It’s also responsible for resetting the FIFO, and (depending on the
configuration) releasing it from reset.</p>

<p>Even though this is only my second virtual packet FIFO, I’ve already had several
diverse needs for this controller.  In one design, the controller was given a
fixed memory allocation.  This is appropriate if the controller is required to
start up and operate without any CPU intervention.  In another design, the CPU
could allocate memory for the FIFO and then enable or disable the FIFO.  When
I can’t decide which of the two I want, sometimes I will generate a combination
of the two, so that the FIFO may start with a default allocation that the CPU
can come back to and adjust later if necessary.</p>

<p>What happens if the CPU gives it a bad allocation?  One of the challenges of
controller design is determining how the virtual packet FIFO should handle
<a href="https://en.wikipedia.org/wiki/Bus_error">bus errors</a>.
In general, a <a href="https://en.wikipedia.org/wiki/Bus_error">bus error</a> indicates
that the FIFO has a bad memory allocation.  This might be the case if the
CPU has allocated memory to the FIFO that isn’t present in the system.  In
this case, it makes the most sense for the FIFO to shut down with some type of
error condition, and to then wait for the CPU to correct its memory allocation.
On the other hand, if the CPU will get its instructions for “fixing” any faults
from the network, then the network must be able to heal itself without any
CPU intervention.</p>

<p>A similar problem might be generated by the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>
if it ever comes across an invalid packet length word.  Such a packet length
migth be equal to zero, greater than the total size of the allocated memory,
or perhaps just big enough to pass the write pointer.</p>

<p>In both of these cases, either a
<a href="https://en.wikipedia.org/wiki/Bus_error">bus error</a> or an invalid
packet length, there should be an appropriate way to fix the situation.
In a hands-off implementation, the FIFO will need to just reset
itself–hopefully in that case memory allocation issues will be handled before
implementation.  In another case, the FIFO will wait for the CPU to issue a
new address before releasing itself from reset.  The same could be done with
the read packet length, or alternatively the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">packet
reader</a>
might just skip to where the write pointer is at–skipping any packets in the
way.</p>

<p>Which method of resolving faults is appropriate depends upon the particular
design requirements.</p>

<h3 id="the-write-state-machine">The Write State Machine</h3>

<p>Once the base address and memory size are known, and once the FIFO has been
released from reset, incoming packets may be written to memory.</p>

<p>This takes place in several discrete steps.</p>

<ol>
  <li>
    <p>First, prior to any packet, <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L397-L423">the packet’s length word must be written as
zero</a>.</p>
  </li>
  <li>
    <p>Then, as packet data enters the FIFO, its data <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L424-L464">gets written directly to
memory</a>.</p>

    <p>Unfortunately, the 32-bit packet length word guarantees that further writes
to memory can not be guaranteed to have any particular alignment.  Incoming
data must then be <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L450-L457">realigned as it is written to memory</a>.
This also means that there may need to be <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L483-L488">N+1 memory writes for
every N memory words</a>.</p>

    <p>A second problem here is associated with the data pointer.  Specifically,
pointers wrap.  Hence, <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L226-L237">any calculation of the next memory
address</a>
must include <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L232-L234">a check against the last memory address, and a wrap back
to the first address if it passes the end of
memory</a>.</p>

    <p>Finally, <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L461-L462">this is the only place where committed memory space is
checked</a>.
If a packet uses all of the available memory space, not just the remaining
memory space, then it must be aborted locally.</p>
  </li>
  <li>
    <p>On any packet <a href="/blog/2022/02/23/axis-abort.html">ABORT</a>s,
the <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L435-L440">write pointer is set to follow the prior NULL
length</a>.  On a local
packet <a href="/blog/2022/02/23/axis-abort.html">ABORT</a>, such
as might take place if the packet overflowed memory, then we need to
resync to the beginning of the next packet.</p>
  </li>
  <li>
    <p>Once a packet is complete, the next word becomes the length field of the
next packet.  <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L495-L523">It is set to
NULL</a>.</p>
  </li>
  <li>
    <p>After this next word has been set to NULL, the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">FIFO
writer</a>
can then <a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L524-L557">go back and write the length of the current (just written) packet
into memory</a>.</p>

    <p>This is what actually commits the packet to memory.  We can know the
packet has been committed once all bus requests have been completed.</p>
  </li>
  <li>
    <p>Once the bus becomes idle, we tell the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>
<a href="https://github.com/ZipCPU/eth10g/blob/8e4af12717cfc96611e47b260236e52f5412d95c/rtl/net/pktvfifowr.v#L579">our new start-of-packet pointer</a>
and go back to step #2 above to handle the next packet.</p>
  </li>
</ol>

<p>All of this is handled via a (monster) state machine that can master the bus.</p>

<h3 id="the-read-state-machine">The Read State Machine</h3>

<p>Once a packet is committed, a second state machine can then read the packet
back from the bus.  (This one still needs a lot of verification work …)</p>

<ol>
  <li>
    <p>First, the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>
reads the length word from memory.</p>

    <p>Knowing when to read this length word is a bit of a problem.  Were this a
piece of CPU software, we might poll this memory word.  If the memory word
was ever non-zero, we’d know a packet was present.  However, this design is
intended for a hardware implementation.  Hardware can poll memory on every
clock cycle, so much so that the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">writer</a>
wouldn’t have any cycles left to write the packet to memory.  To prevent
this, we’d need to specify a polling interval, which would then increase our
latency.  Supporting minimum latency requires a different solution.</p>

    <p>My solution to this problem has been to use an out-of-band communication
scheme through the controller.  In this scheme, the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifowr.v">writer</a>
tells the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>
a pointer to the length word of the last packet committed.  If this address
doesn’t match the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>’s
last memory address, then a packet is present that may be read.
In another version of this FIFO, one with the CPU in the middle, the CPU
provides the reader with the same pointer.  Again, this tells the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>
when it’s safe to go and read the packet length counter for the next packet.</p>
  </li>
  <li>
    <p>Once returned, the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>
then verifies the packet length word.</p>

    <ul>
      <li>It’s not allowed to be zero.</li>
      <li>The packet length may not pass the write pointer.  This would indicate
a memory overrun condition.</li>
      <li>The packet length must be less than the size of memory.</li>
    </ul>

    <p>On any failure, we can either reset the entire FIFO, or (alternatively) just
drop all packets between our current location and the write pointer.</p>
  </li>
  <li>
    <p>If the length pointer is good, we start reading from memory.</p>

    <p>There are two challenges with this task.  The first challenge is that
the memory will (in general) be misaligned.  The second challenge is that
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> has no
concept of <a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>,
and our outgoing stream interface may require it.</p>

    <p>To handle misalignment, we need to keep track of the previously read
memory word.  That, plus the current memory word, both shifted
appropriately, we’ll yield an outgoing stream word.  The trick is that
we may need to read an additional word to get all of the outgoing
stream data associated with this packet.</p>

    <p>The way to handle
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a> when using
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> is to
guarantee that we don’t issue a read request in the first place unless
there’s space available in the following outgoing FIFO for a packet word.</p>
  </li>
</ol>

<p>One of the nice things about the
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfiford.v">reader</a>,
is that we don’t need to generate any
<a href="/blog/2022/02/23/axis-abort.html">ABORT</a>s.  That’s
a pleasant simplification.  Indeed, at this point in our return processing, we
could finally handle (infinite)
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a> if need be.</p>

<h3 id="a-new-interconnect">A New Interconnect</h3>

<p>One piece I wasn’t expecting in this new architecture was an updated/better
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
<a href="/blog/2019/07/17/crossbar.html">interconnect</a>.</p>

<p>As perspective, <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifo.v">this virtual packet
FIFO</a>, is
designed to support a <a href="https://github.com/ZipCPU/eth10g">10Gb, 4-way Ethernet
switch</a>.  That means I want to be able to
support 10Gb arriving (and departing) on each of the 4 interfaces at the same
time.  When using our planned hardware, the memory will run on a 200MHz clock,
reading (or writing) 512-bits (64-bytes) of data per clock cycle.  However, a
10Gb Ethernet switch will generate one 512-bit word every 51.s ns, or (roughly)
once every 11 clocks at 200MHz.  Hence, when the interface is running full
speed, we’ll be getting requests from rotating controllers.  The first
controller might want a beat, but then not need anything for another 10 beats
while the second controller wants a beat, etc.</p>

<p>Typically, I run
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> in a fashion
where I burst data to the bus (i.e. to memory) and then wait for the response
before shutting the interface down.  When using Xilinx’s MIG, this can take
up 20 clock cycles of latency.  If I did that here, I’d never have enough
memory bandwidth to keep up.</p>

<p>My solution to this problem is to use a <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/wbmarbiter.v">special type of
interconnect</a>–one
I first developed for an AXI project.  When using <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/wbmarbiter.v">this
interconnect</a>, N
masters may request bus accesses of a single slave.  In this case, as each bus
master makes its request, the master’s ID is placed in a FIFO.  Since
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> requests are
always returned in the order they are received, I can then use this FIFO to
route responses back to the appropriate master.  This will allow me to
interleave requests from multiple masters together on their way to memory.</p>

<p>That’s the good news–more bandwidth.  The bad news is that this N:1 arbiter
will break <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> in
two ways.  First, since there’s no guaranteed concept of the end of a
transaction, there’s no way to know when to lock the bus.  Second, as I
implement <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>, a
<a href="https://en.wikipedia.org/wiki/Bus_error">bus error</a> terminates any ongoing
transaction.  This means that if N masters are active and only one of those
masters receives a <a href="https://en.wikipedia.org/wiki/Bus_error">bus error</a>–in
response to some erroneous transaction, then all
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a> masters will
receive a <a href="https://en.wikipedia.org/wiki/Bus_error">bus error</a> in return to
their ongoing operations.  For now, this will work: 1) these virtual packet
FIFOs will not be locking the bus, and 2) any
<a href="https://en.wikipedia.org/wiki/Bus_error">bus errors</a> should be rare or
even non-existent.  Still, it’s a risk, and I’ll need to make sure it’s
well documented throughout the project.</p>

<h2 id="conclusions">Conclusions</h2>

<p>This is now the <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifo.v">second virtual packet FIFO I’ve
created</a>.
If any design becomes so useful that you need to build it more than once,
then it’s going to become useful again.</p>

<p>In this case, this <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifo.v">virtual packet
FIFO</a> will
play an important part of the <a href="https://github.com/ZipCPU/eth10g">10Gb Ethernet
switch</a>) I’m working on.  As packets arrive
from the PHY, their CRC’s will be validated, their <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">stream width
expanded</a>,
they’ll then <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">cross clock
domains</a>,
their source MACs will recorded in the router, and they will enter <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifo.v">this virtual
packet FIFO</a>.
Once these packets come out of
<a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/pktvfifo.v">the FIFO</a>,
the’ll go into a separate <a href="https://github.com/ZipCPU/wb2axip/blob/master/rtl/sfifo.v">synchronous
FIFO</a>, have
their destination MACs checked, get routed to an outgoing interface, <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axincdc.v">cross
clock domains</a>
(again), have <a href="https://github.com/ZipCPU/eth10g/blob/master/rtl/net/axinwidth.v">their widths
adjusted</a>
back to the interface width, and finally get blasted out the network.  Feel
free to check out
<a href="https://github.com/ZipCPU/eth10g/doc/eth10g-blocks.png">this picture</a> to see
an overview of this entire operation, as well as the status of the various
components required of this project.</p>

<p>For now, however, the project is still draft.  The hardware, while drafted,
isn’t yet built and I’m still working on the RTL components within it.
<a href="https://www.blueletterbible.org/kjv/jas/4/15/">Lord willing</a>,
I’ll have the RTL done by the time the hardware is available.</p>

<p>Still, the overall concept of a Virtual Packet FIFO was one I felt would
be worth sharing.</p>

  </div>


<div class "verse">
<HR align="center;" width="25%">
<P><em>There is that scattereth, and yet increaseth; and there is that withholdeth more than is meet, but it tendeth to poverty.  (Prov 11:24)</em>


</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The ZipCPU by Gisselquist Technology</h2>
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <!-- <li></li> -->
          <li><a href="mailto:zipcpu@gmail.com">zipcpu@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="soc-medlist">
          
          <li>
            <a href="https://github.com/ZipCPU"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">ZipCPU</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/zipcpu"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">@zipcpu</span></a>

          </li>
          
          
          <li><A href="https://www.patreon.com/ZipCPU"><img src="/img/become_a_patron_button.png"></a></li>
          

        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The ZipCPU blog, featuring how to discussions of FPGA and soft-core CPU design.  This site will be focused on Verilog solutions, using exclusively OpenSource IP products for FPGA design.  Particular focus areas include topics often left out of more mainstream FPGA design courses such as how to debug an FPGA design.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
