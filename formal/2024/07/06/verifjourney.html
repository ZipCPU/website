<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>My Personal Journey in Verification</title>
  <meta name="description" content="This week, I’ve been testing a CI/CD pipeline.  This has been my opportunityto shake the screws and kick the tires on what should become a new verificationpr...">

  <link rel="shortcut icon" type="image/x-icon" href="/img/GT.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://zipcpu.com/formal/2024/07/06/verifjourney.html">
  <link rel="alternate" type="application/rss+xml" title="The ZipCPU by Gisselquist Technology" href="https://zipcpu.com/feed.xml">
</head>


  <body>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4ZK7HKHSVW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4ZK7HKHSVW');
</script>

    <header class="site-header">
  <div id="banner">
  <a href="/"><picture>
    <img height=120 id="site-logo" src="/img/fullgqtech.png" alt="Gisselquist Technology, LLC">
  </picture></A>
  </div>

  <div class="site-nav">
<ul>

<li><a HREF="/">Main/Blog</a>


<li><a HREF="/about/">About Us</a>


<li><a HREF="/fpga-hell.html">FPGA Hell</a>


<li><a HREF="/tutorial/">Tutorial</a>
<li><a HREF="/tutorial/formal.html">Formal training</a>


<li><a HREF="/quiz/quizzes.html">Quizzes</a>


<li><a HREF="/projects.html">Projects</a>


<li><a HREF="/topics.html">Site Index</a>

<HR>

<li><a href="https://twitter.com/zipcpu"><span class="icon--twitter"><svg viewBox="0 0 400 400"><path fill="#1da1f2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>
</span><span class="username">@zipcpu</span></a>

<li><a href="https://www.reddit.com/r/ZipCPU"><span class="username">Reddit</a>
<li><a HREF="https://www.patreon.com/ZipCPU"><IMG SRC="/img/patreon_logomark_color_on_white.png" WIDTH="25"> Support</a>
</ul>
</div>


</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="https://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">My Personal Journey in Verification</h1>
    <p class="post-meta"><time datetime="2024-07-06T00:00:00-04:00" itemprop="datePublished">Jul 6, 2024</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This week, I’ve been testing a CI/CD pipeline.  This has been my opportunity
to shake the screws and kick the tires on what should become a new verification
product shortly.</p>

<p>I thought that a good design to check might be my
<a href="https://github.com/ZipCPU/sdsdpi">SDIO project</a>.  It has roughly all the
pieces in place, and so makes sense for an automated testing pipeline.</p>

<p>This weekend, the CI project engineer shared with me:</p>

<blockquote>
  <p>It’s literally the first time I get to know a good hardware project needs
such many verifications and testings!  There’s even a real SD card
simulation model and RW test…</p>
</blockquote>

<p>After reminiscing about this for a bit, I thought it might be worth taking a
moment to tell how I got here.</p>

<h2 id="verification-the-goal">Verification: The Goal</h2>

<p>Perhaps the best way to explain the “goal” of verification is by way of an
old “war story”–as we used to call them.</p>

<p>At one time, I was involved with a DOD unit whose whole goal and purpose was
to build quick reaction hardware capabilities for the warfighter.  We bragged
about our ability to respond to a call on a Friday night with a new product
shipped out on a C-130 before the weekend was over.</p>

<p>Anyone who has done engineering for a while will easily recognize that this
sort of concept violates all the good principles of engineering.  There’s no
time for a requirements review.  There’s no time for prototyping–or perhaps
there is, to the extent that it’s always the <em>prototype</em> that heads out the
door to the warfighter as if it were a <em>product</em>.  There’s no time to build a
complete test suite, to verify the new capability against all things that could
go wrong.  However, we’d often get only one chance to do this right.</p>

<p>Now, how do you accomplish quality engineering in that kind of environment?</p>

<p>The key to making this sort of shop work lay in the “warehouse”, and what
sort of capabilities we might have “lying on the shelf” as we called it.
Hence, we’d spend our time polishing prior capabilities, as well as
anticipating new requirements.  We’d then spend our time building, verifying,
and testing these capabilities against phantom requirements, in the hopes that
they’d be close to what we’d need to build should a real requirement arise.
We’d then place these concept designs in the “warehouse”, and show them off
to anyone who came to visit wondering what it was that our team was able to
accomplish.  Then, when a new requirement arose, we’d go into this “warehouse”
and find whatever capability was closest to what the customer required and
modify it to fit the mission requirement.</p>

<p>That was how we achieved success.</p>

<table align="center" style="float: right"><tr><td><img src="/img/vlog-wait/rule-of-gold.svg" width="320" /></td></tr></table>

<p>The same applies in digital logic design.  You want to have a good set of
tried, trusted, and true components in your “library” so that whenever a new
customer comes along, you can leverage these components quickly to meet his
needs.  This is why I’ve often said that well written, well tested, well
verified design components are gold in this business.  Such components allow
you to go from zero to product in short order.  Indeed, the more well-tested
components you have that you can
<a href="/blog/2020/01/13/reuse.html">reuse</a>, the faster you’ll be
to market with any new need, and the cheaper it will cost you to get there.</p>

<p>That’s therefore the ultimate goal: a library of
<a href="/blog/2020/01/13/reuse.html">reusable</a>
components that can be quickly composed into new products for customers.</p>

<p>As I’ve tried to achieve this objective over the years, my approach to
component verification has changed, or rather grown, many times over.</p>

<h2 id="hardware-verification">Hardware Verification</h2>

<p>When I first started learning FPGA design, I understood nothing about
simulation.  Rather than learning how to do simulation properly, I instead
learned quickly how to test my designs in hardware.  Most of these designs
were DSP based.  (My background was DSP, so this made sense …)  Hence,
the following approach tended to work for me:</p>

<ul>
  <li>
    <p>I created access points in the hardware that allowed me to read and write
registers at key locations within the design.</p>
  </li>
  <li>
    <p>One of these “registers” I could write to controlled the inputs to my DSP
pipeline.</p>
  </li>
  <li>
    <p>Another register, when written to, would cause the design to “step” the
entire DSP pipeline as if a new sample had just arrived from the A/D.</p>
  </li>
  <li>
    <p>A set of registers within the design then allowed me to read the state of
the entire pipeline, so I could do debugging.</p>
  </li>
</ul>

<p>This worked great for “stepping” through designs.  When I moved to processing
real-time information, such as the A/D results from the antenna connected to
the design, I build an internal logic analyzer to catch and capture key
signals along the way.</p>

<p>I called this “Hardware in the loop testing”.</p>

<p>Management thought I was a genius.</p>

<p>This approach worked … for a while.  Then I started realizing how painful it
was.  I think the transition came when I was trying to debug
<a href="/2018/10/02/fft.html">my FFT</a> by writing test vectors to
an Arty A7 circuit board via UART, and reading the results back to display
them on my screen. Even with the hardware in the loop, hitting all the test
vectors was painfully slow.</p>

<p>Eventually, I had to search for a new and better solution.  This was just too
slow.  Later on, I would start to realize that this solution didn’t catch
enough bugs–but I’ll get to that in a bit.</p>

<h2 id="happy-path-simulation-testing">Happy Path Simulation Testing</h2>

<p><a href="https://en.wikipedia.org/wiki/Happy_path">“Happy path” testing</a>
is a reference to simply testing working paths
through a project’s environment.  To use an aviation analogy, a <a href="https://en.wikipedia.org/wiki/Happy_path">“happy path”
test</a>
might make sure the ground avoidance radar never alerted when you
weren’t close to the ground.  It doesn’t make certain that the radar
necessarily does the right thing when you are close to the ground.</p>

<p>So, let’s talk about my next project: the
<a href="/about/zipcpu.html">ZipCPU</a>.</p>

<p>Verification of the <a href="/about/zipcpu.html">CPU</a>
began with an <a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/bench/asm/simtest.s">assembly
program</a>
the <a href="/about/zipcpu.html">ZipCPU</a> would run.  The
<a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/bench/asm/simtest.s">program</a>
was designed to test all the instructions of the
<a href="/about/zipcpu.html">CPU</a>
with sufficient fidelity to know when/if the
<a href="/about/zipcpu.html">CPU</a> worked.</p>

<p>The test had one of two outcomes.  If the program halted, then the test was
considered a success.  If it detected an error, the
<a href="/about/zipcpu.html">CPU</a> would execute a
<code class="language-plaintext highlighter-rouge">BUSY</code> instruction (i.e. jump to current address) and then perpetually loop.
My test harness could then detect this condition and end with a failing exit
code.</p>

<p>When the <a href="/about/zipcpu.html">ZipCPU</a> acquired a software
tool chain (GCC+Binutils) and C-library support, this <a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/bench/asm/simtest.s">assembly
program</a>
was abandoned and replaced with a <a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/sim/zipsw/cputest.c">similar program in
C</a>.
While I still use <a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/sim/zipsw/cputest.c">this
program</a>,
it’s no longer the core of the <a href="/about/zipcpu.html">ZipCPU</a>’s
verification suite.  Instead, I tend to use it to shake out any bugs in any
new environment the <a href="/about/zipcpu.html">ZipCPU</a> might be
placed into.</p>

<p>This approach failed horribly, however, when I tried integrating an <a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/rtl/core/pfcache.v">instruction
cache</a>
into the <a href="/about/zipcpu.html">ZipCPU</a>.  I built the
<a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/rtl/core/pfcache.v">instruction
cache</a>.
I tested the <a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/rtl/core/pfcache.v">instruction
cache</a>
in isolation.  I tested the
<a href="https://github.com/ZipCPU/zipcpu/blob/a20b6064ea794d66fdeb2e00929287d7f2dc9ac6/rtl/core/pfcache.v">cache</a>
as part of the
<a href="/about/zipcpu.html">CPU</a>.  I convinced myself that it worked.
Then I placed my “working” design onto hardware and <a href="/zipcpu/2017/12/28/ugliest-bug.html">all
hell broke lose</a>.</p>

<p>This was certainly not “the way.”</p>

<h2 id="formal-verification">Formal Verification</h2>

<p>I was then asked to <a href="/blog/2017/10/19/formal-intro.html">review a new, open source, formal verification tool called
SymbiYosys</a>.  The tool
handed my cocky attitude back to me, and took my pride down a couple steps.  In
particular, I found a bunch of bugs in a FIFO I had used for years.  The bugs
had never shown up in hardware testing (that I had noticed at least), and
certainly hadn’t shown up in any of my <a href="https://en.wikipedia.org/wiki/Happy_path">“Happy path”
testing</a>.  This left me wondering,
how many other bugs did I have in my designs that I didn’t know about?</p>

<p>I then started <a href="/blog/2018/01/22/formal-progress.html">working through my previous projects, formally verifying all my
prior work</a>.  In every
case, I found more bugs.  By the time I got to the
<a href="/about/zipcpu.html">ZipCPU</a>–<a href="/blog/2018/04/02/formal-cpu-bugs.html">I found a myriad of bugs
in what I thought was a “working”</a>
<a href="/about/zipcpu.html">CPU</a>.</p>

<p>I’d like to say that the quality of my IP went up at this point.  I was
certainly finding a lot of bugs I’d never found before by using formal methods.
I now knew, for example, how to guarantee I’d never have any more of those
cache bugs I’d had before.</p>

<p>So, while it is likely that my IP quality was going up, the unfortunate
reality was that I was still finding bugs in my “formally verified”
IP–although not nearly as many.</p>

<p>A <a href="/formal/2020/06/12/four-keys.html">couple of improvements</a>
helped me move forward here.</p>

<ol>
  <li>
    <p>Bidirectional formal property sets</p>

    <p>The biggest danger in formal verification is that you might <code class="language-plaintext highlighter-rouge">assume()</code>
something that isn’t true.  The first way to limit this is to make
sure you never <code class="language-plaintext highlighter-rouge">assume()</code> a property within the design, but rather you
only <code class="language-plaintext highlighter-rouge">assume()</code> properties of inputs–never outputs, and never local
registers.</p>

    <p>But how do you know when you’ve assumed too much?  This can be a challenge.</p>

    <p>One of the best ways I’ve found to do this is to create a bidirectional
property set.  A bus master, for example, would make assumptions about
how the slave would respond.  A similar property set for the bus slave
would make assumptions about what the master would do.  Further, the slave
would turn the master’s assumptions into verifiable assertions–guaranteeing
that the master’s assumptions were valid.  If you can use the same property
set in this manner for both master and slave, save that you swap
assumptions and assertions, then you can verify both in isolation to
include only assuming those things that can be verified elsewhere.</p>

    <p>Creating such property sets for both AXI-Lite and AXI led me to find
many bugs in Xilinx IP.  This alone suggested that I was on the “right path”.</p>
  </li>
  <li>
    <p>Cover checking</p>

    <p>I also learned to use <a href="/formal/2018/07/14/dev-cycle.html">formal coverage
checking</a>, in
addition to straight assertion
based verification.  Cover checks weren’t the end all, but they could
be useful in some key situations.  For example, a quick cover check might
help you discover that you had gotten the reset polarity wrong, and so
all of your formal assertions were passing because your design was assumed
to be held in reset.  (This has happened to me more than once.  Most
recently, the <a href="/blog/2024/06/13/kimos.html">cost was a couple of months
delay</a> on what should’ve
otherwise been a straight forward hardware bringup–but that wasn’t really
a <em>formal</em> verification issue.)</p>

    <p>For a while, I also <a href="/formal/2018/07/14/dev-cycle.html">used cover checking to quickly discover (with minimal
work) how a design component might work within a larger
environment</a>.  I’ve
since switched to simulation checking (with assertions enabled) for my
most recent examples of this type of work, but I do still find it valuable.</p>
  </li>
  <li>
    <p><a href="/blog/2018/03/10/induction-exercise.html">Induction</a></p>

    <p><a href="/blog/2018/03/10/induction-exercise.html">Induction</a> isn’t
really a “new” thing I learned along the way, but it is worth mentioning
specially.  As I learned formal verification, I learned to use
<a href="/blog/2018/03/10/induction-exercise.html">induction</a>
right from the start and so I’ve tended to use
<a href="/blog/2018/03/10/induction-exercise.html">induction</a>
in every proof I’ve ever done.  It’s just become my normal practice from day
one.</p>

    <p><a href="/blog/2018/03/10/induction-exercise.html">Induction</a>,
however, takes a lot of work.  Sometimes it takes so much work I wonder
if there’s really any value in it.  Then I tend to find some key bug or
other–perhaps a buffer overflow or something–some bug I’d have never found
without
<a href="/blog/2018/03/10/induction-exercise.html">induction</a>.
That alone keeps me running
<a href="/blog/2018/03/10/induction-exercise.html">induction</a>
every time I can.  Even better, once the
<a href="/blog/2018/03/10/induction-exercise.html">induction</a>
proof is complete, you can often <a href="/formal/2019/08/03/proof-duration.html">trim the entire formal proof down from
15-20 minutes down to less than a single
minute</a>.</p>
  </li>
  <li>
    <p>Contract checking</p>

    <p>My initial formal proofs were haphazard.  I’d throw assertions at the wall
and see what I could find.  Yes, I found bugs.  However, I never really had
the confidence that I was “proving” a design worked.  That is, not until I
learned of the idea of a “formal contract”.  The “formal contract” simply
describes the essence of how a component worked.</p>

    <p>For example, in a memory system, the formal contract might have the solver
track a single value of memory.  When written to, the value should change.
When read, the value should be returned.  If this contract holds for all such
memory addresses, then the memory acts (as you would expect) … like a
<em>memory</em>.</p>
  </li>
  <li>
    <p>Parameter checks</p>

    <p>For a while, I was maintaining <a href="https://github.com/ZipCPU/zbasic">“ZBasic”–a basic ZipCPU
distribution</a>.  This was where I did all
my simulation based testing of the
<a href="/about/zipcpu.html">ZipCPU</a>.  The problem was, this
approach didn’t work.  Sure, I’d test the
<a href="/about/zipcpu.html">CPU</a> in one configuration, get it
to work, and then put it down believing the
“<a href="/about/zipcpu.html">CPU</a>” worked.  Some time later,
I’d try the <a href="/about/zipcpu.html">CPU</a> in a different
configuration–such as pipelined vs non-pipelined, and … it
would fail in whatever mode it had not been tested in.  The problem with the
<a href="https://github.com/ZipCPU/zbasic">ZBasic approach</a> is that it tended to only
check one mode–leaving all of the others unchecked.</p>

    <p>This lead me to adjust the proofs of the
<a href="/about/zipcpu.html">ZipCPU</a> so that the
<a href="/about/zipcpu.html">CPU</a> would at least be formally
verified with as many parameter configurations as I could to make sure it
would work in all environments.</p>
  </li>
</ol>

<p>I’ve written more about <a href="/formal/2020/06/12/four-keys.html">these parts of a proof some time
ago</a>, and I still stand
by them today.</p>

<p>Yes, formal verification is hard work.  However, a well verified design is
highly valuable–on the shelf, waiting for that new customer requirement to
come in.</p>

<p>The problem with all this formal verification work lies in its (well known)
Achilles heel.  Because formal verification includes an exhaustive
combinatorial search for bugs across all potential design inputs and states,
it can be computationally expensive.  Yeah, it can take a while.  To reduce
this expense, it’s important to limit the scope of what is verified.  As a
result, I tend to verify design <em>components</em> rather than entire designs.  This
leaves open the possibility of a failure in the logic used to connect all
these smaller, verified components together.</p>

<h2 id="autofpga-and-better-crossbars">AutoFPGA and Better Crossbars</h2>

<p>Sure enough, the next class of bugs I had to deal with were integration bugs.</p>

<p>I had to deal with several.  Common bugs included:</p>

<ol>
  <li>
    <p>Using unnamed ports, and connecting module ports to the wrong signals.</p>

    <p>At one point, I decided the
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
“stall” port should come before the
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
acknowledgment port.  Now, how many designs had to change to accommodate
that?</p>
  </li>
  <li>
    <p>I had a bunch of problems with my <a href="/blog/2017/06/22/simple-wb-interconnect.html">initial interconnect
design</a>
methodology.  Initially, I used the slave’s
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
strobe signal as an address decoding signal.  I then had a bug where the
address would move off of the slave of interest, and the acknowledgment
was never returned.  The result of that bug was that the design hung any
time I tried to read the entirety of <a href="/blog/2019/03/27/qflexpress.html">flash
memory</a>.</p>

    <p>Think about how much simulation time and effort I had to go through to
simulate reading an <em>entire</em> <a href="/blog/2019/03/27/qflexpress.html">flash
memory</a>–just to find
this bug at the end of it.  Yes, it was painful.</p>
  </li>
</ol>

<p>Basically, when connecting otherwise “verified” modules together by hand,
I had problems where the result wasn’t reliably working.</p>

<p>The first and most obvious solution to something like this is to use a linting
tool, such as <code class="language-plaintext highlighter-rouge">verilator -Wall</code>. 
<a href="https://www.veripool.org/verilator/">Verilator</a> can find things like
unconnected pins and such.  That’s a help, but I had been doing that from
early on.</p>

<p>My eventual solution was twofold.  First, I redesigned my <a href="/blog/2019/07/17/crossbar.html">bus
interconnect</a> from the
top to the bottom.  You can find the new and redesigned
<a href="/blog/2019/07/17/crossbar.html">interconnect</a> components
in my <a href="https://github.com/ZipCPU/wb2axip">wb2axip repository</a>.  Once these
components were verified, I then had a proper guarantee: all masters would get
acknowledgments (or errors) from all slave requests they made.  Errors would
no longer be lost.  Attempts to interact with a non-existent slave would
(properly) return bus errors.</p>

<p>To deal with problems where signals were connected incorrectly, I built a tool
I call <a href="/zipcpu/2017/10/05/autofpga-intro.html">AutoFPGA</a> to
connect components into designs.  A special tag given to the tool would
immediately connect all bus signals to a bus component–whether it be a slave
or master, whether it be connected to a
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>,
<a href="/formal/2018/12/28/axilite.html">AXI-Lite</a>, or
<a href="/formal/2019/05/13/axifull">AXI</a> bus.  This required that my
slaves followed one of two conventions.  Either all the bus ports had to
follow a basic port ordering convention, or they needed to follow a bus
naming convention.  Ideally, a slave should follow both.  Further, after
finding even more port connection bugs, I’m slowly moving towards the practice
of naming all of my port connections.</p>

<p>This works great for composing designs of bus components.  Almost all of my
designs now use this approach, and only a few (mostly test bench) designs
remain where I connect bus components by hand manually.</p>

<h2 id="mcy">MCY</h2>

<p>At one time along the way, I was asked to review <a href="https://github.com/YosysHQ/mcy">MCY: Mutation Coverage with
Yosys</a>.  My review back to the team was …
mixed.</p>

<p><a href="https://github.com/YosysHQ/mcy">MCY</a>
works by intentionally breaking your design.  Such changes to the design are
called “mutations”.  The goal is to determine whether or not the mutated
(broken) design will trigger a test failure.  In this fashion, the test suite
can be evaluated.  A “good” test suite will be able to find any mutation.
Hence, <a href="https://github.com/YosysHQ/mcy">MCY</a>
allows you to measure how good your test suite is in the first place.</p>

<p>Upon request, I tried <a href="https://github.com/YosysHQ/mcy">MCY</a> with the
<a href="/about/zipcpu.html">ZipCPU</a>.  This turned into a bigger
challenge than I had expected.  Sure, <a href="https://github.com/YosysHQ/mcy">MCY</a>
works with <a href="https://github.com/steveicarus/iverilog">Icarus Verilog</a>,
<a href="https://www.veripool.org/verilator/">Verilator</a>, and even (perhaps) some other
(not so open) simulators as well.  However, when I ran a design under
<a href="https://github.com/YosysHQ/mcy">MCY</a>, my simulations tended to find only a
(rough) 70% of any mutations.  The formal proofs, however, could find 95-98% of
any mutations.</p>

<p>That’s good, right?</p>

<p>Well, not quite.  The problem is that I tend to place all of my formal
logic in the same file as the component that would be mutated.  In order to
keep the mutation engine from mutating the formal properties, I had to remove
the formal properties from the file to be mutated into a separate file.
Further, I then had to access the values that were to be assumed or asserted
external from the file under test using something often known as “dot notation”.
While (System)Verilog allows such descriptions natively, there weren’t any open
source tools that allowed such external formal property descriptions.
(Commercial tools allowed this, just not the open source
<a href="https://github.com/YosysHQ/sby">SymbiYosys</a>.) This left me stuck with a couple
of unpleasant choices:</p>

<ol>
  <li>I could remove the ability of the
<a href="/about/zipcpu.html">ZipCPU</a>
(or whatever design) to be formally verified with Open Source tools,</li>
  <li>I could give up on using
<a href="/blog/2018/03/10/induction-exercise.html">induction</a>,</li>
  <li>I could use <a href="https://github.com/YosysHQ/mcy">MCY</a> with simulation only, or</li>
  <li>I could choose to not use <a href="https://github.com/YosysHQ/mcy">MCY</a> at all.</li>
</ol>

<p>This is why I don’t use <a href="https://github.com/YosysHQ/mcy">MCY</a>.  It may be a
“good” tool, but it’s just not for me.</p>

<p>What I did learn, however, was that my
<a href="/about/zipcpu.html">ZipCPU</a> test suite was checking the
<a href="/about/zipcpu.html">CPU</a>’s functionality nicely–just not
the debugging port.  Indeed, none of my tests checked the debugging port to the
<a href="/about/zipcpu.html">CPU</a>
at all.  As a result, none of the (simulation-based) mutations of the
debugging port were ever caught.</p>

<p>Lesson learned?  My test suite still wasn’t good enough.  Sure, the
<a href="/about/zipcpu.html">CPU</a> might
“work” today, but how would I know some change in the future wouldn’t break it?</p>

<p>I needed a better way of knowing whether or not my test suite was good enough.</p>

<h2 id="coverage-checking">Coverage Checking</h2>

<p>Sometime during this process I discovered
<a href="https://en.wikipedia.org/wiki/Code_coverage">coverage checking</a>.
<a href="https://en.wikipedia.org/wiki/Code_coverage">Coverage checking</a>
is a process of automatically watching over all of your simulation based tests
to see which lines get executed and which do not.  Depending on the tool,
coverage checks can also tell whether particular signals are ever flipped or
adjusted during simulation.  A good coverage check, therefore, can provide
some level of indication of whether or not all control paths within a design
have been exercised, and whether or not all signals have been toggled.</p>

<p>Coverage metrics are actually kind of nice in this regard.</p>

<p>Sadly, coverage checking isn’t as good as mutation coverage, but … it’s
better than nothing.</p>

<p>Consider a classic coverage failure: many of my simulations check for
AXI <a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>.  Such
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a> is generated when
either <code class="language-plaintext highlighter-rouge">BVALID &amp;&amp; !BREADY</code>, or <code class="language-plaintext highlighter-rouge">RVALID &amp;&amp; !RREADY</code>.  If your design is to
follow the AXI specification, it should be able to handle
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>
properly.  That is, if you hold <code class="language-plaintext highlighter-rouge">!BREADY</code> long enough, it should be possible
to force <code class="language-plaintext highlighter-rouge">!AWREADY</code> and <code class="language-plaintext highlighter-rouge">!WREADY</code>.  Likewise, it should be possible to hold
<code class="language-plaintext highlighter-rouge">RREADY</code> low long enough that <code class="language-plaintext highlighter-rouge">ARREADY</code> gets held low.  A well verified,
bug-free design should be able to deal with these conditions.</p>

<p>However, a “good” design should never create any significant
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>.
Hence, if you build a simulation environment from “good” working components,
you aren’t likely to see much
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>.  How then should a
component’s <a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>
capability be tested?</p>

<p>My current solution here is to test
<a href="https://en.wikipedia.org/wiki/Back_pressure">backpressure</a>
via formal methods, with the unfortunate consequence that some conditions
will never get tested under simulation.  The result is that I’ll never get
to 100% coverage with this approach.</p>

<p>A second problem with coverage regards the unused signals.  For example,
AXI-Lite has two signals, <code class="language-plaintext highlighter-rouge">AWPROT</code> and <code class="language-plaintext highlighter-rouge">ARPROT</code>, that are rarely used by
any of my designs.  However, they are official AXI-Lite (and AXI) signals.
As a result,
<a href="/zipcpu/2017/10/05/autofpga-intro.html">AutoFPGA</a>
will always try to connect them to an AXI-Lite (or AXI) port, yet none of my
designs use these.  This leads to another set of exceptions that needs to be
made when measuring coverage.</p>

<p>So, coverage metrics aren’t perfect.  Still, they can help me find
what parts of the design are (and are not) being tested well.  This can then
help feed into better (and more complete) test design.</p>

<p>That’s the good news.  Now let’s talk about some of the not so good parts.</p>

<p>When learning formal verification, I spent some time formally verifying
Xilinx IP.  After finding several bugs, I spoke to a Xilinx executive
regarding how they verified their IP.  Did they use formal methods?  No.
Did they use their own AXI Verification IP?  No.  Yet, they were very proud of
how well they had verified their IP.  Specifically, their executive bragged
about how good their coverage metrics were, and the number of test points
checked for each IP.</p>

<p>Hmm.</p>

<p>So, let me get this straight: Xilinx IP gets good coverage metrics, and hits
a large number of test points, yet still has bugs within it that I can find
via formal methods?</p>

<p>Okay, so … how severe are these bugs?  In one case, the bugs would totally
break the AXI bus and bring the system containing the IP down to a screeching
halt–if the bug were ever tripped.  For example, if the system requested both
a read burst and a write burst at the same time, one particular slave might
accomplish the read burst with the length of the write burst–or vice versa.
(It’s been a while, so I’d have to look up the details to be exact regarding
them.)  In another case dealing with a network controller, it was possible
to receive a network packet, capture that packet correctly, and then return
a corrupted packet simply because the <a href="/blog/2021/08/28/axi-rules.html">AXI bus
handshakes</a> weren’t properly
implemented.  To this day this bugs have not been fixed, and it’s nearly five
years later.</p>

<p>Put simply, if it is possible for an IP to lock up your system completely,
then that IP shouldn’t be trusted until the bug is fixed.</p>

<p>How then did Xilinx manage to convince themselves that their IP was high
quality?  By “good” coverage metrics.</p>

<p>Lesson learned?  <a href="https://en.wikipedia.org/wiki/Code_coverage">Coverage
checking</a> is a good thing, and it
can reveal holes in any simulation-based verification suite.  It’s just not
good enough on its own to find all of what you are missing.</p>

<p>My conclusion?  Formal verification, followed by a simulation test suite that
evaluates coverage statistics is something to pay attention to, but not the
end all be-all.  One tool isn’t enough.  Many tools are required.</p>

<h2 id="self-checking-testbenches">Self-Checking Testbenches</h2>

<p>I then got involved with ASIC design.</p>

<p><a href="/blog/2017/10/13/fpga-v-asic.html">ASIC design differs from FPGA design in a couple of
ways</a>.  Chief among them
is the fact that the ASIC design must work the first time.  There’s little to
no room for error.</p>

<table align="center" style="float: right"><caption>Fig 1. A typical verification environment</caption><tr><td><img src="/img/vjourney/verilogtb.svg" width="320" /></td></tr></table>

<p>When working with my first ASIC design, I was introduced to a more formalized
simulation flow.  Let me explain it this way, looking at Fig. 1.  Designs
tend to have two interfaces: a bus interface, together with a device I/O
interface.  A test script can then be used to drive some form of bus functional
model, which will then control the design under test via its bus interface.  A
device model would then mimic the device the design was intended to talk to.
When done well, the test script would evaluate the values returned by the
design–after interacting with the device, and declare “success” or “failure”.</p>

<p>Here’s the key to this setup: I can run many different tests from this starting
point by simply changing the test script and nothing else.</p>

<p>For example, let’s imagine an external memory controller.  A “good” memory
controller should be able to accept any bus request, convert it into
I/O wires to interact with the external memory, and then return a response from
the memory.  Hence, it should be possible to first write to the external memory
and then (later) read from the same external memory.  Whatever is then read
should match what was written previously.  This is the minimum test
case–measuring the “contract” with the memory.</p>

<p>Other test cases might evaluate this contract across all of the modes the
memory supports.  Still other cases might attempt to trigger all of the faults
the design is supposed to be able to handle.  The only difference between these
many test cases would then be their test scripts.  Again, you can measure
whether or not the test cases are sufficient using coverage measures.</p>

<p>The key here is that all of the test cases must produce either a “pass” or
“fail” result.  That is, they must be self-checking.  Now, using self checking
test cases, I can verify (via simulation) something like the 
<a href="/about/zipcpu.html">ZipCPU</a> across all of its instructions,
in SMP and single CPU environments, using the DMA (or not), and so forth.
Indeed, the <a href="/about/zipcpu.html">ZipCPU</a>’s test environment
takes this approach one step farther, by not just changing the test script
(in this case a <a href="/about/zipcpu.html">ZipCPU</a> software program)
but also the configuration of the test environment as well.  This allows me
to make sure the <a href="/about/zipcpu.html">ZipCPU</a> will continue
to work in 32b, 64b, or even wider bus environments in a single test suite.</p>

<p>Yes, this was a problem I was having before I adopted this methodology: I’d
test the <a href="/about/zipcpu.html">ZipCPU</a> with a 32b bus, and then
deploy the <a href="/about/zipcpu.html">ZipCPU</a> to a board whose
memory was 64b wide or wider.  The <a href="https://github.com/ZipCPU/kimos">Kimos
project</a>, for example, has a 512b bus.  Now
that I run test cases on multiple bus widths, I have the confidence that I
can easily adjust the <a href="/about/zipcpu.html">ZipCPU</a> from one
bus width to another.</p>

<p>This is now as far as I’ve now come in my verification journey.  I now use
formal tests, simulation tests, coverage checking, and a self-checking test
suite on new design components.  Is this perfect?  No, but at least its more
rigorous and repeatable than where I started from.</p>

<h2 id="next-steps-softwarehardware-interaction">Next Steps: Software/Hardware interaction</h2>

<p>The testing regiment discussed above continues to have a very large and
significant hole: I can’t test software drivers very well.</p>

<p>Consider as an example my <a href="https://github.com/ZipCPU/sdsdpi">SD card
controller</a>.  The 
<a href="https://github.com/ZipCPU/sdsdpi">repository</a> actually contains three
controllers: <a href="https://github.com/ZipCPU/sdspi/blob/master/rtl/sdspi.v">one for interacting with SD cards via their SPI
interface</a>, <a href="https://github.com/ZipCPU/sdspi/blob/master/rtl/sdio_top.v">one via
the SDIO interface</a>,
and a third for use with eMMC cards (<a href="https://github.com/ZipCPU/sdspi/blob/master/rtl/sdio_top.v">using the SDIO
interface</a>).
The <a href="https://github.com/ZipCPU/sdsdpi">repository</a> contains formal proofs
for all leaf modules, and two types of SD card models–a <a href="https://github.com/ZipCPU/blob/master/bench/cpp/sdspi.cpp">C++ model for
SPI</a> and all Verilog
models for
<a href="https://github.com/ZipCPU/blob/master/sdspi/bench/verilog/mdl_sdio.v">SDIO</a> and
<a href="https://github.com/ZipCPU/sdspi/blob/master/bench/verilog/mdl_emmc.v">eMMC</a>.</p>

<p>This controller IP also contains a set of <a href="https://github.com/ZipCPU/sdspi/tree/master/sw">software
drivers</a> for use when working
with SD cards.  Ideally, these drivers should be tested together with the
<a href="https://github.com/ZipCPU/sdsdpi">SD card controller(s)</a>, so they could be
verified together.</p>

<p>Recently, for example, I added a <a href="https://github.com/ZipCPU/sdspi/blob/master/rtl/sddma.v">DMA
capability</a> to the
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
version of <a href="https://github.com/ZipCPU/sdspi/blob/master/rtl/sdio.v">the SDIO (and eMMC)
controller(s)</a>.  This
(new) <a href="https://github.com/ZipCPU/sdspi/blob/master/rtl/sddma.v">DMA
capability</a>
then necessitated quite a few changes to the
<a href="https://github.com/ZipCPU/sdspi/tree/master/sw">control software</a>, so that it
could take advantage of it.  With no tests, how well do you think
<a href="https://github.com/ZipCPU/sdspi/blob/master/sw/sdiodrv.c">this software</a>
worked when I first tested it in hardware?</p>

<p>It didn’t.</p>

<p>So, for now, the <a href="https://github.com/ZipCPU/sdspi/tree/master/sw">software
directory</a> simply holds the
software I will copy to other designs and test in actual hardware.</p>

<table align="center" style="float: right"><caption>Fig 2. Software driven test bench</caption><tr><td><img src="/img/cpusim/softwaretb.svg" width="320" /></td></tr></table>

<p>The problem is, testing the <a href="https://github.com/ZipCPU/sdspi/tree/master/sw">software
directory</a> requires many
design components beyond just the
<a href="https://github.com/ZipCPU/sdspi">SD card controllers</a> that would be under test.
It requires memory, a console port, a CPU, and the CPU’s tool chain–all in
addition to the <a href="https://github.com/ZipCPU/sdspi">design</a> under test.
These extra components aren’t a part of the <a href="https://github.com/ZipCPU/sdspi">SD controller
repository</a>, nor perhaps should they be.  How
then should these <a href="https://github.com/ZipCPU/sdspi/tree/master/sw">software
drivers</a> be tested?</p>

<p>Necessity breeds invention, so I’m sure I’ll eventually solve this problem.
This is just as far as I’ve gotten so far.</p>

<h2 id="automated-testing">Automated testing</h2>

<p>At any rate, I submitted this
<a href="https://github.com/ZipCPU/sdspi">repository</a> to an automated continuous
integration facility the team I was working with was testing.  The utility
leans heavily on the existence of a variety of <code class="language-plaintext highlighter-rouge">make test</code> capabilities within
the <a href="https://github.com/ZipCPU/sdspi">repository</a>, and so the
<a href="https://github.com/ZipCPU/sdspi">SD Card repository</a> was a good fit for
testing.  Along the way, I needed some help from the test facility engineer to
get <a href="https://github.com/YosysHQ/sby">SymbiYosys</a>,
<a href="https://github.com/steveicarus/iverilog">IVerilog</a> and
<a href="https://www.veripool.org/verilator/">Verilator</a> capabilities installed.  His
response?</p>

<blockquote>
  <p>It’s literally the first time I get to know a good hardware project needs
such many verifications and testings!  There’s even a real SD card
simulation model and RW test…</p>
</blockquote>

<p>Yeah.  Actually, there’s three SD card models–as discussed above.  It’s been
a long road to get to this point, and I’ve certainly learned a lot along the
way.</p>

  </div>


<div class "verse">
<HR align="center;" width="25%">
<P><em>Watch therefore: for ye know not what hour your Lord doth come. (Matt 24:42)</em>


</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The ZipCPU by Gisselquist Technology</h2>
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <!-- <li></li> -->
          <li><a href="mailto:zipcpu@gmail.com">zipcpu@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="soc-medlist">
          
          <li>
            <a href="https://github.com/ZipCPU"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">ZipCPU</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/zipcpu"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">@zipcpu</span></a>

          </li>
          
          
          <li><A href="https://www.patreon.com/ZipCPU"><img src="/img/become_a_patron_button.png"></a></li>
          

        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The ZipCPU blog, featuring how to discussions of FPGA and soft-core CPU design.  This site will be focused on Verilog solutions, using exclusively OpenSource IP products for FPGA design.  Particular focus areas include topics often left out of more mainstream FPGA design courses such as how to debug an FPGA design.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
