<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Wrap addressing</title>
  <meta name="description" content="Welcome to the ZipCPU blog.  I started it years ago after building my ownsoft core CPU, the ZipCPU, anddedicated this blog to helping individuals stay out of...">

  <link rel="shortcut icon" type="image/x-icon" href="/img/GT.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://zipcpu.com/zipcpu/2025/03/29/pfwrap.html">
  <link rel="alternate" type="application/rss+xml" title="The ZipCPU by Gisselquist Technology" href="https://zipcpu.com/feed.xml">
</head>


  <body>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4ZK7HKHSVW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4ZK7HKHSVW');
</script>

    <header class="site-header">
  <div id="banner">
  <a href="/"><picture>
    <img height=120 id="site-logo" src="/img/fullgqtech.png" alt="Gisselquist Technology, LLC">
  </picture></A>
  </div>

  <div class="site-nav">
<ul>

<li><a HREF="/">Main/Blog</a>


<li><a HREF="/about/">About Us</a>


<li><a HREF="/fpga-hell.html">FPGA Hell</a>


<li><a HREF="/tutorial/">Tutorial</a>
<li><a HREF="/tutorial/formal.html">Formal training</a>


<li><a HREF="/quiz/quizzes.html">Quizzes</a>


<li><a HREF="/projects.html">Projects</a>


<li><a HREF="/topics.html">Site Index</a>

<HR>

<li><a href="https://twitter.com/zipcpu"><span class="icon--twitter"><svg viewBox="0 0 400 400"><path fill="#1da1f2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>
</span><span class="username">@zipcpu</span></a>

<li><a href="https://www.reddit.com/r/ZipCPU"><span class="username">Reddit</a>
<li><a HREF="https://www.patreon.com/ZipCPU"><IMG SRC="/img/patreon_logomark_color_on_white.png" WIDTH="25"> Support</a>
</ul>
</div>


</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="https://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Wrap addressing</h1>
    <p class="post-meta"><time datetime="2025-03-29T00:00:00-04:00" itemprop="datePublished">Mar 29, 2025</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Welcome to the <em>ZipCPU</em> blog.  I started it years ago after building my own
<a href="/about/zipcpu.html">soft core CPU, the ZipCPU</a>, and
dedicated this blog to helping individuals stay out of <a href="/fpga-hell.html">FPGA
Hell</a>.  I then transitioned from working on
the <a href="/about/zipcpu.html">ZipCPU</a>,
to building <a href="https://github.com/ZipCPU/wb2axip">bus components that might be used by every project–crossbars,
bridges, DMAs</a> and such.  Since that time,
my time is primarily spent not on the CPU, but rather its peripherals.  This
last year, for example, has seen work on several memory controllers, to
include both <a href="https://www.arasan.com/product/xspi-nor-ip/">NOR</a> and
<a href="https://www.arasan.com/products/nand-flash/">NAND</a> flash controllers, an
<a href="https://github.com/ZipCPU/sdspi">SD Card(SDIO)/eMMC controller</a>, and (now)
<a href="https://github.com/ZipCPU/wbsata">a SATA controller</a>.  I’ve also had the
opportunity to work on <a href="https://github.com/ZipCPU/eth10g">high speed
networking</a>, video, and even SONAR
applications.  All of this work is made easier by having both my own <a href="/about/zipcpu.html">soft-core
CPU</a>, together with <a href="https://github.com/ZipCPU/wb2axip">bus interconnect
components</a>, that <a href="/zipcpu/2019/02/04/debugging-that-cpu.html">I’m not afraid to dig
into to debug if
necessary</a>.</p>

<p>With all of these distractions, its nice every now and then to come back the
the <a href="/about/zipcpu.html">ZipCPU</a>.</p>

<p>One of my current projects requires that I bench mark AMD(Xilinx)’s
DDR3 SDRAM MIG controller against the open source <a href="https://github.com/AngeloJacobo/UberDDR3">UberDDR3
controller</a>.  The performance
differences are dramatic and very significant.  My current (draft) article
discussing these results works through a series of CPU and DMA based tests.
For each test, the article describes first the C code for the test, then the
assembly for the critical section, then a diagram of the CPU’s
pipeline–reconstructed from simulation traces, and then finally traces
showing the differences between the two controllers.</p>

<p>All of that led me to this trace from the data cache, shown in Fig. 1 below.</p>

<table align="center" style="float: none"><caption>Fig 1. ZipCPU Data Cache Miss</caption><tr><td><a href="/img/migbench/cp2.svg"><img src="/img/migbench/cp2.svg" width="720" /></a></td></tr></table>

<p>For quick reference, the top line is the clock.  The <code class="language-plaintext highlighter-rouge">JMP</code> line beneath it is
the signal from the <a href="/about/zipcpu.html">CPU</a>’s core to the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">instruction fetch</a> that
the <a href="/about/zipcpu.html">CPU</a>
needs to branch.  The <a href="/zipcpu/2017/08/23/cpu-pipeline.html"><code class="language-plaintext highlighter-rouge">PF</code>
line</a> shows the output
of the prefetch (cache), and whether an instruction is available for the
<a href="/about/zipcpu.html">CPU</a> to consume and if so which one.
The <a href="/zipcpu/2017/08/23/cpu-pipeline.html"><code class="language-plaintext highlighter-rouge">DCD</code> line shows the output of the instruction
decoder</a>.
<code class="language-plaintext highlighter-rouge">OP</code> is the output of the <a href="/zipcpu/2017/08/23/cpu-pipeline.html">read operands pipeline
stage</a>, and <a href="/zipcpu/2017/08/23/cpu-pipeline.html"><code class="language-plaintext highlighter-rouge">WB</code> is
the writeback stage</a>.
The <code class="language-plaintext highlighter-rouge">CYC</code>, <code class="language-plaintext highlighter-rouge">STB</code>, and <code class="language-plaintext highlighter-rouge">ACK</code> lines are a subset of the <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone
bus signaling</a>
used to communicate with memory.  First there’s the <code class="language-plaintext highlighter-rouge">Zip-*</code> version of these
signals, showing them coming out of the
<a href="/about/zipcpu.html">CPU</a>, and then the <code class="language-plaintext highlighter-rouge">SDRAM-*</code> signals
coming from the <a href="/blog/2019/07/17/crossbar.html">crossbar</a>
showing these signals actually going to the memory controller itself.</p>

<p>At issue is how long it takes the <a href="/about/zipcpu.html">CPU</a>
to respond to a cache miss.  Notice how it takes the
<a href="/about/zipcpu.html">CPU</a> 3 clock cycles from receiving an
<a href="/zipcpu2018/01/01/zipcpu-isa.html">LW (load word)
instruction</a> from the
read operands stage until when the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data cache</a>
initiates a bus request, another 3 cycles before the request can make it to
SDRAM controller, one cycle to return, and another 5 cycles from the
completion of that request before the
<a href="/about/zipcpu.html">CPU</a> can continue.  That’s 11 clock
cycles on every <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data
cache</a>
miss above and beyond the cost of the memory access itself.</p>

<p>Ouch.</p>

<p>When it comes to raw performance, every cycle counts.  Can we do better?</p>

<p>Yes, we can.  Let’s talk about <em>wrap</em> addressing today.</p>

<p>That said, I’d like to focus this article on saving a couple clock cycles in
the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v"><em>instruction</em>
cache</a> rather
than the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v"><em>data</em>
cache</a> shown
in my example.  Why?  For the simple practical reason that the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v"><em>instruction</em>
cache</a>
has been easier to update and get working–although I have yet to post the
updates.  My <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data
cache</a>
upgrades to date remain a (broken) work in progress.  Both, however, can be
motivated by the diagram in Fig. 1 above.</p>

<h2 id="wrap-addressing">Wrap Addressing</h2>

<p>What might we do to improve the performance of the trace in Fig. 1?</p>

<p>The first thing we might do is speed up how long it takes to recognize that
a particular value is not in the cache.  There’s only so much that can be
done here, however, since the cache tag memory is <em>clocked</em>.  As a result, it
will always take a clock cycle to look up the cache tag for any new request,
and another clock cycle to know it’s not the right tag, and then a third
clock cycle to activate the bus.</p>

<p>The <a href="/blog/2019/07/17/crossbar.html">crossbar</a> is separate
from the <a href="/about/zipcpu.html">CPU</a>, and its timing is
dominated by the need for a clock rate that matches the
<a href="/about/zipcpu.html">CPU</a>.</p>

<p>The <a href="https://github.com/AngeloJacobo/UberDDR3">UberDDR3 memory controller</a>
is a separate product from the <a href="/about/zipcpu.html">CPU</a>,
so its performance is independent from the
<a href="/about/zipcpu.html">CPU</a> itself.</p>

<p>How about the return?  Once a value has been returned from memory to the
cache, it then takes another clock cycle to shift the value into place for
the CPU, so there’s not much to be done there … or is there?</p>

<p>There are two optimizations that can be made on this return path.  The first
is that we can take the value directly from the bus and return it to the
<a href="/about/zipcpu.html">CPU</a>–rather than waiting for the
value to first be written to and then read back from the cache’s memory.
The second optimization is <em>wrap</em> addressing.  We’ll discuss both of these
optimizations today.</p>

<p>First, though, let me introduce the concept of a <em>cache line</em>.  A <em>cache line</em>
is the minimum amount of memory that can be read into the cache at a time.
The cache itself is composed of many of these cache lines.  Upon a cache miss,
the cache controller will always go and read a whole cache line.</p>

<p>A long discussion can be had regarding how big a cache line can or should be.
For me, I tend to follow the results published by <a href="https://www.amazon.com/Computer-Architecture-Quantitative-Approach-Kaufmann/dp/0443154066/">Hennessey and
Patterson</a>,
and keep my cache lines (roughly) 8 words in length.  For simplicity, the
<a href="/about/zipcpu.html">ZipCPU</a>’s
caches are all one-way caches, but, yes, a significant performance
can be gained by upgrading to two or even four-way caches–but that’s a story
for another day.</p>

<p>Now that you know what a cache line is, notice how the cache miss in Fig. 1
results in reading an entire cache line.  As we’ll discuss in the memory
performance benchmarking article (still to be finished), memory performance
can be quantified by latency and throughput.  Caches can get an advantage
over <a href="/zipcpu/2021/09/30/axiops.html">single-beat read or write
instructions</a> by
reading more than one beat at a time, and so increasing the line size improves
efficiency.  One problem with increasing the line size, however, is that
1) it increases the amount of time the bus is busy handling any request
(remember all requests are for a full cache line), and 2) it increases the
risk that you spend a lot of time handling requests for instructions or data
you’ll never use or need.</p>

<p>Now we can discuss wrap addressing.  Wrap addressing is a means of reading
the cache line out of order.  Without wrap addressing, we might read the
words in the cache line in order from 0-7.  With wrap addressing, the cache
will specifically read the requested item from the cache line first, then
finish to the end of the line, then go back and get what was missing from
the beginning.  This way, as soon as the word that caused the cache miss in
the first place has been read, the <a href="/about/zipcpu.html">CPU</a>
can be unblocked and continue whatever it needs to do next while the
cache controller finishes its read of the cache line.  The big difference is
that with wrap addressing the cache line is read in more of a priority fashion.
“Wrap addressing” is the just name given to this style of out of order
addressing.</p>

<p>That’s what it is.  Let’s now look at its impact.</p>

<h2 id="wrap-addressing-with-the-zipcpus-instruction-cache">Wrap Addressing with the ZipCPU’s Instruction Cache</h2>

<p>Some years ago, I added wrap addressing to the
<a href="/about/zipcpu.html">ZipCPU</a>’s <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/axiicache.v">AXI
instruction</a>
and <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/axidcache.v">data
caches</a>.  Up
until that time, I had poo-poo’d the benefit that might be had by using it.
The <a href="/about/zipcpu.html">ZipCPU</a> was designed to be a “simple”
and “low-logic” CPU, and wrap addressing would just complicate things–or so
I judged.  Then I tried it.  At the time, I just needed <em>something</em> that used
wrap addressing–the AXI bus functional model I had been given just wasn’t up to
the task, but the <a href="/about/zipcpu.html">ZipCPU</a> could issue
wrap addressing requests quite nicely.  In the process, I was surprised at how
much faster the <a href="/about/zipcpu.html">ZipCPU</a> ran when the
caches used wrap addressing.</p>

<p>That experiment died, however, once the need was over.  The big reason for it
dying was simply that I don’t use AXI often.  Sure, the
<a href="/about/zipcpu.html">ZipCPU</a> has AXI memory controllers, but
they only fit the CPU so well.  The AXI bus is little endian, and the
<a href="/about/zipcpu.html">ZipCPU</a> is big endian, so the two aren’t
a natural fit.  There’s plenty of pain at the seams.  Further, adding wrap
addressing to my
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
memory controllers was simply work that wasn’t being paid for.  No, it doesn’t
help that the <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
bus doesn’t really offer burst or wrap support, but I think you’ll find that
issue to be irrelevant to today’s discussion.</p>

<p>As a result, <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone</a>
wrap addressing for the <a href="/about/zipcpu.html">ZipCPU</a> has
therefore languished until I was recently motivated by examining the MIG and
<a href="https://github.com/AngeloJacobo/UberDDR3">UberDDR3</a> memory controller bench
mark results.  Indeed, I found myself a touch embarrassed at the performance
the <a href="/about/zipcpu.html">CPU</a> was delivering.</p>

<p>For illustration, let’s look at the first several instructions of a basic
<a href="/about/zipcpu.html">ZipCPU</a> test program I use.  We’ll
break it into two portions.  There’s the first several instructions.</p>

<figure class="highlight"><pre><code class="language-asm" data-lang="asm">	; Clear all registers
	;   The "|" separates two instructions, both of which are
	;   packed into a single instruction word.
 4000000:	86 00 8e 00 	CLR        R0            | CLR        R1
 4000004:	96 00 9e 00 	CLR        R2            | CLR        R3
 4000008:	a6 00 ae 00 	CLR        R4            | CLR        R5
 400000c:	b6 00 be 00 	CLR        R6            | CLR        R7
 4000010:	c6 00 ce 00 	CLR        R8            | CLR        R9
 4000014:	d6 00 de 00 	CLR        R10           | CLR        R11
 4000018:	66 00 00 00 	CLR        R12
	; Set up the initial stack stack pointer
 400001c:	6a 00 00 10 	LDI        0x08000000,SP	; Top of stack
 4000020:	6a 40 00 00 
	; Guarantee we are in supervisor mode, and trap into supervisor
	; mode if not
 4000024:	76 00 00 00 	TRAP
	; Provide a set of initial values for all of the user registers 
 4000028:	7b 47 c0 1e 	MOV        $120+PC,uPC
 400002c:	03 44 00 00 	MOV        R0,uR0
 4000030:	0b 44 00 00 	MOV        R0,uR1
 4000034:	13 44 00 00 	MOV        R0,uR2
 4000038:	1b 44 00 00 	MOV        R0,uR3
 400003c:	23 44 00 00 	MOV        R0,uR4</code></pre></figure>

<p>These get us to the end of the first cache line and now the beginning of the
second.  Take note that there have been no jumps or branches in this
assembly, it’s just straightforward walking from one instruction to the
next through the test program.  (Yes, we’ll get to branches soon enough.)</p>

<p>The instructions then continue loading the user register set with default
values.</p>

<figure class="highlight"><pre><code class="language-asm" data-lang="asm"> 4000040:	2b 44 00 00 	MOV        R0,uR5
 4000044:	33 44 00 00 	MOV        R0,uR6
 4000048:	3b 44 00 00 	MOV        R0,uR7
 400004c:	43 44 00 00 	MOV        R0,uR8
 4000050:	4b 44 00 00 	MOV        R0,uR9
 4000054:	53 44 00 00 	MOV        R0,uR10
 4000058:	5b 44 00 00 	MOV        R0,uR11
 400005c:	63 44 00 00 	MOV        R0,uR12
 4000060:	6b 44 00 00 	MOV        R0,uSP
 4000064:	73 44 00 00 	MOV        R0,uCC
	; Finally, we call the bootloader function to load software into RAM
	; from flash if necessary (it isn't in this case), and to zero any
	; uninitialized global values
 4000068:	03 43 c0 02 	LJSR       @0x040000b4    // Bootloader
 400006c:	7c 87 c0 00 
 4000070:	04 00 00 b4 
	; Software continues, but the next section is outside the scope
	; of today's discussion.
	; ....</code></pre></figure>

<p>These end with a jump to subroutine instruction, followed by the beginning
of the “_bootloader” subroutine below.</p>

<p>In this case, the cache line starts at address 0x04000080.  However, we don’t
start executing there in our example.  Instead, we start executing
partway through the cache line at the beginning of the bootloader
subroutine.</p>

<figure class="highlight"><pre><code class="language-asm" data-lang="asm">040000b4 &lt;\_bootloader&gt;:
	; Our first step is to create a stack frame.  For this, we
	; subtract from the stack pointer, and then store any
	; registers we might clobber onto the stack.  As before,
	; the "|" separates two instructions, both of which are
	; packed into a single instruction word.
 40000b4:	e8 10 ad 00 	SUB        $16,SP        | SW         R5,(SP)
 40000b8:	b5 04 bd 08 	SW         R6,$4(SP)     | SW         R7,$8(SP)
 40000bc:	44 c7 40 0c 	SW         R8,$12(SP)
 40000c0:	0a 00 00 00 	LDI        0x00000004,R1
 40000c4:	0a 40 00 04 
 40000c8:	0c 00 00 04 	CMP        $4,R1
 40000cc:	78 88 01 0c 	BZ         @0x040001dc
 40000d0:	0a 00 00 00 	LDI        0x00000004,R1
 40000d4:	0a 40 00 04 
 40000d8:	0c 00 00 04 	CMP        $4,R1
 40000dc:	32 08 00 20 	LDI.Z      0x04000000,R6
	; ....</code></pre></figure>

<p>Together, these two sets of instructions make an awesome example to see how
wrap addressing would work from an instruction fetch perspective.</p>

<p>One of the things I like about this example is the fact that the test starts
with many sequential instructions and no jumps (branches).  This will help
provide us a baseline of how things work–before jumps start making things
complicated.</p>

<p>For today’s discussion, our cache line size is 8 words, each having 64bits.
The <a href="/about/zipcpu.html">ZipCPU</a>’s nominal instruction
size is 32bits.  Therefore, each cache line will nominally contain 16
instructions.  Our first cache line, however, contains many clear (CLR)
instructions (really load-immediate 0 into register …), and two of these
instructions can be packed into a single 32b word.  This is shown above using
the “|” characters.  Fig. 2 shows how the <a href="/zipcpu/2017/08/23/cpu-pipeline.html">CPU
pipeline</a>
works through these initial instructions–without wrap adddressing.</p>

<table align="center" style="float: none"><caption>Fig 2. Starting the cache, without wrap addressing</caption><tr><td><a href="/img/pfwrap/pf-startup.svg"><img src="/img/pfwrap/pf-startup.svg" width="720" /></a></td></tr></table>

<p>Following the CPU reset, the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">cache</a>
starts with the JUMP flag set.  Following a jump, it takes us 4 clock cycles
to determine that the new address is not in the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">cache</a>
and to therefore start a bus cycle.</p>

<p>This bus cycle is painful.  When using the MIG, it requires a (rough)
35 cycles (on a good day) to read all eight words.  When using the
<a href="https://github.com/AngeloJacobo/UberDDR3">UberDDR3</a> controller,
it requires a (rough) 18 cycles.  Since the
<a href="/about/zipcpu.html">ZipCPU</a>
can nominally execute one instruction per cycle, this is a painful wait.</p>

<p>Once the bus cycle completes, we take another two cycles to present the
instruction from the cache line that we just read to the
<a href="/about/zipcpu.html">CPU</a>.  The decoder
then takes two clock cycles with this instruction, since it contains two
instructions packed into a single word, and so forth.  From here on out,
instructions are passed to the
<a href="/about/zipcpu.html">CPU</a> at one instruction word per clock
cycle–unless the <a href="/about/zipcpu.html">CPU</a>
needs to take more clock cycles with them–as is the case of the <a href="/zipcpu2018/01/01/zipcpu-isa.html">compressed
instruction</a>.</p>

<p>Some instructions, such as the <a href="/zipcpu2018/01/01/zipcpu-isa.html">load immediate
instruction</a>, are actually
two separate instructions–a bit reverse instruction to load the high order
bits and a load immediate lo.
Other than that, things stay straight forward until the end of the cache line.
Once we get to the end, it takes us another 4 cycles to determine the next
instruction is not in the cache, and so a new cycle begins again.</p>

<p>Now that we now how things work normally, we have our first chance for an
improvement: what if we started feeding instructions to the
<a href="/about/zipcpu.html">CPU</a> <em>before</em>
all of the instructions had been read from memory and returned across the bus?
What if we fed the next instruction to the
<a href="/about/zipcpu.html">CPU</a>
as soon as it was available?</p>

<p>In that case, we might see a trace similar to Fig. 3 below.</p>

<table align="center" style="float: none"><caption>Fig 3. Feeding instructions straight from the bus returns</caption><tr><td><a href="/img/pfwrap/wrap-startup.svg"><img src="/img/pfwrap/wrap-startup.svg" width="720" /></a></td></tr></table>

<p>We can now overlap our instruction read time with our instruction issue,
saving ourselves a full 10 cycles!</p>

<p>Let’s follow this further.  What would happen in the case of a jump/branch?
Without any modifications to <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">our instruction
cache</a>
(i.e. before <em>wrap</em> addressing), the JSR initiates a jump at the end of
Fig. 4 below.</p>

<table align="center" style="float: none"><caption>Fig 4. A Jump Instruction</caption><tr><td><a href="/img/pfwrap/pf-jsr.svg"><img src="/img/pfwrap/pf-jsr.svg" width="720" /></a></td></tr></table>

<p>This trace is a touch more eventful.  For example, it includes a move to the
<a href="/zipcpu2018/01/01/zipcpu-isa.html">CC register</a>.
On the <a href="/about/zipcpu.html">ZipCPU</a>, this register
contains more than just the condition codes.  It also contains the
<a href="/zipcpu2018/01/01/zipcpu-isa.html">user vs supervisor mode
control</a>.  This creates a
pipeline hazard, and so instructions need to be stalled throughout the pipeline
until this instruction has had a chance to write back–clearing the hazard.</p>

<p>The <a href="/about/zipcpu.html">ZipCPU</a>’s
<a href="/zipcpu2018/01/01/zipcpu-isa.html">JSR instruction</a>
follows, requiring three instruction words.  The first instruction word moves
the program counter plus two into R0.  This will now contain the return address
for the subroutine.  On other architectures, such an instruction is often
called a “Link Register” instruction, but on the
<a href="/about/zipcpu.html">ZipCPU</a>
this is simply the first of the three word
<a href="/zipcpu2018/01/01/zipcpu-isa.html">JSR instruction</a>.
The second instruction loads a new value into the program register.
Technically, this is a <a href="/zipcpu2018/01/01/zipcpu-isa.html"><code class="language-plaintext highlighter-rouge">LW (PC),PC</code> instruction–loading the
value of memory, as found at the program counter, into the program
counter</a>.
Practically, it just allows us to place a 32b destination address into the
instruction stream.  Once the address is passed to the decoder, the decoder
recognizes the unconditional jump and sets a flag for the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction
cache</a>
that it now wants a new instruction out of order.  The <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction
cache</a>
now takes four clock cycles to determine this new value is not in the cache,
and our cycle repeats.</p>

<p>As before, we can compress this a touch by serving our instructions to the
<a href="/about/zipcpu.html">CPU</a>
immediately as they are read from the bus–instead of waiting for the
entire cache line to be read first.  You can see how this optimization might
speed things up in Fig. 5.</p>

<table align="center" style="float: none"><caption>Fig 5. JSR instruction, post optimization</caption><tr><td><a href="/img/pfwrap/wrap-jsr.svg"><img src="/img/pfwrap/wrap-jsr.svg" width="720" /></a></td></tr></table>

<p>That’s how the first of our two optimizations works.</p>

<p>Following the jump, without WRAP addressing, the pipeline would look like
FIG 6.</p>

<table align="center" style="float: none"><caption>Fig 6. JSR Landing, no optimization</caption><tr><td><a href="/img/pfwrap/pf-land.svg"><img src="/img/pfwrap/pf-land.svg" width="720" /></a></td></tr></table>

<p>To see what’s happening here, notice that we just jumped to address
0x040000b4.  Given our cache line size of
eight words, with each word being 64bits, this cache line starts at address
0x04000080.  If we just returned the value from the bus as soon as it was
available, we’d have to read six bus words before we get to the one we’re
interested in–as shown in Fig. 6.</p>

<figure class="highlight"><pre><code class="language-text" data-lang="text"> 4000080:	; Word 0: I don't care about these instructions.  I'm jumping
 4000084:	;	to address 0x040000b4.  I just have to read
 4000088:	; Word 1: these excess instructions because I'm operating on an
 400008c:		entire cache line.
 4000090:	; Word 2:
 4000094:
 4000098:	; Word 3: Still haven't gotten to anything I care about ...
 400009c:
 40000a0:	; Word 4:
 40000a4:
 40000a8:	; Word 5:
 40000ac:
 40000b0:	; Word 6: This is the first half of the word I do care about
 40000b4:	;	THIS IS THE FIRST INSN OF INTEREST!
 40000b8:	; Word 7:
 40000bc:	;</code></pre></figure>

<p>Why not, instead, request the address we are interested in first?  Instead
of starting with word 0, and reading until word 6, we might instead start with
word 6, read word 7, and then finish by reading the first part of the cache
line (words 0-5) while the <a href="/about/zipcpu.html">CPU</a>
takes our instruction and gets (potentially) busy doing useful things.</p>

<p>Fig. 7 shows how this wrap addressing might look.</p>

<table align="center" style="float: none"><caption>Fig 7. Instruction cache miss using WRAP addressing</caption><tr><td><a href="/img/pfwrap/wrap-land.svg"><img src="/img/pfwrap/wrap-land.svg" width="720" /></a></td></tr></table>

<p>Here, we request the last two instruction words, words 6 and 7, of the cache
line, and then instruction words 0-5.  Word 6 contains two instructions, but
we’re only interested in the second of those two.  That one is a compressed
instruction, packing two instructions into 32bits.  Word 7 then contains
another three instructions–one packed instruction word and one normal one.</p>

<p>The trace gets a touch more interesting, though, given that the second
instruction wants to <em>store</em> a word into memory.  The
<a href="/about/zipcpu.html">ZipCPU</a>,
however, has only one bus interface–an interface that needs to be shared
between instruction and data bus accesses.  This means that the data access,
i.e. the store word instruction, must wait until the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction
cache</a>’s bus
cycle completes.</p>

<h2 id="conclusions">Conclusions</h2>

<p>The next step in this article should really be an analysis section that
artificially quantifies the additional performance achieved by using wrap
addressing over what I had been using.  This should then be compared against
some actual performance measure.  Sadly, that’s one part of caches
that I haven’t managed to get right–the performance analysis.  Even worse,
the lack of a solid ability to analyze this improvement has kept me from
writing an article introducing the
<a href="/about/zipcpu.html">ZipCPU</a>’s <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction
cache</a>
in the first place.  Perhaps I’ll manage to come back to this later–although
it’s held me back for a couple of years now.</p>

<p>Since I haven’t presented the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction
cache</a> in
the first place, it doesn’t really make sense to write an article presenting
the <em>modifications</em> required to introduce wrap addressing.  That said, it was
easier to do than I was expecting.</p>

<table align="center" style="float: right; padding: 25px"><caption>Fig 8. Is formal worth it?</caption><tr><td><img src="/img/pfwrap/formal-value.svg" width="420" /></td></tr></table>

<p>I suppose “easier” is a relative term.  I upgraded both
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction</a>
and <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data</a>
caches quickly–perhaps even in an afternoon.  Then, when everything
failed in simulation, I reverted the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data cache</a>
updates to focus on the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction</a>
cache updates.  Those updates are now complete, as is their formal proof,
so I expect I’ll push them soon.  All in all, the work took me a couple of
days to do spread over a month or so, with (as expected) the verification
part taking the longest.</p>

<p>No, the updates aren’nt (yet) posted.  Why not?  Because this update lies
behind the <a href="/about/zipcpu.html">ZipCPU</a>’s AXI DMA upgrade,
and … that one still has bugs to be worked out in it.  What bugs?  Well,
after posting the DMA initially, I then decided I wanted to change how the
DMA handled unaligned FIXed addressing.  My typical answer to unaligned FIX
addressing is to declare it disallowed in the user manual, but for some reason
I thought I might support it.  The new/changed requirements then made it so
that nothing worked, and so I have some updates left to do there before
formal proofs and simulations pass again.</p>

<p>So my next steps are to 1) repeat this work with the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data cache</a>,
and 2) finish working with the
<a href="/about/zipcpu.html">ZipCPU</a>’s DMA, so that 3) I can post
another upgrade to the <a href="/about/zipcpu.html">ZipCPU</a>’s
repository.  In the meantime, I’ll probably post my DDR3 controller memory
performance benchmarks before these updates hit the
<a href="/about/zipcpu.html">ZipCPU</a>
<a href="https://github.com/ZipCPU/zipcpu">official repository</a>.</p>

<p>For now, let me point out that the WRAP addressing performance is significantly
better, and the logic cost associated with it is (surprisingly) rather minimal.
How much better?  Well, that answer will have to wait until I can do a better
job quantifying cache performance …</p>

  </div>


<div class "verse">
<HR align="center;" width="25%">
<P><em>So the last shall be first, and the first last: for many be called, but few chosen.  -- Matt 20:16</em>


</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The ZipCPU by Gisselquist Technology</h2>
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <!-- <li></li> -->
          <li><a href="mailto:zipcpu@gmail.com">zipcpu@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="soc-medlist">
          
          <li>
            <a href="https://github.com/ZipCPU"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">ZipCPU</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/zipcpu"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">@zipcpu</span></a>

          </li>
          
          
          <li><A href="https://www.patreon.com/ZipCPU"><img src="/img/become_a_patron_button.png"></a></li>
          

        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The ZipCPU blog, featuring how to discussions of FPGA and soft-core CPU design.  This site will be focused on Verilog solutions, using exclusively OpenSource IP products for FPGA design.  Particular focus areas include topics often left out of more mainstream FPGA design courses such as how to debug an FPGA design.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
