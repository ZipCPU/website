<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Why does blinky make a CPU appear to be so slow?</title>
  <meta name="description" content="Imagine if you will that you’ve just built a brand newsoft-coreCPU.How fast do you think your brand-newsoft-coreCPUwill be able to toggle a GPIOpin?">

  <link rel="shortcut icon" type="image/x-icon" href="/img/GT.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://zipcpu.com/zipcpu/2019/02/09/cpu-blinky.html">
  <link rel="alternate" type="application/rss+xml" title="The ZipCPU by Gisselquist Technology" href="https://zipcpu.com/feed.xml">
</head>


  <body>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4ZK7HKHSVW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-4ZK7HKHSVW');
</script>

    <header class="site-header">
  <div id="banner">
  <a href="/"><picture>
    <img height=120 id="site-logo" src="/img/fullgqtech.png" alt="Gisselquist Technology, LLC">
  </picture></A>
  </div>

  <div class="site-nav">
<ul>

<li><a HREF="/">Main/Blog</a>


<li><a HREF="/about/">About Us</a>


<li><a HREF="/fpga-hell.html">FPGA Hell</a>


<li><a HREF="/tutorial/">Tutorial</a>
<li><a HREF="/tutorial/formal.html">Formal training</a>


<li><a HREF="/quiz/quizzes.html">Quizzes</a>


<li><a HREF="/projects.html">Projects</a>


<li><a HREF="/topics.html">Site Index</a>

<HR>

<li><a href="https://twitter.com/zipcpu"><span class="icon--twitter"><svg viewBox="0 0 400 400"><path fill="#1da1f2" d="M153.62,301.59c94.34,0,145.94-78.16,145.94-145.94,0-2.22,0-4.43-.15-6.63A104.36,104.36,0,0,0,325,122.47a102.38,102.38,0,0,1-29.46,8.07,51.47,51.47,0,0,0,22.55-28.37,102.79,102.79,0,0,1-32.57,12.45,51.34,51.34,0,0,0-87.41,46.78A145.62,145.62,0,0,1,92.4,107.81a51.33,51.33,0,0,0,15.88,68.47A50.91,50.91,0,0,1,85,169.86c0,.21,0,.43,0,.65a51.31,51.31,0,0,0,41.15,50.28,51.21,51.21,0,0,1-23.16.88,51.35,51.35,0,0,0,47.92,35.62,102.92,102.92,0,0,1-63.7,22A104.41,104.41,0,0,1,75,278.55a145.21,145.21,0,0,0,78.62,23"/></svg>
</span><span class="username">@zipcpu</span></a>

<li><a href="https://www.reddit.com/r/ZipCPU"><span class="username">Reddit</a>
<li><a HREF="https://www.patreon.com/ZipCPU"><IMG SRC="/img/patreon_logomark_color_on_white.png" WIDTH="25"> Support</a>
</ul>
</div>


</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="https://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Why does blinky make a CPU appear to be so slow?</h1>
    <p class="post-meta"><time datetime="2019-02-09T00:00:00-05:00" itemprop="datePublished">Feb 9, 2019</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Imagine if you will that you’ve just built a brand new
<a href="https://en.wikipedia.org/wiki/Soft_microprocessor">soft-core</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>.
How fast do you think your brand-new
<a href="https://en.wikipedia.org/wiki/Soft_microprocessor">soft-core</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
will be able to toggle a <a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO
pin</a>?</p>

<table align="center" style="float: right"><caption>Fig 1. Comparing several GPIO toggle rates</caption><tr><td><img src="/img/tweets/gpio.svg" alt="" width="360" /></td></tr></table>

<p>The question is a fascinating one, and it applies to more than just home-brew
<a href="https://en.wikipedia.org/wiki/Soft_microprocessor">soft cores</a>.
Several individuals for example have been surprised that the
<a href="https://www.xilinx.com">Xilinx</a>
<a href="https://en.wikipedia.org/wiki/MicroBlaze">MicroBlaze</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
can’t toggle a
<a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO pin</a>
very fast.  One individual <a href="https://forum.digilentinc.com/topic/4930-arty-microblaze-speed-question">measured his 82MHz
MicroBlaze CPU toggling an IO pin at
37kHz</a>.
Another looked at his <a href="https://forums.xilinx.com/t5/Embedded-Processor-System-Design/Why-microblaze-loop-speed-is-40-times-slower-than-I-expected/td-p/111342">80MHz MicroBlaze CPU, and measured his I/O toggle rate
only at 2.5MHz</a>.
Still others measured a 100MHz
<a href="https://en.wikipedia.org/wiki/MicroBlaze">MicroBlaze</a> toggling an I/O at 
<a href="https://forums.xilinx.com/t5/7-Series-FPGAs/SDK-GPIO-implementation/td-p/784626">1.7MHz</a>
or <a href="https://forums.xilinx.com/t5/Embedded-Processor-System-Design/AXI-GPIO-max-rate/td-p/484496">2.3MHz</a>.
The problem isn’t unique to
<a href="https://en.wikipedia.org/wiki/MicroBlaze">MicroBlaze</a>
either.  Using a Zynq with a 250MHz
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
clock, someone else <a href="https://forums.xilinx.com/t5/Evaulation-Boards/AXI_GPIO-too-slow/td-p/725431">measured the IO pins toggle frequency at no more than
3.8 MHz</a>.
Without insight into these architectures and their
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
implementations, it’s hard to answer why these
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>s
achieve the toggle rates they do.</p>

<p>This isn’t true of the <a href="/about/zipcpu.html">ZipCPU</a>.
The <a href="/about/zipcpu.html">ZipCPU</a>’s
implementation is entirely open and available for inspection.
It’s not closed source.  In other words, using an open source
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
like
<a href="/about/zipcpu.html">this one</a>
we should be able to answer the basic question, “Why do
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>s
toggle <a href="https://en.wikipedia.org/wiki/General-purpose_input/output">I/O pins</a>
so slowly?”  We might even get so far as to answer the question of, “How much
I/O speed might I expect from a
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>?”
But this latter question is really
very
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
dependent, and we might be pushing our luck to answer it today.</p>

<p>So, let’s take a look at a basic
<a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO</a> controller.
We can then run some tests on the
<a href="/about/zipcpu.html">ZipCPU</a>, to see how fast the
<a href="/about/zipcpu.html">ZipCPU</a> can toggle an LED from
<a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">software</a>.</p>

<h2 id="a-basic-gpio-controller">A Basic GPIO controller</h2>

<p><a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO</a>
controllers are a dime a dozen.  They are easy to build and easy to
implement.  If you are an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
developer and haven’t built your own
<a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO</a>
controller before, then let me encourage you to do so as a good exercise.</p>

<p>For this article, I’ll just point out a couple features of the <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">GPIO controller
I use</a>
on many of my designs.  If you are a regular reader of this blog, you’ll
already know that I use the
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone Bus</a>.
You’ll also recognize the
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone I/O signals from our earlier article on the
topic</a>.  So
I’m not going to repeat those here.</p>

<p><a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">My own GPIO
controller</a>, one I
call <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">WBGPIO</a>,
handles up to 16 inputs and 16 outputs as part of a single 32-bit
register.  The top
16-bits are input bits, whereas the bottom 16 are output bits.  Not all of
these bits need to be wired in any given design.  Further, all of the
input/output wires have fixed directions in this controller.
I basically judged that, at least on an
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>,
by the time you’ve wired everything up to build your design you already
know which direction the pins need to go.</p>

<p>The <a href="https://openrisc.io">OpenRISC</a> ecosystem offers a <a href="https://github.com/openrisc/orpsoc-cores/blob/master/cores/gpio/gpio.v">nice alternative
if you want to examine a controller where the pins have a programmable
direction</a>,
but I digress.</p>

<p>In the <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">WBGPIO
controller</a>,
adjusting an output bit requires writing to two
bits in the control word at once.  First, you want to set the new value of
the bit, but second, in order to avoid the need to set all of the other output
bits, you also need to set a second bit in the upper half of the register.
The software supporting <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">this
controller</a>,
therefore includes the definitions:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="cp">#define GPIO_SET(WIRE)		(((WIRE)&lt;&lt;16)|(WIRE))
#define GPIO_CLEAR(WIRE)	((WIRE)&lt;&lt;16)</span></code></pre></figure>

<p>This means that we can set a bit,</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++">	<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_SET</span><span class="p">(</span><span class="n">LED0</span><span class="p">);</span></code></pre></figure>

<p>or even clear the same bit,</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++">	<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_CLEAR</span><span class="p">(</span><span class="n">LED0</span><span class="p">);</span></code></pre></figure>

<p>without needing to first read the register and adjust the one bit of interest,
as in,</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++">	<span class="c1">// This is not the WBIO approach:</span>
	<span class="c1">//</span>
	<span class="c1">// Setting a bit without hardware support</span>
	<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">_gpio</span><span class="p">)</span> <span class="o">|</span> <span class="n">LED0</span><span class="p">;</span>
	<span class="c1">// Clearing a bit without hardware support</span>
	<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">_gpio</span><span class="p">)</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">LED0</span><span class="p">;</span></code></pre></figure>

<p>The Verilog logic necessary to handle this is trivially simple to write,</p>

<figure class="highlight"><pre><code class="language-verilog" data-lang="verilog"><span class="k">initial</span> <span class="n">o_gpio</span> <span class="o">=</span> <span class="n">DEFAULT_OUTPUTS</span><span class="p">;</span>
<span class="k">always</span> <span class="o">@</span><span class="p">(</span><span class="kt">posedge</span> <span class="n">i_clk</span><span class="p">)</span>
<span class="k">if</span> <span class="p">((</span><span class="n">i_wb_stb</span><span class="p">)</span><span class="o">&amp;&amp;</span><span class="p">(</span><span class="n">i_wb_we</span><span class="p">))</span>
	<span class="n">o_gpio</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">o_gpio</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">i_wb_data</span><span class="p">[</span><span class="mi">31</span><span class="o">:</span><span class="mi">16</span><span class="p">])</span>
		<span class="o">|</span> <span class="p">(</span><span class="n">i_wb_data</span><span class="p">[</span><span class="mi">15</span><span class="o">:</span><span class="mi">0</span><span class="p">]</span> <span class="o">&amp;</span> <span class="n">i_wb_data</span><span class="p">[</span><span class="mi">31</span><span class="o">:</span><span class="mi">16</span><span class="p">]);</span></code></pre></figure>

<p>The input logic is really irrelevant to our discussion today, but it’s not
much more than a <a href="/blog/2017/10/20/cdc.html">2FF
synchronizer</a>.</p>

<figure class="highlight"><pre><code class="language-verilog" data-lang="verilog"><span class="kt">reg</span>	<span class="p">[</span><span class="mi">31</span><span class="o">:</span><span class="mi">0</span><span class="p">]</span>	<span class="n">p_gpio</span><span class="p">;</span>
<span class="kt">reg</span>	<span class="p">[</span><span class="mi">15</span><span class="o">:</span><span class="mi">0</span><span class="p">]</span>	<span class="n">r_gpio</span><span class="p">;</span>
<span class="k">always</span> <span class="o">@</span><span class="p">(</span><span class="kt">posedge</span> <span class="n">i_clk</span><span class="p">)</span>
	<span class="o">{</span> <span class="n">r_gpio</span><span class="p">,</span> <span class="n">p_gpio</span> <span class="o">}</span> <span class="o">&lt;=</span> <span class="o">{</span> <span class="n">p_gpio</span><span class="p">,</span> <span class="n">i_gpio</span> <span class="o">}</span><span class="p">;</span></code></pre></figure>

<p>A quick check for anything changing can be used to create a
<a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO</a>
<a href="https://en.wikipedia.org/wiki/Interrupt">interrupt</a>,</p>

<figure class="highlight"><pre><code class="language-verilog" data-lang="verilog"><span class="k">always</span> <span class="o">@</span><span class="p">(</span><span class="kt">posedge</span> <span class="n">i_clk</span><span class="p">)</span>
	<span class="n">o_int</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">r_gpio</span> <span class="o">!=</span> <span class="n">p_gpio</span><span class="p">[</span><span class="mi">31</span><span class="o">:</span><span class="mi">16</span><span class="p">]);</span></code></pre></figure>

<p>… or at least something like that.  The actual implementation tries to
<a href="/blog/2017/06/12/minimizing-luts.html">free up as much logic as
possible</a>
by only adjusting a parameterizable <code class="language-plaintext highlighter-rouge">NOUT</code> output bits and only
reading and testing for changes from <code class="language-plaintext highlighter-rouge">NIN</code> input bits.</p>

<p>It really is just that basic.</p>

<p>The logic is also fast.  As you can see, it only takes a single clock cycle
to toggle any <a href="https://en.wikipedia.org/wiki/General-purpose_input/output">output
pins</a>.
Surely this wouldn’t slow a
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
down, right?</p>

<h2 id="running-a-zipcpu-program">Running a ZipCPU program</h2>

<p>If you are interested in trying out the
<a href="/about/zipcpu.html">ZipCPU</a>
in simulation, the <a href="/zipcpu/2018/02/12/zbasic-intro.html">ZBasic
repository</a>
is one of the better repositories for that purpose.  Sure, I have
<a href="/projects.html">other repositories</a> tailored for
specific boards, but
<a href="/zipcpu/2018/02/12/zbasic-intro.html">this one</a>
 is fairly generic.</p>

<p>To support this test, I recently added the
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">WBGPIO module</a>
to the repository, as well as the <a href="https://github.com/ZipCPU/zbasic/blob/master/auto-data/gpio.txt">WBGPIO AutoFPGA configuration
file</a>.  One
run of <a href="/zipcpu/2017/10/05/autofpga-intro.html">AutoFPGA</a>,
and this <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">new module</a>
has been merged: new I/Os are created at the <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/toplevel.v">top
level</a>, the
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/main.v">main.v file</a>
now connects it to my <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone
bus</a>,
the <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">GPIO control</a>
register has been added to the <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/host/regdefs.h">list of host accessible
registers</a>,
and the <a href="/about/zipcpu.html">CPU</a>’s
<a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/board.h">hardware definition header
file</a>
now includes the defines necessary to access this peripheral.</p>

<p>Pretty neat, huh?</p>

<p>If you want to test it, you’ll need to build the
<a href="/about/zipcpu.html">ZipCPU</a>’s
<a href="/zipcpu/2018/01/31/cpu-build.html">tool chain</a>.
You’ll find <a href="/zipcpu/2018/01/31/cpu-build.html">instructions for building the toolchain
here</a>.
Once built, you should have <code class="language-plaintext highlighter-rouge">zip-gcc</code> in your path.</p>

<p>I placed a <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">gpiotoggle.c</a>
program in the <a href="https://github.com/ZipCPU/zbasic/tree/master/sw/board">sw/board</a>
directory for you, and adjusted the
<a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/Makefile">makefile</a>
so it should build one of several tests for us.  Feel free to examine <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">that
program</a>,
and adjust it as you see fit should you wish to repeat or modify this test.</p>

<p>Once you build <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">gpiotoggle</a>,
you’ll then want to start the simulation.  The easiest way is to run
<code class="language-plaintext highlighter-rouge">main_tb</code> from from the
<a href="https://github.com/ZipCPU/zbasic/tree/master/sim/verilated">sim/verilated</a>
directory, and to instruct it to load and run the 
<a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">gpiotoggle</a>
program.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">% <span class="nb">cd </span>sim/verilated
% main_tb <span class="nt">-d</span> ../../sw/board/gpiotoggle</code></pre></figure>

<p>The <code class="language-plaintext highlighter-rouge">-d</code> flag above turns on the internal debugging options.  In particular,
it tells the simulator to create a <a href="/blog/2017/07/31/vcd.html">VCD
trace</a>
file output that will be placed into <code class="language-plaintext highlighter-rouge">trace.vcd</code>.</p>

<p>Since the program will go on forever, you’ll need to press a control-C to
kill it.  On my ancient computer, about five seconds is all that is required
to create 250MB file, which should be completely sufficient for our needs today.</p>

<p>Once killed, you can pull the
<a href="/blog/2017/07/31/vcd.html">VCD trace file</a> up in
<a href="http://gtkwave.sourceforge.net">GTKWave</a>
to see how fast the LED toggled.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">% gtkwave trace.vcd</code></pre></figure>

<p>Once you have the trace up, pull in the <code class="language-plaintext highlighter-rouge">o_gpio</code> trace at the top level
and expand it.  You should get something like Fig. 2.</p>

<table align="center" style="float: none"><caption>Fig 2. A Simulation VCD trace, showing the GPIO pin(s) toggling</caption><tr><td><img src="/img/cpu-blinky/toggle-gtkwave-single-wide.png" alt="" width="780" /></td></tr></table>

<p>Don’t forget, you’ll have to scan past the program getting loaded by the
<a href="/zipcpu/2018/02/12/zbasic-intro.html">bootloader</a>
to get to the point where the I/O is toggling.  If you zoom into this
section where <code class="language-plaintext highlighter-rouge">o_gpio[0]</code> toggles, you should see something like Fig. 3 below.</p>

<table align="center" style="float: none"><caption>Fig 3. Zooming in on the trace, we can see a 700ns period</caption><tr><td><img src="/img/cpu-blinky/toggle-gtkwave-single.png" alt="" width="780" /></td></tr></table>

<p>Shall we see how fast we can toggle this pin?</p>

<h2 id="running-our-experiments">Running our Experiments</h2>

<p>Let’s run some experiments.  We’ll start slow, with the
<a href="/about/zipcpu.html">CPU</a>’s
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
disabled, and then slowly enable features along the way.</p>

<p>Before starting, though, let me ask you to do one thing: take out a piece
of paper, and write onto it the fastest speed you expect the
<a href="/about/zipcpu.html">CPU</a>
to be able to toggle the
<a href="https://en.wikipedia.org/wiki/General-purpose_input/output">I/O pin</a>,
assuming that the
<a href="/about/zipcpu.html">CPU</a>
is running at 100MHz.</p>

<p>Why 100MHz?  Well, it’s sort of my baseline
system clock speed, dating back to my work on the
<a href="https://store.digilentinc.com/basys-3-artix-7-fpga-trainer-board-recommended-for-introductory-users">Basys3</a>.  Since the 
<a href="https://store.digilentinc.com/basys-3-artix-7-fpga-trainer-board-recommended-for-introductory-users">Basys3</a>
offered a 100MHz clock input, I got used to using that speed for
development.  The reality is that some
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>s
will run slower, such as the Spartan 6 or the iCE40,
and some will run faster.  One individual even reported running the
<a href="/about/zipcpu.html">ZipCPU</a>
at 150MHz.  100MHz just happens to be a nice number in between that makes
reading data from a
<a href="/blog/2017/07/31/vcd.html">trace file</a>
easier–since each clock tick is 10ns.</p>

<p>Now, fold that paper up with your prediction on it, and then let’s continue.</p>

<h4 id="starting-out-slow">Starting out slow</h4>

<p>Let’s start out as slow as we can, just to see how things improve by
adding more logic to our design.  If you go into the file
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/cpudefs.v">rtl/cpu/cpudefs.v</a>,
you can edit the
<a href="/about/zipcpu.html">ZipCPU</a>’s
default configuration.  Let’s start by uncommenting the line defining
<code class="language-plaintext highlighter-rouge">OPT_SINGLE_FETCH</code>.  We’ll also comment the <code class="language-plaintext highlighter-rouge">OPT_EARLY_BRANCHING</code> definition,
and so disable it.  This is almost the lowest logic configuration of the
<a href="/about/zipcpu.html">ZipCPU</a>.
We’ve just turned off the
<a href="/about/zipcpu.html">ZipCPU</a>’s
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline logic</a>,
we’ve
turned off all caches and the pipelined data access mode.
(More about pipelined data access in a moment.)  If we wanted, we could also
disable the multiply and divide instructions, but those should be irrelevant
for the purposes of today’s test.</p>

<p>Go ahead and rebuild the design now, by typing <code class="language-plaintext highlighter-rouge">make</code> from the root directory.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">% <span class="c"># cd to the root directory, then</span>
% make</code></pre></figure>

<p>Now let’s look at our <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">LED toggling
program</a>.
The
<a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">program</a>
contains many different approaches to toggling the LED.  We’ll work through
them one at a time.  We’ll start by using the following loop to toggle the LED.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
	<span class="kt">int</span>	<span class="n">gpiocmd</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">16</span><span class="p">;</span>
	<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">gpiocmd</span><span class="p">;</span>
		<span class="n">gpiocmd</span> <span class="o">^=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span></code></pre></figure>

<p>Notice that, because of how we built our
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">WBGPIO controller</a>,
we don’t need to read from the output port prior to writing the new value in
order to ensure that we only toggle a single bit.</p>

<p>I shouldn’t have to mention that for any type of test of this type, you need
to turn compiler optimizations on with <code class="language-plaintext highlighter-rouge">-O3</code>.  Without optimization, this
little snippet of code will turn into,</p>

<figure class="highlight"><pre><code class="language-asm" data-lang="asm">00e00000 &lt;main&gt;:
  ; Create a stack frame, and save some registers to it so they can be
  ; recovered later.
  e00000:	e8 14 85 0c 	SUB        $20,SP         | SW         R0,$12(SP)
  e00004:	64 c7 40 10 	SW         R12,$16(SP)
  ; Now create a stack frame where our local variables may be stored
  e00008:	63 43 40 0c 	MOV        $12+SP,R12
  ; Save a copy of argc and argv (we won't be using them anyway
  e0000c:	0c c7 3f f8 	SW         R1,$-8(R12)
  e00010:	14 c7 3f f4 	SW         R2,$-12(R12)
  ; gpiocmd = 0x10000, or 65536.  Load this into a register, then store it into
  ; the stack location for gpio
  e00014:	0e 01 00 00 	LDI        $65536,R1
  e00018:	0c c7 3f fc 	SW         R1,$-4(R12)
  ; Load the address of our GPIO register into R1
  ; This is also the first instruction within our our while(1) loop
  e0001c:	0a 00 03 00 	LDI        0x00c0000c,R1  // c0000c &lt;_kram+0xc0000c&gt;
  e00020:	0a 40 00 0c 
  ; Load gpiocmd into R0, and write that to the GPIO register
  e00024:	84 e4 85 88 	LW         -4(R12),R0     | SW         R0,(R1)
  ; Load gpiocmd back into R0, and XOR it with one
  e00028:	04 87 3f fc 	LW         -4(R12),R0
  e0002c:	01 00 00 01 	XOR        $1,R0
  ; Write the result back into gpiocmd
  e00030:	04 c7 3f fc 	SW         R0,$-4(R12)
  ; and loop back to the top of our while loop
  e00034:	78 83 ff e4 	BRA        @0x00e0001c    // e0001c &lt;main+0x1c&gt;_</code></pre></figure>

<p>Each instruction takes, at a minimum, one cycle.
You’ll see in a moment how difficult it can be to fetch each of these
many instructions.</p>

<p>Part of the problem with this unoptimized implementation is that all of the
data values are kept in a local variable space in memory, never in any
registers.  As you’ll also see below, each <code class="language-plaintext highlighter-rouge">SW</code> (store word) instruction
to write our <code class="language-plaintext highlighter-rouge">gpiocmd</code> variable to memory can take many clock cycles.
Loads are worse, since the
<a href="/about/zipcpu.html">ZipCPU</a>
needs to wait for a load to complete before continuing.
In other words, this is a very slow way to toggle an
<a href="https://en.wikipedia.org/wiki/General-purpose_input/output">I/O</a>,</p>

<p>On the other hand, if you place variables into registers, such as placing
<code class="language-plaintext highlighter-rouge">gpiocmd</code> into <code class="language-plaintext highlighter-rouge">R1</code>, you’ll get much faster code.  GCC’s <code class="language-plaintext highlighter-rouge">-O3</code> applies
three levels of optimizations doing just this, so don’t forget to use it.
In this example, if you run a “make gpiotoggle.txt” from within the
<a href="https://github.com/ZipCPU/zbasic/tree/master/sw/board">sw/board</a> directory,
you’ll get both an optimized executable as well as a
<a href="/about/zipcpu.html">ZipCPU</a>
disassembly file.  If you look through that file, you can find
the main program generated from our C-code above.  I added some comments to it
below, to help it make more sense to a reader.</p>

<figure class="highlight"><pre><code class="language-asm" data-lang="asm">00e00000 &lt;main&gt;:
  ; gpiocmd = (1&lt;&lt;16)
  e00000:       0e 01 00 00     LDI        $65536,R1
  ; R2 = _gpio, the address of the WBGPIO's one control register
  e00004:       12 00 03 00     LDI        0x00c0000c,R2  // c0000c &lt;_kram+0xc0000c&gt;
  e00008:       12 40 00 0c
  ; Here's the first instruction of our while loop: *_gpio = gpiocmd;
  e0000c:       0c c4 80 00     SW         R1,(R2)
  ; Now we toggle the bottom bit, gpiocmd ^= 1;
  e00010:       09 00 00 01     XOR        $1,R1A
  ; And repeat back to the beginning of our loop
  e00014:       78 83 ff f4     BRA        @0x00e0000c    // e0000c &lt;main+0xc&gt;</code></pre></figure>

<p>Within this main program are three instructions: one store instruction to
set the I/O, one XOR instruction to toggle the lower bit of the register <code class="language-plaintext highlighter-rouge">R1</code>
containing our <code class="language-plaintext highlighter-rouge">gpiocmd</code>, and then a
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
always instruction to return to the top of the loop.</p>

<p>This looks much better than the unoptimized version!</p>

<p>So, let’s see … a quick back of the envelope estimate says that if we are
running at a 100MHz clock, these three instructions should take <code class="language-plaintext highlighter-rouge">10ns</code> each,
so we should be able to toggle our LED every <code class="language-plaintext highlighter-rouge">30ns</code>, right?</p>

<p>In this case, with <code class="language-plaintext highlighter-rouge">OPT_SINGLE_FETCH</code> defined, it takes <code class="language-plaintext highlighter-rouge">350ns</code> to toggle
the LED once, or <code class="language-plaintext highlighter-rouge">700ns</code> per cycle.  The
<a href="https://en.wikipedia.org/wiki/General-purpose_input/output">I/O pin</a>
therefore toggles at 1.4MHz.</p>

<p>Wow.  What just happened?!</p>

<p>If you examine a <a href="/blog/2017/07/31/vcd.html">VCD
trace</a>,
you’ll see something like Fig. 4 below.</p>

<table align="center" style="float: none"><caption>Fig 4. VCD trace showing the CPU pipeline signals associated with toggling an LED</caption><tr><td><a href="/img/cpu-blinky/toggle-gtkwave-single-all.png"><img src="/img/cpu-blinky/toggle-gtkwave-single-all-snap.png" alt="" width="780" /></a></td></tr></table>

<p>The <a href="/blog/2017/07/31/vcd.html">trace</a> can be rather confusing
to understand.  First, you need to know the meanings of the various
<a href="/zipcpu/2017/11/07/wb-formal.html">bus signals</a>,
the names of the four bus interfaces shown, mem_ (the CPU
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/memops.v">memory controller</a>),
pf_ (the CPU
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>),
zip_ (the combined memory/prefetch bus), and wb_ (the <a href="/blog/2017/06/22/simple-wb-interconnect.html">master
bus</a>
within the system), as well as the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">CPU pipeline signals</a>
such
as pf_valid (there’s a valid instruction coming out of the prefetch stage),
dcd_valid (a valid instruction has come out of the instruction decoder),
op_valid (operands are ready to be used by the execution units) and
wr_reg_ce (a value may now be written to the register file).  At the end of all
of that is the <code class="language-plaintext highlighter-rouge">new_pc</code> signal indicating that a
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
has been detected at the end of the pipeline.</p>

<p>While I <a href="/zipcpu/2017/08/23/cpu-pipeline.html">covered the basics of what these signals meant
before</a>, the overall
trace can be difficult to follow.  Therefore, I’ve summarized the signals
from this trace in Fig. 5 below–to make for easy reading.  You should also
be able to click on the figure for an expanded version that might be easier
to view.</p>

<table align="center" style="float: none"><caption>Fig 5. Toggling an LED with a basic prefetch</caption><tr><td><a href="/img/cpu-blinky/toggle-slow-single.svg"><img src="/img/cpu-blinky/toggle-slow-single.svg" alt="" width="780" /></a></td></tr></table>

<p>Basically, any time there’s an instruction at the output of a particular stage,
instead of just listing the stage valid signal, I’ve  also placed the
instruction’s name into the figure as well–so you can see the instruction work
its way through the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>.</p>

<p>So let’s spend some time examining Fig 5.</p>

<p>This <a href="/blog/2017/07/31/vcd.html">trace</a>
basically shows just one cycle in this loop.</p>

<p>Within the trace, you can see the three instructions of our loop marked as
<code class="language-plaintext highlighter-rouge">SW</code>, <code class="language-plaintext highlighter-rouge">XOR</code>, and <code class="language-plaintext highlighter-rouge">BRA</code> respectively.</p>

<p>The timing within this trace is driven primarily by the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>,
shown in the <code class="language-plaintext highlighter-rouge">I-Bus</code> line at the top.  (This captures the <code class="language-plaintext highlighter-rouge">pf_cyc</code> and
<code class="language-plaintext highlighter-rouge">pf_stb</code> lines from Fig. 4 further up.)
<a href="/zipcpu/2017/11/18/wb-prefetch.html">We discussed this particular prefetch and how it
works</a>
some time ago.  In
<a href="/zipcpu/2017/11/18/wb-prefetch.html">that discussion</a>,
I showed some clearer timing diagrams when illustrating how it would work.
In this chart, I’ve now collapsed the <code class="language-plaintext highlighter-rouge">CYC &amp; STB</code> signal into the <code class="language-plaintext highlighter-rouge">STB</code> line,
the <code class="language-plaintext highlighter-rouge">CYC &amp; ACK</code> signal into the <code class="language-plaintext highlighter-rouge">ACK</code> cycle and <code class="language-plaintext highlighter-rouge">WAIT</code> in between for those
times when <code class="language-plaintext highlighter-rouge">CYC &amp; !STB &amp; !ACK</code>.</p>

<p>But, why are their four wait cycles?  To answer that question, let me direct
your attention to Fig. 6 to the right.</p>

<table align="center" style="float: right"><caption>Fig 6. Bus signals for a memory access</caption><tr><td><img src="/img/cpu-blinky/toggle-memack.svg" alt="" width="360" /></td></tr></table>

<p>In this figure, you can see that the
<a href="/about/zipcpu.html">ZipCPU</a>’s
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>
starts a <a href="/zipcpu/2017/11/07/wb-formal.html">memory cycle</a>.
by setting its cycle and strobe lines.  It then takes one clock cycle to
arbitrate between the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/zipcpu.v">CPU</a>
and the
<a href="/blog/2017/06/16/dbg-bus-forest.html">debugging bus</a>,
creating a STB signal that actually gets broadcast across the bus.  As
currently configured, the
<a href="/zipcpu/2018/07/13/memories.html">block RAM memory</a>
takes two cycles to return.  While we could drop this to one cycle, the
<a href="/zipcpu/2018/07/13/memories.html">memory</a>
was configured for two cycles in this test.  One cycle would be just one clock
faster on every
<a href="/zipcpu/2018/07/13/memories.html">memory</a>
operation.  The next problem is in the
<a href="/blog/2017/06/22/simple-wb-interconnect.html">interconnect</a>
where we have to select a result from among many items that might respond.
Hence, an additional clock is taken to multiplex among these many
possible answers.  Finally, there’s an additional clock cycle to get our
data back into the
<a href="/about/zipcpu.html">CPU</a>.</p>

<p>Why are so many clocks involved?  Are they required?</p>

<table align="center" style="float: left; padding: 15px"><caption>Fig 7. ZBasic's bus structure</caption><tr><td><img src="/img/cpu-blinky/toggle-bus.svg" alt="" width="360" /></td></tr></table>

<p>To answer this question, let’s look at the
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
structure for the
<a href="/zipcpu/2018/02/12/zbasic-intro.html">ZBasic</a>
design, shown on the left.</p>

<p>The <a href="/about/zipcpu.html">ZipCPU</a> in this figure
contains two
<a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone bus</a>
drivers.  Because the <a href="/about/zipcpu.html">ZipCPU</a>
is a <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">Von Neumann
architecture</a>, there is
only one <a href="/zipcpu/2017/11/07/wb-formal.html">bus interface</a>
leaving the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/zipcpu.v">CPU</a>.
(Not shown in the picture is a second
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/ex/wbarbiter.v">arbiter</a>
dealing with the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/peripherals/wbdmac.v">DMA</a>
in the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/zipsystem.v">ZipSystem</a>.)
Once the <a href="/zipcpu/2017/11/07/wb-formal.html">bus</a> leaves the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/zipcpu.v">CPU</a>,
it is then merged with the
<a href="/blog/2017/06/16/dbg-bus-forest.html">debugging bus</a>.  An
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/wbpriarbiter.v">arbiter</a>
then selects between the <a href="/blog/2017/06/08/simple-wb-master.html">two bus
masters</a>.  However,
by this point in time there’s now
been <a href="/blog/2017/09/18/clocks-for-sw-engineers.html">too much combinatorial logic for the clock
period</a>.
In order to maintain a high clock speed, a
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/busdelay.v">delay</a>
needs to be inserted on the path to the primary
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>.</p>

<p>This gets us to the master <a href="/zipcpu/2017/11/07/wb-formal.html">bus
strobe</a>.</p>

<p>But what about the delay in the acknowledgement?</p>

<p>The purpose of the acknowledgment delay is basically the same thing: to
<a href="/blog/2017/09/18/clocks-for-sw-engineers.html">reduce the necessary amount of logic within one clock
period</a>.
In particular, our bus implementation contains a <a href="/blog/2017/06/22/simple-wb-interconnect.html">large case
statement</a>
controlling the return data.</p>

<figure class="highlight"><pre><code class="language-verilog" data-lang="verilog">	<span class="k">always</span> <span class="o">@</span><span class="p">(</span><span class="kt">posedge</span> <span class="n">i_clk</span><span class="p">)</span>
	<span class="k">begin</span>
		<span class="k">casez</span><span class="p">(</span><span class="o">{</span>		<span class="n">scope_sdcard_ack</span><span class="p">,</span>
				<span class="n">flctl_ack</span><span class="p">,</span>
				<span class="n">sdcard_ack</span><span class="p">,</span>
				<span class="n">uart_ack</span><span class="p">,</span>
				<span class="n">rtc_ack</span><span class="p">,</span>
				<span class="n">wb_sio_ack</span><span class="p">,</span>
				<span class="n">bkram_ack</span>	<span class="o">}</span><span class="p">)</span>
			<span class="mb">7'b1??????</span><span class="o">:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">scope_sdcard_data</span><span class="p">;</span>
			<span class="mb">7'b01?????</span><span class="o">:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">flctl_data</span><span class="p">;</span>
			<span class="mb">7'b001????</span><span class="o">:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">sdcard_data</span><span class="p">;</span>
			<span class="mb">7'b0001???</span><span class="o">:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">uart_data</span><span class="p">;</span>
			<span class="mb">7'b00001??</span><span class="o">:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">rtc_data</span><span class="p">;</span>
			<span class="mb">7'b000001?</span><span class="o">:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">wb_sio_data</span><span class="p">;</span>
			<span class="mb">7'b0000001</span><span class="o">:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">bkram_data</span><span class="p">;</span>
			<span class="nl">default:</span> <span class="n">wb_idata</span> <span class="o">&lt;=</span> <span class="n">flash_data</span><span class="p">;</span>
		<span class="k">endcase</span>
	<span class="k">end</span></code></pre></figure>

<p>This takes time.  To keep the
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
speed up, this case statement was given it’s own clock period.  Indeed,
there’s yet another clock taken to get back through the
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/busdelay.v">bus delay</a>.</p>

<p>Hence, when you count it all out, fetching an instruction using <a href="/zipcpu/2017/11/18/wb-prefetch.html">this
prefetch</a>
takes five cycles within
<a href="https://github.com/ZipCPU/zbasic">this deslgn</a>.</p>

<p>So, why were there six cycles shown in the trace in Fig. 5 above?  Because an
extra cycle was used within the priority prefetch/memory arbiter.  That
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/ex/wbdblpriarb.v">priority arbiter</a>
defaults to offering access to the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/memops.v">memory
unit</a> over the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>.  An extra
clock is required to switch from
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/memops.v">data</a>
to <a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a> access.
This makes perfect sense when the
<a href="/about/zipcpu.html">ZipCPU</a>’s <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">prefetch uses an instruction
cache</a>,
but the priority probably needs to switch when it isn’t.</p>

<table align="center" style="float: left; padding: 15px"><caption>Fig 8. GPIO peripheral access timing</caption><tr><td><img src="/img/cpu-blinky/toggle-gpioack.svg" alt="" width="360" /></td></tr></table>

<p>The timing associated with accessing the
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">GPIO device</a>
to toggle the LED is similar.  The big difference is that
there’s only a single clock delay within the
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">GPIO device</a>.</p>

<p>Still, 1.4MHz is a good start.  Let’s consider this the pretest: we can do
better.  The only problem is that doing better will cost us
<a href="/blog/2017/06/12/minimizing-luts.html">more logic</a>.
Therefore, we’ll need to adjust our <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/cpudefs.v">configuration
option(s)</a>
to control how much logic will be used.</p>

<h4 id="adding-a-better-fetch-routine">Adding a better fetch routine</h4>

<p>Some time ago, <a href="/zipcpu/2018/03/21/dblfetch.html">I discussed how to build a prefetch that would fetch two
instructions at once</a>.
Let’s take a look at the difference we might expect by using <a href="/zipcpu/2018/03/21/dblfetch.html">this “better”
prefetch</a>
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dblfetch.v">unit</a>.
To enable <a href="/zipcpu/2018/03/21/dblfetch.html">this second/alternate
prefetch</a>,
we’ll comment the <code class="language-plaintext highlighter-rouge">OPT_SINGLE_FETCH</code> option and uncomment the
<code class="language-plaintext highlighter-rouge">OPT_DOUBLE_FETCH</code> option from within the <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/cpudefs.v">CPU configuration
file</a>.</p>

<p>An example trace from this updated configuration is shown above in
Fig. 9 below.</p>

<table align="center" style="float: none"><caption>Fig 9. Toggling an LED with our pipelind prefetch</caption><tr><td><a href="/img/cpu-blinky/toggle-slow-double.svg"><img src="/img/cpu-blinky/toggle-slow-double.svg" alt="" width="780" /></a></td></tr></table>

<p>The big difference between this trace and the one in Fig. 5 above is that the
<a href="/zipcpu/2018/03/21/dblfetch.html">prefetch</a>
<a href="/zipcpu/2017/11/07/wb-formal.html">strobe signals</a>
are now
three cycles long, and there are two acknowledgement cycles.  Given that the
first strobe cycle deals with the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/ex/wbdblpriarb.v">priority
arbiter</a>
focusing on <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/memops.v">data
access</a>
instead of the <a href="/zipcpu/2018/03/21/dblfetch.html">instruction
fetch</a>, we’re still
<a href="/zipcpu/2018/03/21/dblfetch.html">fetching two instructions</a>
in eight cycles now instead
of one instruction in seven cycles.  Clearly doubling the speed of the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>
should speed up our algorithm, right?</p>

<p>Well, yes, just not much.  We went from taking 700ns down to 580ns per cycle.</p>

<p>What happened?  Why aren’t we going any faster?</p>

<p>In this case, the problem is the fact that the
<a href="/about/zipcpu.html">CPU</a>
isn’t fully
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipelined</a>,</p>

<p>Yes, the
<a href="/about/zipcpu.html">ZipCPU</a>
is a <a href="/zipcpu/2017/08/23/cpu-pipeline.html">fully pipelined
CPU</a>.
This requires multiple copies of the internal CPU data structures–one per
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline stage</a>.
This also requires some
rather elaborate stall calculation logic.  To create a non-pipelined
CPU, such as we have been testing so far, the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
stall logic has been simplified and many of the stages share data.  In
other words: the <a href="/about/zipcpu.html">CPU</a>’s
instructions take several exclusive clock cycles to complete in this mode.</p>

<p>The next problem is that the <code class="language-plaintext highlighter-rouge">XOR</code> instruction has to wait at the output of the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>
until it has been accepted into the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/idecode.v">decode unit</a>.
This will keep the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>
from getting another instruction until the <code class="language-plaintext highlighter-rouge">XOR</code> has moved
into the decode operands stage.</p>

<p>As you might expect, the store word instruction takes five cycles on the data
bus to finally complete.  This means that we took a whole eight cycles to
execute this one instruction before the next instruction could enter
the instruction decode stage.</p>

<p>Sure, the <code class="language-plaintext highlighter-rouge">XOR</code> instruction executes faster, taking only four clocks,
but during this time <a href="/about/zipcpu.html">the CPU</a>
is waiting for the memory cycle to complete before starting this
instruction.</p>

<p>Worse, the <code class="language-plaintext highlighter-rouge">BRA</code> instruction cannot be fetched until the CPU accepts
the <code class="language-plaintext highlighter-rouge">XOR</code> instruction from the
<a href="/zipcpu/2018/03/21/dblfetch.html">prefetch</a>.</p>

<p>When the next instruction is finally available, it’s a
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a> always
instruction (BRA).  When this instruction gets to the write-back stage, the
<a href="/zipcpu/2018/03/21/dblfetch.html">prefetch</a>
has to reset itself and start fetching the next instruction from a new
address: the result of the <a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>.  This also means we just aborted an ongoing
<a href="/zipcpu/2018/03/21/dblfetch.html">prefetch</a>
memory operation for the instruction that would’ve followed the <code class="language-plaintext highlighter-rouge">BRA</code>
operation, had it not been a
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>.</p>

<p>In all, we are now taking <code class="language-plaintext highlighter-rouge">290ns</code> for three instructions, or just under <a href="https://en.wikipedia.org/wiki/Instructions_per_cycle">ten
clocks per instruction</a>.
At this rate we can toggle our LED at <code class="language-plaintext highlighter-rouge">1.7MHz</code>.</p>

<p>That doesn’t feel like much of an improvement over the last
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/prefetch.v">implementation</a>.</p>

<p>What if we turned on the
<a href="/about/zipcpu.html">CPU</a>’s
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipelining logic</a>?
Would that help?</p>

<h4 id="going-full-pipeline">Going full pipeline</h4>

<p><a href="/blog/2017/08/14/strategies-for-pipelining.html">Pipelining</a>
allows the <a href="/about/zipcpu.html">ZipCPU</a>
to execute multiple instructions at the same time.  To do this, instruction
processing is split into stages, with the effect that multiple instructions
can be processed at once–with one instruction in each
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline stage</a>.</p>

<table align="center" style="float: right"><caption>Fig 10. The ZipCPU's pipeline structure</caption><tr><td><img src="/img/zipcpu.png" alt="" width="240" /></td></tr></table>

<p>As you may recall, the
<a href="/about/zipcpu.html">ZipCPU</a>
has <a href="/zipcpu/2017/08/23/cpu-pipeline.html">five basic pipeline
stages</a>:
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>, decode,
<a href="/formal/2018/07/21/zipcpu-icoboard.html">read-operands</a>,
<a href="/zipcpu/2017/08/11/simple-alu.html">execute ALU/mem/divide</a>,
and write-back, as shown in Fig. 10 on the right.  In general,
each instruction takes one clock cycle to work through each stage,
although most of my charts today just show when the outputs of all the given
stages are valid, with the exception that the <code class="language-plaintext highlighter-rouge">WB</code> (write-back) line shows when
the input of the write-back stage is valid.</p>

<p>All that said, if you execute multiple instructions at once, the result should
be faster, right?</p>

<p>Let’s find out!</p>

<p>In order to enable the <a href="/zipcpu/2017/08/23/cpu-pipeline.html">CPU
pipeline</a>
in the <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/cpudefs.v">configuration
file</a>,
both the <code class="language-plaintext highlighter-rouge">OPT_DOUBLE_FETCH</code>
and the earlier <code class="language-plaintext highlighter-rouge">OPT_SINGLE_FETCH</code> options need to be commented out.  This
also enables the <a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/pfcache.v">instruction
cache</a>,
in order to be able to feed the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/zipcpu.v">CPU</a>
enough instructions to keep it busy.  Just to give us something to examine
later, let’s also turn off the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data cache</a>,
early branching, and the
<a href="/about/zipcpu.html">ZipCPU</a>’s
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pipemem.v">pipelined bus</a>
capability.  (More on that later.)  We can do this by commenting the
<code class="language-plaintext highlighter-rouge">OPT_EARLY_BRANCHING</code> and <code class="language-plaintext highlighter-rouge">OPT_PIPELINED_BUS_ACCESS</code> configuration options.
Turning off the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/dcache.v">data cache</a>
is a bit more difficult, since it requires setting <code class="language-plaintext highlighter-rouge">LGDCACHE</code> to zero in the
<a href="/about/zipcpu.html">ZipCPU</a>’s
<a href="/zipcpu/2017/10/05/autofpga-intro.html">AutoFPGA</a>
<a href="https://github.com/ZipCPU/zbasic/blob/master/auto-data/zipmaster.txt">configuration file</a>
and then rerunning
<a href="/zipcpu/2017/10/05/autofpga-intro.html">AutoFPGA</a>.</p>

<p>Once done, we can run our three instruction loop again.</p>

<p>You can see the basic results in the
<a href="/blog/2017/07/31/vcd.html">trace</a>
shown in Fig. 11 below.</p>

<table align="center" style="float: none"><caption>Fig 11. Toggling an LED with full CPU pipelining enabled</caption><tr><td><a href="/img/cpu-blinky/toggle-pip-noearly.svg"><img src="/img/cpu-blinky/toggle-pip-noearly.svg" alt="" width="780" /></a></td></tr></table>

<p>Wait, I thought
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">CPU pipelines</a>
were supposed to be able to execute with one instruction in every stage?
What’s with all the empty stages?</p>

<p>We’ll pick up the story after the
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
instruction gets into the write-back stage.  This forces the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
to be cleared, so all the work we’ve done on any subsequent instructions
needs to be thrown away.</p>

<p>Ouch!  That’ll slow us down.</p>

<p>Second, in addition to killing our
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>,
we also suffer a clock in the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction
cache</a>
due to switching between cache lines.  This manifests itself in an extra clock
before <code class="language-plaintext highlighter-rouge">SW</code> shows on the <code class="language-plaintext highlighter-rouge">PF</code> line, as well as an extra clock between the
<code class="language-plaintext highlighter-rouge">SW</code> and <code class="language-plaintext highlighter-rouge">XOR</code> instructions on that same line.  After the
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>,
our first instruction, <code class="language-plaintext highlighter-rouge">SW</code> is ready to move through our
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
after two cycles.  However, the cycle after <code class="language-plaintext highlighter-rouge">SW</code> is valid in the <code class="language-plaintext highlighter-rouge">PF</code> stage
is empty again.  Why?  Because we are again switching cache lines: <code class="language-plaintext highlighter-rouge">SW</code> is
the last instruction in a given cache line.  Our
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">cache implementation</a>
requires an extra instruction cycle when switching cache lines.</p>

<p>Why?  Shouldn’t a
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">cache</a>
be able to deliver one <a href="https://en.wikipedia.org/wiki/Instructions_per_cycle">instruction per
cycle</a>?</p>

<p>Yes, perhaps it should.  However, the cache tag is stored in block RAM.
Therefore, it costs us one cycle to look up the cache tag, and a second
cycle to compare if its the right tag.  (I really need to blog about this.)
With some optimization, we can skip this in the great majority of
cases, but every now and then an access crosses cache lines and must suffer
a stall.</p>

<p>Once we get to the <code class="language-plaintext highlighter-rouge">XOR</code> instruction, the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">instruction
cache</a>
seems to be doing well.</p>

<p>A second optimization in the
<a href="/about/zipcpu.html">ZipCPU</a>’s
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
implementation allows the <code class="language-plaintext highlighter-rouge">XOR</code> to complete before the <code class="language-plaintext highlighter-rouge">SW</code> instruction does.
This only works for store instructions, not data load instructions.  Because
store instructions don’t modify any registers, the
<a href="/about/zipcpu.html">ZipCPU</a>
doesn’t need to hold the next instruction waiting for a result.</p>

<p>This optimization doesn’t apply to
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
instructions however.  Looking at this
stall, I’m not sure I can explain it very well.  I have a vague recollection
of some complication forcing this, but I might need to go back and re-examine
that stall logic.  That’s the fun thing about examining traces in detail,
though–you see all kinds of things you might not have been expecting.</p>

<p>Of course, since the early branching we’re going to discuss in the next section
is such a cheap optimization, costing <a href="/blog/2017/06/12/minimizing-luts.html">so few extra logic
elements</a>, that I
hardly ever run the
<a href="/about/zipcpu.html">ZipCPU</a>
without it as we have just done here.</p>

<p>In the end, this took us <code class="language-plaintext highlighter-rouge">120ns</code> to execute these three instructions
and toggle our LED, or 4 <a href="https://en.wikipedia.org/wiki/Instructions_per_cycle">clock cycles per
instruction</a>.  This
leads to <code class="language-plaintext highlighter-rouge">240ns</code> per LED cycle, or <code class="language-plaintext highlighter-rouge">4.2MHz</code>.  While this is better than
<code class="language-plaintext highlighter-rouge">580ns</code> per cycle, it’s still a far cry from the speed I’d expect from a
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipelined</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>.</p>

<p>We can do better.</p>

<h4 id="early-branching">Early Branching</h4>

<p>Perhaps you noticed in the last section all the instructions filling the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
that had to be thrown out when the
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
instruction was encountered.  This is unfortunate.  Let’s do better in this
section.</p>

<p>The <a href="/about/zipcpu.html">ZipCPU</a>
has the capability of recognizing certain
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
instructions
from within the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/idecode.v">decode stage</a> This allows the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/idecode.v">decode stage</a>
to send any
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
always instructions directly to the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>.  This
also allows the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
to fill back up while the
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
instruction bubble works its way through the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>.</p>

<p>In order to enable this early branching capability, we’ll uncomment and set
the <code class="language-plaintext highlighter-rouge">OPT_EARLY_BRANCHING</code> flag within the configuration file.</p>

<p>With this new configuration, we’re now down to <a href="https://en.wikipedia.org/wiki/Instructions_per_cycle"><code class="language-plaintext highlighter-rouge">60ns</code> per three
instructions</a>
to toggle the I/O, or <code class="language-plaintext highlighter-rouge">120ns</code> per cycle, for a cycle rate now of 8.3MHz.
You can see the resulting trace shown in Fig. 12 below.</p>

<table align="center" style="float: none"><caption>Fig 12. Toggling an LED with both pipelining and early branching enabled</caption><tr><td><a href="/img/cpu-blinky/toggle-piped.svg"><img src="/img/cpu-blinky/toggle-piped.svg" alt="" width="780" /></a></td></tr></table>

<p>Unlike our previous figures, I’m now showing multiple toggles in this trace.
Why?  Because I can!  The
<a href="/blog/2017/07/31/vcd.html">trace</a>
is now that short that I can fit multiple toggles in a single image.</p>

<p>The
<a href="/blog/2017/07/31/vcd.html">trace</a>
starts out much as before, with the <code class="language-plaintext highlighter-rouge">SW</code>, the stall, and then the
<code class="language-plaintext highlighter-rouge">XOR</code> and <code class="language-plaintext highlighter-rouge">BRA</code> instructions coming from the instruction
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">prefetch</a>
unit.</p>

<p>This time, however, while the <code class="language-plaintext highlighter-rouge">BRA</code> instruction is in the decode stage, the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>
stage is invalidated and the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">cache</a>
gets sent to get the next instruction in the loop.</p>

<p>While there appear to be some further room for optimization here, the data
bus is now completely loaded.  As a result, the only way we might go faster
would be to speed up the data bus by, for example, simplifying the
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
structure or removing some of the peripherals.</p>

<p>While <code class="language-plaintext highlighter-rouge">8.3MHz</code> is much faster than we started, it’s still much slower than
the <a href="/about/zipcpu.html">CPU</a>’s
clock speed.  Indeed, looking over our program, if our
<a href="/about/zipcpu.html">CPU</a>
had no stalls at all, we would only ever be able to do <code class="language-plaintext highlighter-rouge">60ns</code> per cycle, or
<code class="language-plaintext highlighter-rouge">16.6MHz</code>.</p>

<h4 id="pipelined-multiple-bus-accesses">Pipelined Multiple Bus Accesses</h4>

<p>What if we wanted our
<a href="/about/zipcpu.html">CPU</a>
to toggle this LED faster?  Speeding up the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">I-cache</a>
won’t help, nor would better branching logic.  Right now, our
<a href="/zipcpu/2017/11/07/wb-formal.html">bus</a>
is the bottleneck.  It’s at its highest speed.  Hence, we can’t push any
more instructions into our
<a href="/about/zipcpu.html">CPU</a>
if we are stuck waiting four cycles for the
<a href="/zipcpu/2017/11/07/wb-formal.html">bus cycle</a>
to complete.  While we might be able to shave a clock cycle off in our
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
implementation latency, doing that would essentially strip the
<a href="https://github.com/ZipCPU/zbasic">ZBasic SoC</a>
down so bare that it could no longer be used for general purpose processing.</p>

<p>That leaves only one way to go faster: to stuff more than one store in each
<a href="/zipcpu/2017/11/07/wb-formal.html">bus</a>
transaction.</p>

<p>The first step towards making this happen is to uncomment the
<code class="language-plaintext highlighter-rouge">OPT_PIPELINED_BUS_ACCESS</code> parameter in the
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/cpu/cpudefs.v">CPU configuration file</a>,
and then to rebuild the
<a href="/zipcpu/2018/02/12/zbasic-intro.html">ZBasic</a>
simulator.</p>

<p>This time, let’s update our
<a href="https://github.com/ZipCPU/zbasic/blob/master/rtl/wbgpio.v">GPIO</a> <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">toggling
software</a>
as well.  Instead of using the <code class="language-plaintext highlighter-rouge">XOR</code> instruction, let’s instead issue
back-to-back set and clear instructions.</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++">	<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_SET</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_CLEAR</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
	<span class="p">}</span></code></pre></figure>

<p>This also compiles into a three instruction loop, like before, but this time
it’s slightly different.</p>

<figure class="highlight"><pre><code class="language-asm" data-lang="asm">00e00000 &lt;main&gt;:
  e00000:       0a 00 03 00     LDI        0x00c0000c,R1  // c0000c &lt;_kram+0xc0000c&gt;
  e00004:       0a 40 00 0c 
  e00008:       1e 01 00 01     LDI        $65537,R3
  e0000c:       13 40 df ff     MOV        $-1+R3,R2
  e00010:       9d 88 95 88     SW         R3,(R1)        | SW         R2,(R1)
  e00014:       78 83 ff f8     BRA        @0x00e00010    // e00010 &lt;main+0x10&gt;</code></pre></figure>

<p>In this case, we have two store instructions in a row followed by our
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
always instruction.  Further, the two store word instructions use
the <a href="/about/zipcpu.html">ZipCPU</a>’s
compressed instruction set encoding, so both are shown as part
of the same instruction word.</p>

<p>The <a href="/about/zipcpu.html">ZipCPU</a> has a <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pipemem.v">memory
controller</a>
that has the ability to issue multiple subsequent memory
requests.  This is the controller we just selected by enabling
<code class="language-plaintext highlighter-rouge">OPT_PIPELINED_BUS_ACCESS</code>.  Issuing multiple
<a href="/zipcpu/2017/11/07/wb-formal.html">bus requests</a>,
however, has some requirements in order to avoid crossing devices
and thus losing acknowledgments or getting any results out of order.
Specifically, multiple requests must be to the same identical, or to
subsequent, addresses.  Hence these two store word instructions
will be placed into the same memory transfer.</p>

<p>To understand what that might look like, let’s take a look at Fig. 13 below.</p>

<table align="center" style="float: none"><caption>Fig 13. Issuing two write operations on consecutive clocks</caption><tr><td><a href="/img/cpu-blinky/toggle-pipemem2.svg"><img src="/img/cpu-blinky/toggle-pipemem2.svg" alt="" width="780" /></a></td></tr></table>

<p>The first thing to notice is that we are now issuing two back to back store
word requests of the bus.  (The <a href="/zipcpu/2017/11/07/wb-formal.html">WB STB
lines</a>
are high for two consecutive cycles, while the stall lines are low.)  These
two instructions fly through the
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a> logic in
adjacent clock periods.  Hence, when the acknowledgments
come back, they are still together.</p>

<p>If you look down at the LED line, you’ll also notice the two changes are made
back to back.  First the LED is set, then it is cleared.  Then nothing happens
until the next
<a href="/zipcpu/2017/11/07/wb-formal.html">bus cycle</a>.
This means we now have a duty cycle of only 14%.  Sure, we’re
toggling faster, now at a rate of <code class="language-plaintext highlighter-rouge">70ns</code> per cycle or equivalently at
a rate of 14MHz, but we now lost the 50% duty cycle we once had in our
original square wave.</p>

<p>Next, did you notice that the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">prefetch</a>
now issues a valid instruction immediately following the <code class="language-plaintext highlighter-rouge">BRA</code> instruction
from the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/idecode.v">decode
stage</a>?
This is because we aren’t crossing cache lines anymore.</p>

<p>Further, did you notice the instructions highlighted in blue?  These represent
the first half of the decompressed compressed instructions.  Since the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">prefetch</a>
knows nothing about the compressed instruction encoding, all of the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">prefetches control
state</a>
is captured by the stall signal–independent of the blue marking.  The
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/idecode.v">decoder</a>
is the first to recognize the compressed instruction, and so I’ve then split
the store word instruction word into <code class="language-plaintext highlighter-rouge">SW1</code> and <code class="language-plaintext highlighter-rouge">SW2</code> representing the first
and second store word instruction respectively.</p>

<p>Finally, notice how the first <code class="language-plaintext highlighter-rouge">SW</code> instruction gets stuck in the read-operands
stage for an extra three cycles.  This is due to the fact that the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pipemem.v">memory unit</a>
is busy, and so these commands cannot (yet) issue until the two memory
acknowledgements come back.  Just to help illustrate this, I added the
data bus <code class="language-plaintext highlighter-rouge">CYC</code> signal back into my trace summary, outlining the time when the
<a href="/zipcpu/2017/11/07/wb-formal.html">bus</a>
is busy.  The first store instruction, <code class="language-plaintext highlighter-rouge">SW1</code>, cannot issue until this
<a href="/zipcpu/2017/11/07/wb-formal.html">memory cycle</a>
finishes.  Hence we are still limited by the
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
speed.</p>

<p>Can we do better than <code class="language-plaintext highlighter-rouge">14MHz</code>?  What if we unrolled our loop a bit and so
packed eight store instructions per loop?  Our C code would now look like,</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++">	<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_SET</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_CLEAR</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_SET</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_CLEAR</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_SET</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_CLEAR</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_SET</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
		<span class="o">*</span><span class="n">_gpio</span> <span class="o">=</span> <span class="n">GPIO_CLEAR</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
	<span class="p">}</span></code></pre></figure>

<p>with the associated assembly code,</p>

<figure class="highlight"><pre><code class="language-asm" data-lang="asm">00e00000 &lt;main&gt;:
  e00000:	0a 00 03 00 	LDI        0x00c0000c,R1  // c0000c &lt;_kram+0xc0000c&gt;
  e00004:	0a 40 00 0c 
  e00008:	1e 01 00 01 	LDI        $65537,R3
  e0000c:	13 40 df ff 	MOV        $-1+R3,R2
  e00010:	9d 88 95 88 	SW         R3,(R1)        | SW         R2,(R1)
  e00014:	9d 88 95 88 	SW         R3,(R1)        | SW         R2,(R1)
  e00018:	9d 88 95 88 	SW         R3,(R1)        | SW         R2,(R1)
  e0001c:	9d 88 95 88 	SW         R3,(R1)        | SW         R2,(R1)
  e00020:	78 83 ff ec 	BRA        @0x00e00010    // e00010 &lt;main+0x10&gt;</code></pre></figure>

<p>How fast would you expect this loop to toggle our LED?</p>

<table align="center" style="float: none"><caption>Fig 14. Issuing eight write operations on consecutive clocks</caption><tr><td><a href="/img/cpu-blinky/toggle-pipemem.svg"><img src="/img/cpu-blinky/toggle-pipemem.svg" alt="" width="780" /></a></td></tr></table>

<p>As you can see from Fig. 14 above, we are now toggling our LED four times in
<code class="language-plaintext highlighter-rouge">130ns</code>, for a rough rate of <code class="language-plaintext highlighter-rouge">30MHz</code>.</p>

<p>This is finally starting to look like we might expect from a
<a href="/about/zipcpu.html">CPU</a>.  We’re still taking a pretty big
hit from the <a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>,
which is also forcing the ongoing memory operation to be flushed.</p>

<p>Might we go further?  Certainly!  If you check the <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">GPIO toggling
program</a>,
there’s an example within it that now toggles our LED 72 times in <code class="language-plaintext highlighter-rouge">770ns</code>.
At first I didn’t believe this would be possible, since the instruction
stream would now cross multiple cache lines.  If the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
isn’t kept filled, the <a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pipemem.v">memory
controller</a>
will break the extended memory cycle.  However, in this case, because
there are so many compressed instructions, the extra cache cycles associated
with crossing cache lines aren’t noticed.</p>

<p>This gives us our ultimate LED toggling rate of <code class="language-plaintext highlighter-rouge">47MHz</code>.</p>

<p>It’s also the time to check your notes.  Remember how I asked you to scribble
down the speed you expected at first, indicating how fast you felt a
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
could toggle a <a href="https://en.wikipedia.org/wiki/General-purpose_input/output">GPIO
pin</a>?  Go ahead,
take a peek at your scribbled note.  How close did you come to the results
we just presented?</p>

<h2 id="other-bus-implementations">Other Bus Implementations</h2>

<p>I would be remiss if I didn’t point out two things regarding other
implementations.</p>

<p>First, the <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone Classic implementation found in the Wishbone B3
specification</a>
requires a minimum of three cycles per
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
access, and doesn’t allow multiple transactions to be in flight at once.
At best, this would have limited us to <code class="language-plaintext highlighter-rouge">120ns</code> per cycle and our <code class="language-plaintext highlighter-rouge">8.3MHz</code>
number above.  At worst, this will slow down the entire
<a href="https://en.wikipedia.org/wiki/System_on_a_chip">SoC</a>
operation from
100MHz down to 50MHz.  However, this is the
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
structure used by the <a href="https://openrisc.io">OpenRISC</a>
<a href="https://en.wikipedia.org/wiki/Soft_microprocessor">soft-core</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>.</p>

<p>Second, while the <a href="/formal/2018/12/28/axilite.html">AXI bus</a>
is more universally accepted than the <a href="/zipcpu/2017/11/07/wb-formal.html">Wishbone
Bus</a>,
<a href="https://www.xilinx.com">Xilinx</a>’s <a href="/formal/2018/12/28/axilite.html">default AXI-lite
implementation</a>
can’t handle once access per clock.  At best, it can only do one access
every other clock.  At that rate, your best speed would only ever be <code class="language-plaintext highlighter-rouge">90ns</code>
per loop (assuming only two toggles, or one LED cycle per loop), not <code class="language-plaintext highlighter-rouge">70ns</code> per
loop.  Likewise, if you tried to do the 72 toggles per loop using
that demo <a href="/formal/2018/12/28/axilite.html">AXI</a>-lite
peripheral, you’d be stuck at <code class="language-plaintext highlighter-rouge">24MHz</code> instead of the <code class="language-plaintext highlighter-rouge">47MHz</code> mentioned above.</p>

<p>My whole point here is that if speed is important to you, then your choice of
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
matters.</p>

<p>These same observations apply to your choice of
<a href="/blog/2017/06/22/simple-wb-interconnect.html">interconnect</a>
implementation as well.  However, without any insight into how the various
<a href="https://en.wikipedia.org/wiki/System_on_a_chip">SoC</a>
projects have implemented their interconnects, it’s a bit difficult to
compare them.</p>

<h2 id="conclusions">Conclusions</h2>

<p>We’ve now examined several implementations of
<a href="/blog/2017/05/19/blinky.html">blinky</a>
from the standpoint of the
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>.
What sorts of conclusions can we draw?
Perhaps the first and most obvious conclusion is that the speed of
<a href="/blog/2017/05/19/blinky.html">blinky</a>
<em>depends</em>.  Just because
<a href="https://en.wikipedia.org/wiki/Soft_microprocessor">your processor</a>
runs at 100MHz doesn’t mean you’ll be able to
<a href="/blog/2017/05/19/blinky.html">blink</a>
an LED at anywhere near that rate.  The closest we managed to get was 47MHz.</p>

<ol>
  <li>
    <p>Our first two examples showed how critical the
<a href="/zipcpu/2017/11/18/wb-prefetch.html">prefetch</a>’s
performance is to overall
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
speed.  Indeed, reality is usually worse than these examples.  Our <a href="https://github.com/ZipCPU/zbasic/blob/master/sw/board/gpiotoggle.c">code
above</a>
ran from an on-chip
<a href="/zipcpu/2018/07/13/memories.html">block RAM component</a>.
Had we been trying to read instructions from an external
<a href="https://en.wikipedia.org/wiki/Static_random-access_memory">SRAM</a>
or even <a href="https://en.wikipedia.org/wiki/Synchronous_dynamic_random-access_memory">SDRAM</a>,
our performance would’ve likely been an order of magnitude worse.</p>
  </li>
  <li>
    <p>We saw several examples of how an ongoing memory operation would bring the
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
to a halt.  A really fast
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>,
therefore, can only go as fast as its memory unit and
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
will let it.</p>
  </li>
  <li>
    <p><a href="/zipcpu/2017/08/23/cpu-pipeline.html">Pipelined</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>’s
are commonly known for being able to retire one <a href="https://en.wikipedia.org/wiki/Instructions_per_cycle">instruction
per clock</a>.
Here we saw several examples where a
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipelined</a>
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
wasn’t able to retire <a href="https://en.wikipedia.org/wiki/Instructions_per_cycle">one instruction per
clock</a>.
.  We saw examples where the
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/pfcache.v">prefetch</a>
couldn’t keep the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>
filled with instructions, where an ongoing
<a href="/zipcpu/2017/11/07/wb-formal.html">memory operation</a>
forced the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a> to stall,
and where a
<a href="https://en.wikipedia.org/wiki/Branch_(computer_science)">branch</a>
instruction forced the
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a> to clear the
<a href="/zipcpu/2017/08/23/cpu-pipeline.html">pipeline</a>.</p>

    <p>All of these realities are known for keeping a
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>’s
number of <a href="https://en.wikipedia.org/wiki/Instructions_per_cycle">instructions per clock
cycle</a>
lower than the ideal.</p>
  </li>
  <li>
    <p>We saw the importance of a good early branching scheme.  While the
<a href="/about/zipcpu.html">ZipCPU</a>
doesn’t really implement a traditional
<a href="https://en.wikipedia.org/wiki/Branch_predictor">branch prediction</a> scheme,
it’s early branching mechanism can often compensate.  Just adding this
capability to our
<a href="https://github.com/ZipCPU/zipcpu/blob/master/rtl/core/zipcpu.v">CPU</a>
under test nearly doubled its performance from 4.7MHZ to 8.3 MHz.</p>
  </li>
  <li>
    <p>At one point, we got to the point where our performance was entirely
dominated by the speed of a
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
interaction.  The
<a href="/about/zipcpu.html">CPU</a>
could run faster, but the
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
could not.  In many ways, I might argue that this test does more to measure a
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>’s
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>
speed than it measures the speed of the
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>.
The only problem with that argument is that you can still mess up the
speed of <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
implementation.  Hence the I/O speed really depends upon both the speed of
the <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>.
as well as the speed of the
<a href="https://en.wikipedia.org/wiki/Bus_(computing)">bus</a>.</p>
  </li>
  <li>
    <p>We also learned that we could cheat the whole system if we could stuff
multiple store requests into the same memory transaction, yielding our
highest total toggle rate but yet distorting the square wave produced.</p>
  </li>
</ol>

<p>Finally, I should point out that computers aren’t optimized for toggling LEDs,
so in many ways this is a very poor measure of how fast a
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
can perform.
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>s
are optimized for executing instructions and for …
<a href="https://en.wikipedia.org/wiki/Interrupt">interrupt</a>s!
<a href="https://en.wikipedia.org/wiki/Computer_multitasking">Multi-tasking</a>!
By the time you limit your
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
down to three instructions
only, you’ve really destroyed the power your
<a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a>
had initially to
execute arbitrary instructions, to execute diverse functions, to handle
<a href="https://en.wikipedia.org/wiki/Interrupt">interrupt</a>s
and more.</p>

<p>You might argue that this is like taking a semi-tractor, filling the trailer
with a single loaf of bread, and taking it out onto the race track.  That’s
not what it was designed for!</p>

<p>On the other hand, if you really wanted to toggle an LED quickly, why not
just do it from Verilog?</p>

<figure class="highlight"><pre><code class="language-verilog" data-lang="verilog">	<span class="n">initiali</span> <span class="n">o_led</span> <span class="o">=</span> <span class="mb">1'b0</span><span class="p">;</span>
	<span class="k">always</span> <span class="o">@</span><span class="p">(</span><span class="kt">posedge</span> <span class="n">i_clk</span><span class="p">)</span>
		<span class="n">o_led</span> <span class="o">&lt;=</span> <span class="o">!</span><span class="n">o_led</span><span class="p">;</span></code></pre></figure>

<p>That design is a whole lot simpler than all the work we just spent to get our
<a href="/about/zipcpu.html">ZipCPU</a>
to toggle an LED at high speed, and it uses far fewer
<a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>
<a href="/blog/2017/06/12/minimizing-luts.html">resources</a>,
while toggling our LED at a whole 50MHz!</p>

  </div>


<div class "verse">
<HR align="center;" width="25%">
<P><em>And he gave some, apostles; and some, prophets; and some, evangelists; and some, pastors and teachers (Eph 4:11)</em>


</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The ZipCPU by Gisselquist Technology</h2>
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <!-- <li></li> -->
          <li><a href="mailto:zipcpu@gmail.com">zipcpu@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="soc-medlist">
          
          <li>
            <a href="https://github.com/ZipCPU"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">ZipCPU</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/zipcpu"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">@zipcpu</span></a>

          </li>
          
          
          <li><A href="https://www.patreon.com/ZipCPU"><img src="/img/become_a_patron_button.png"></a></li>
          

        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The ZipCPU blog, featuring how to discussions of FPGA and soft-core CPU design.  This site will be focused on Verilog solutions, using exclusively OpenSource IP products for FPGA design.  Particular focus areas include topics often left out of more mainstream FPGA design courses such as how to debug an FPGA design.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
